{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "env.yml not found in /disk/u/nikhil/mind!\n",
      "Setting MODEL_ROOT=\"\". Models will now be downloaded to conda env cache, if not already there\n",
      "Other defaults are set to:\n",
      "    DATA_DIR = \"data\"\n",
      "    RESULTS_DIR = \"results\"\n",
      "    HPARAMS_DIR = \"hparams\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import Any, List, Optional\n",
    "import nnsight\n",
    "from nnsight import CONFIG, LanguageModel\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from einops import einsum\n",
    "import time\n",
    "from einops import rearrange, reduce\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from src.dataset import SampleV3, DatasetV3, STORY_TEMPLATES\n",
    "from src.utils import env_utils\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "random.seed(10)\n",
    "\n",
    "CONFIG.set_default_api_key(\"d9e00ab7d4f74643b3176de0913f24a7\")\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "CONFIG.APP.REMOTE_LOGGING = False\n",
    "\n",
    "# Define random seed\n",
    "seed = 10\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = {}\n",
    "all_containers= {}\n",
    "all_characters = json.load(open(os.path.join(env_utils.DEFAULT_DATA_DIR, \"synthetic_entities\", \"characters.json\"), \"r\"))\n",
    "\n",
    "for TYPE, DCT in {\"states\": all_states, \"containers\": all_containers}.items():\n",
    "    ROOT = os.path.join(\n",
    "        env_utils.DEFAULT_DATA_DIR, \"synthetic_entities\", TYPE\n",
    "    )\n",
    "    for file in os.listdir(ROOT):\n",
    "        file_path = os.path.join(ROOT, file)\n",
    "        with open(file_path, \"r\") as f:\n",
    "            names = json.load(f)\n",
    "        DCT[file.split(\".\")[0]] = names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [00:37<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# model = LanguageModel(\"meta-llama/Meta-Llama-3.1-405B\")\n",
    "model = LanguageModel(\"meta-llama/Meta-Llama-3-70B-Instruct\", cache_dir=\"/disk/u/nikhil/.cache/huggingface/hub/\", device_map=\"auto\", load_in_4bit=True, torch_dtype=torch.float16, dispatch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ques_start_token_idx(batch_size, tokenizer, prompt, padding_side=\"right\"):\n",
    "    input_tokens = tokenizer(prompt, return_tensors=\"pt\", padding=True, padding_side=padding_side).input_ids\n",
    "    colon_token = tokenizer.encode(\":\", return_tensors=\"pt\").squeeze()[-1].item()\n",
    "    ques_start_idx = (input_tokens == colon_token).nonzero()[torch.arange(2, 4*batch_size, 4)][:, 1] - 1\n",
    "\n",
    "    return ques_start_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_token_len(tokenizer, prompt, padding_side=\"right\"):\n",
    "    input_tokens = tokenizer(prompt, return_tensors=\"pt\", padding=True, padding_side=padding_side)\n",
    "    return input_tokens.attention_mask.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pred(pred, target):\n",
    "    prompt = f\"Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No'.\\nGround truth: {target}\\nPrediction: {pred}\\nAnswer:\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with model.generate(prompt, max_new_tokens=5, do_sample=False, num_return_sequences=1, pad_token_id=model.tokenizer.pad_token_id):\n",
    "            out = model.generator.output.save()\n",
    "\n",
    "    prompt_len = get_prompt_token_len(model.tokenizer, prompt)\n",
    "\n",
    "    return out, prompt_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading BigToM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a csv file\n",
    "df_false = pd.read_csv(\"../data/bigtom/0_forward_belief_false_belief/stories.csv\", delimiter=\";\")\n",
    "df_true = pd.read_csv(\"../data/bigtom/0_forward_belief_true_belief/stories.csv\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each row in the dataframe extract story, answer, and distractor\n",
    "true_stories, false_stories = [], []\n",
    "for i in range(len(df_true)):\n",
    "    story = df_true.iloc[i]['story']\n",
    "    question = df_true.iloc[i]['question']\n",
    "    answer = df_true.iloc[i]['answer']\n",
    "    distractor = df_true.iloc[i]['distractor']\n",
    "    true_stories.append({\"story\": story, \"question\": question, \"answer\": answer, \"distractor\": distractor})\n",
    "\n",
    "for i in range(len(df_false)):\n",
    "    story = df_false.iloc[i]['story']\n",
    "    question = df_true.iloc[i]['question']\n",
    "    answer = df_false.iloc[i]['answer']\n",
    "    distractor = df_false.iloc[i]['distractor']\n",
    "    false_stories.append({\"story\": story, \"question\": question, \"answer\": answer, \"distractor\": distractor})\n",
    "\n",
    "dataset = []\n",
    "instruction = \"1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\"\n",
    "\n",
    "for i in range(min(len(true_stories), len(false_stories))):\n",
    "    question = true_stories[i]['question']\n",
    "    visible_prompt = f\"Instructions: {instruction}\\n\\nStory: {true_stories[i]['story']}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "    question = false_stories[i]['question']\n",
    "    invisible_prompt = f\"Instructions: {instruction}\\n\\nStory: {false_stories[i]['story']}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "    visible_ans = true_stories[i]['answer'].split()\n",
    "    invisible_ans = false_stories[i]['answer'].split()\n",
    "\n",
    "    # Find the index of first word which is different in both answers\n",
    "    diff_idx = 0\n",
    "    for idx, (v, j) in enumerate(zip(visible_ans, invisible_ans)):\n",
    "        if v != j:\n",
    "            diff_idx = idx\n",
    "            break\n",
    "    \n",
    "    visible_ans = \" \".join(visible_ans[diff_idx:])[:-1]\n",
    "    invisible_ans = \" \".join(invisible_ans[diff_idx:])[:-1]\n",
    "\n",
    "    random_choice = random.choice([0, 1])\n",
    "\n",
    "    dataset.append({\n",
    "        \"alt_story\": true_stories[i]['story'] if random_choice == 0 else false_stories[i]['story'],\n",
    "        \"alt_question\": true_stories[i]['question'] if random_choice == 0 else false_stories[i]['question'],\n",
    "        \"alt_prompt\": visible_prompt if random_choice == 0 else invisible_prompt,\n",
    "        \"alt_ans\": visible_ans if random_choice == 0 else invisible_ans,\n",
    "        \"org_story\": false_stories[i]['story'] if random_choice == 0 else true_stories[i]['story'],\n",
    "        \"org_question\": false_stories[i]['question'] if random_choice == 0 else true_stories[i]['question'],\n",
    "        \"org_prompt\": invisible_prompt if random_choice == 0 else visible_prompt,\n",
    "        \"org_ans\": invisible_ans if random_choice == 0 else visible_ans,\n",
    "        \"target\": visible_ans if random_choice == 0 else invisible_ans,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\n",
      "\n",
      "Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor sees her coworker swapping the milk.\n",
      "Question: Does Noor believe the milk pitcher contains oat milk or almond milk?\n",
      "Answer: almond milk\n",
      "Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\n",
      "\n",
      "Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor does not see her coworker swapping the milk.\n",
      "Question: Does Noor believe the milk pitcher contains oat milk or almond milk?\n",
      "Answer: oat milk\n",
      "Target: almond milk\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(dataset[idx]['alt_prompt'], dataset[idx]['alt_ans'])\n",
    "print(dataset[idx]['org_prompt'], dataset[idx]['org_ans'])\n",
    "print(f\"Target: {dataset[idx]['target']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 80\n",
    "valid_size = 40\n",
    "batch_size = 4\n",
    "\n",
    "train_dataset = dataset[:train_size]\n",
    "valid_dataset = dataset[train_size:train_size+valid_size]\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Custom Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 80\n",
    "valid_size = 20\n",
    "batch_size = 4\n",
    "\n",
    "train_dataset = get_visibility_align_exps(STORY_TEMPLATES,\n",
    "                             all_characters,\n",
    "                             all_containers,\n",
    "                             all_states,\n",
    "                             train_size,\n",
    "                             question_type=\"belief_question\",\n",
    "                             diff_visibility=True)\n",
    "valid_dataset = get_visibility_align_exps(STORY_TEMPLATES,\n",
    "                             all_characters,\n",
    "                             all_containers,\n",
    "                             all_states,\n",
    "                             valid_size,\n",
    "                             question_type=\"belief_question\",\n",
    "                             diff_visibility=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=valid_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any beliefs about the container and its contents which they cannot observe. 4. To answer the question, predict only what is inside the queried container, strictly based on the belief of the character, mentioned in the question. 5. If the queried character has no belief about the container in question, then predict 'unknown'. 6. Do not predict container or character as the final output.\n",
      "\n",
      "Story: Jack and Heidi are working in a busy restaurant. To complete an order, Jack grabs an opaque dispenser and fills it with cocktail. Then Heidi grabs another opaque mug and fills it with punch. They are working side by side and can clearly observe each other's actions.\n",
      "Question: What does Heidi believe the dispenser contains?\n",
      "Answer: cocktail\n",
      "Instruction: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any beliefs about the container and its contents which they cannot observe. 4. To answer the question, predict only what is inside the queried container, strictly based on the belief of the character, mentioned in the question. 5. If the queried character has no belief about the container in question, then predict 'unknown'. 6. Do not predict container or character as the final output.\n",
      "\n",
      "Story: Jack and Heidi are working in a busy restaurant. To complete an order, Jack grabs an opaque dispenser and fills it with porter. Then Heidi grabs another opaque mug and fills it with bourbon. They are working in the entirely separate sections, with no visibility between them.\n",
      "Question: What does Heidi believe the dispenser contains?\n",
      "Answer: unknown\n",
      " porter\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(train_dataset[idx]['corrupt_prompt'], train_dataset[idx]['corrupt_ans'])\n",
    "print(train_dataset[idx]['clean_prompt'], train_dataset[idx]['clean_ans'])\n",
    "print(train_dataset[idx]['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Singular Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sing_vecs = defaultdict(dict)\n",
    "for l in range(41):\n",
    "    sing_vecs[l] = torch.load(f\"../svd_results/bigtom/singular_vecs/{l}.pt\").cpu()\n",
    "    # sing_vecs[l] = torch.load(f\"../svd_results/toy/singular_vecs/{l}.pt\").cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full vector | Acc = 0.97\n",
    "\n",
    "lamb = 0.2 | 1 Epoch | Lr = 0.1 | 49 SVs | Acc = 0.50\n",
    "lamb = 0.1 | 1 Epoch | Lr = 0.1 | 75 (66) SVs | Acc = 0.775(0.72)\n",
    "lamb = 0.075 | 1 Epoch | Lr = 0.1 | 86 (76) SVs | Acc = 0.8 (0.72)\n",
    "lamb = 0.05 | 1 Epoch | Lr = 0.1 |  103 (88) SVs | Acc = 0.8 (0.75)\n",
    "lamb = 0.045 | 1 Epoch | Lr = 0.1 | 115 SVs | Acc = 0.8\n",
    "lamb = 0.04 | 1 Epoch | Lr = 0.1 | 127 SVs | Acc = 0.8\n",
    "lamb = 0.035 | 1 Epoch | Lr = 0.1 | 140(117) SVs | Acc = 0.8(0.775)\n",
    "lamb = 0.03 | 1 Epoch | Lr = 0.1 |  158(138) SVs | Acc = 0.80\n",
    "lamb = 0.025 | 1 Epoch | Lr = 0.1 | 159 SVs | Acc = 0.80\n",
    "lamb = 0.02 | 1 Epoch | Lr = 0.1 | 198 SVs | Acc = 0.80\n",
    "lamb = 0.01 | 1 Epoch | Lr = 0.1 | 387 SVs | Acc = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Task Loss: -23.0938, L1 Loss: 190.0000, Total Loss: 166.9062\n",
      "#Causal SVs: 3814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [01:47<07:11, 26.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 4, Task Loss: -37.3750, L1 Loss: 116.0000, Total Loss: 120.8563\n",
      "#Causal SVs: 3814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [03:51<05:57, 29.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 8, Task Loss: -40.0312, L1 Loss: 44.2500, Total Loss: 80.4132\n",
      "#Causal SVs: 124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [05:44<03:50, 28.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 12, Task Loss: -42.6250, L1 Loss: 5.6875, Total Loss: 43.8918\n",
      "#Causal SVs: 105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [07:44<01:59, 29.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 16, Task Loss: -34.2500, L1 Loss: 5.0000, Total Loss: 25.9421\n",
      "#Causal SVs: 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [09:46<00:00, 29.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for layer: 30, lambda: 0.05\n",
      "Validation started for layer: 30\n",
      "Rank: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [05:47<00:00, 34.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.85 | Correct: 34 | Total: 40\n",
      "\n",
      "Training layer: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Task Loss: -23.1250, L1 Loss: 190.0000, Total Loss: 166.8750\n",
      "#Causal SVs: 3814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [01:23<07:53, 27.88s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m org_prompt_len \u001b[38;5;241m=\u001b[39m get_prompt_token_len(model\u001b[38;5;241m.\u001b[39mtokenizer, org_prompt, padding_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrace() \u001b[38;5;28;01mas\u001b[39;00m tracer:\n\u001b[1;32m     36\u001b[0m     alt_acts \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39minvoke(alt_prompt):\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/intervention/contexts/interleaving.py:96\u001b[0m, in \u001b[0;36mInterleavingTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoker\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39m_envoy\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/tracing/contexts/tracer.py:25\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglobals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalTracingContext\n\u001b[1;32m     23\u001b[0m GlobalTracingContext\u001b[38;5;241m.\u001b[39mtry_deregister(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/tracing/contexts/base.py:72\u001b[0m, in \u001b[0;36mContext.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     69\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mstack\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc_val, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(graph\u001b[38;5;241m.\u001b[39mstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], graph, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[25], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[1;32m     39\u001b[0m         alt_acts[j] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[layer_idx]\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;241m0\u001b[39m][j, alt_ques_idx[j]:alt_prompt_len[j]]\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39minvoke(org_prompt):\n\u001b[1;32m     42\u001b[0m     sing_vec \u001b[38;5;241m=\u001b[39m sing_vecs[layer_idx]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     43\u001b[0m     masked_vec \u001b[38;5;241m=\u001b[39m sing_vec \u001b[38;5;241m*\u001b[39m mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/intervention/contexts/invoker.py:141\u001b[0m, in \u001b[0;36mInvoker.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39minvoker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc_val, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n",
      "Cell \u001b[0;32mIn[25], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m         alt_acts[j] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[layer_idx]\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;241m0\u001b[39m][j, alt_ques_idx[j]:alt_prompt_len[j]]\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39minvoke(org_prompt):\n\u001b[0;32m---> 42\u001b[0m     sing_vec \u001b[38;5;241m=\u001b[39m \u001b[43msing_vecs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     masked_vec \u001b[38;5;241m=\u001b[39m sing_vec \u001b[38;5;241m*\u001b[39m mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     44\u001b[0m     proj_matrix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(masked_vec\u001b[38;5;241m.\u001b[39mt(), masked_vec)\u001b[38;5;241m.\u001b[39mhalf()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "valid_accs_dcm, rank_dcm, preds = {}, {}, {}\n",
    "\n",
    "valid_accs = defaultdict(dict)\n",
    "for layer_idx in range(32, 34, 2):\n",
    "    preds[layer_idx] = {\"correct\": [], \"incorrect\": []}\n",
    "    model.tokenizer.padding_side = \"right\"\n",
    "\n",
    "    modules = [i for i in range(sing_vecs[layer_idx].shape[0])]\n",
    "    mask = torch.ones(len(modules), requires_grad=True, device=\"cuda\", dtype=torch.bfloat16)\n",
    "    optimizer = torch.optim.Adam([mask], lr=1e-1)\n",
    "    n_epochs = 1\n",
    "    lamb = 0.05\n",
    "\n",
    "    print(f\"Training layer: {layer_idx}\")\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for bi, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "            alt_prompt = batch[\"alt_prompt\"]\n",
    "            org_prompt = batch[\"org_prompt\"]\n",
    "            target = batch[\"target\"]\n",
    "            target_token = model.tokenizer(target, return_tensors=\"pt\", padding=True, padding_side=\"right\")\n",
    "            target_input_ids = target_token.input_ids[:, 1:]\n",
    "            target_attention_mask = target_token.attention_mask[:, 1:]\n",
    "            batch_size = target_input_ids.size(0)\n",
    "\n",
    "            alt_ques_idx = get_ques_start_token_idx(batch_size, model.tokenizer, alt_prompt, padding_side=\"right\")\n",
    "            alt_prompt_len = get_prompt_token_len(model.tokenizer, alt_prompt, padding_side=\"right\")\n",
    "            org_ques_idx = get_ques_start_token_idx(batch_size, model.tokenizer, org_prompt, padding_side=\"right\")\n",
    "            org_prompt_len = get_prompt_token_len(model.tokenizer, org_prompt, padding_side=\"right\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with model.trace() as tracer:\n",
    "\n",
    "                alt_acts = defaultdict(dict)\n",
    "                with tracer.invoke(alt_prompt):\n",
    "                    for j in range(batch_size):\n",
    "                        alt_acts[j] = model.model.layers[layer_idx].output[0][j, alt_ques_idx[j]:alt_prompt_len[j]].clone().save()\n",
    "\n",
    "                with tracer.invoke(org_prompt):\n",
    "                    sing_vec = sing_vecs[layer_idx].cuda()\n",
    "                    masked_vec = sing_vec * mask.unsqueeze(-1)\n",
    "                    proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "\n",
    "                    for j in range(batch_size):\n",
    "                        curr_output = model.model.layers[layer_idx].output[0][j, org_ques_idx[j]:org_prompt_len[j]].clone()\n",
    "\n",
    "                        alt_proj = torch.matmul(alt_acts[j], proj_matrix)\n",
    "                        org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "\n",
    "                        modified_out = curr_output - org_proj + alt_proj\n",
    "                        model.model.layers[layer_idx].output[0][j, org_ques_idx[j]:org_prompt_len[j]] = modified_out\n",
    "\n",
    "                    logits = model.lm_head.output[torch.arange(batch_size), org_prompt_len-1].save()\n",
    "\n",
    "                    del sing_vec, proj_matrix, masked_vec\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            target_logit = 0\n",
    "            for j in range(batch_size):\n",
    "                target_logit += logits[j, target_input_ids[j, target_attention_mask[j] == 1]].sum()\n",
    "            task_loss = -(target_logit/batch_size)\n",
    "            l1_loss = lamb * torch.norm(mask, p=1)\n",
    "            loss = task_loss + l1_loss.to(task_loss.device)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if bi % 4 == 0:\n",
    "                mean_loss = epoch_loss / (bi + 1)\n",
    "                print(f\"Epoch: {epoch}, Batch: {bi}, Task Loss: {task_loss.item():.4f}, \"\n",
    "                    f\"L1 Loss: {l1_loss.item():.4f}, Total Loss: {mean_loss:.4f}\")\n",
    "                with torch.no_grad():\n",
    "                    mask.data.clamp_(0, 1)\n",
    "                    rounded = torch.round(mask)\n",
    "                    print(f\"#Causal SVs: {(rounded == 1).sum().item()}\")\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                mask.data.clamp_(0, 1)\n",
    "    \n",
    "    del alt_acts, alt_prompt, org_prompt, target, target_token, target_input_ids, target_attention_mask, logits, task_loss, l1_loss, loss\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Training finished for layer: {layer_idx}, lambda: {lamb}\")\n",
    "\n",
    "    print(f\"Validation started for layer: {layer_idx}\")\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        model.tokenizer.padding_side = \"left\"\n",
    "        mask_data = mask.data.clone()\n",
    "        mask_data.clamp_(0, 1)\n",
    "        rounded = torch.round(mask)\n",
    "        print(f\"Rank: {(rounded == 1).sum().item()}\")\n",
    "        rank_dcm[layer_idx] = (rounded == 1).sum().item()\n",
    "\n",
    "        # Save the mask\n",
    "        torch.save(mask_data, f\"../masks/bigtom/{layer_idx}.pt\")\n",
    "\n",
    "        for bi, batch in tqdm(enumerate(valid_dataloader), total=len(valid_dataloader)):\n",
    "            alt_prompt = batch[\"alt_prompt\"]\n",
    "            org_prompt = batch[\"org_prompt\"]\n",
    "            alt_ans = batch[\"alt_ans\"]\n",
    "            batch_size = len(alt_ans)\n",
    "\n",
    "            alt_ques_idx = get_ques_start_token_idx(batch_size, model.tokenizer, alt_prompt, padding_side=\"left\")\n",
    "            alt_prompt_len = get_prompt_token_len(model.tokenizer, alt_prompt, padding_side=\"left\")\n",
    "            org_ques_idx = get_ques_start_token_idx(batch_size, model.tokenizer, org_prompt, padding_side=\"left\")\n",
    "            org_prompt_len = get_prompt_token_len(model.tokenizer, org_prompt, padding_side=\"left\")\n",
    "\n",
    "            with model.session() as session:\n",
    "\n",
    "                alt_acts = defaultdict(dict)\n",
    "                with model.trace(alt_prompt):\n",
    "                    for j in range(batch_size):\n",
    "                        alt_acts[j] = model.model.layers[layer_idx].output[0][j, alt_ques_idx[j]:].save()\n",
    "\n",
    "                with model.generate(org_prompt, max_new_tokens=8, do_sample=False, num_return_sequences=1, pad_token_id=model.tokenizer.pad_token_id, eos_token_id=model.tokenizer.eos_token_id):\n",
    "                    sing_vec = sing_vecs[layer_idx].cuda()\n",
    "                    masked_vec = sing_vec.to(rounded.device) * rounded.unsqueeze(-1)\n",
    "                    proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "\n",
    "                    for j in range(batch_size):\n",
    "                        curr_output = model.model.layers[layer_idx].output[0][j, org_ques_idx[j]:].clone()\n",
    "\n",
    "                        alt_proj = torch.matmul(alt_acts[j], proj_matrix)\n",
    "                        org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "\n",
    "                        modified_out = curr_output - org_proj + alt_proj\n",
    "                        model.model.layers[layer_idx].output[0][j, org_ques_idx[j]:] = modified_out\n",
    "``\n",
    "                    out = model.generator.output.save()\n",
    "\n",
    "                    del sing_vec, proj_matrix\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            for j in range(batch_size):\n",
    "                check_out, prompt_len = check_pred(model.tokenizer.decode(out[j, max(org_prompt_len):]), alt_ans[j])\n",
    "                check = model.tokenizer.decode(check_out[0, prompt_len:-1]).strip()\n",
    "                # print(model.tokenizer.decode(check_out[0]).strip()+'\\n')\n",
    "\n",
    "                if check == \"Yes\":\n",
    "                    correct += 1\n",
    "                    preds[layer_idx]['correct'].append({\n",
    "                        'pred': model.tokenizer.decode(out[j]).replace(\"<|eot_id|>\", \"\").replace(\"<|begin_of_text|>\", \"\").strip(),\n",
    "                        'ground_truth': alt_ans[j],\n",
    "                    })\n",
    "                else:\n",
    "                    preds[layer_idx]['incorrect'].append({\n",
    "                        'pred': model.tokenizer.decode(out[j]).replace(\"<|eot_id|>\", \"\").replace(\"<|begin_of_text|>\", \"\").strip(),\n",
    "                        'ground_truth': alt_ans[j],\n",
    "                    })\n",
    "                total += 1\n",
    "\n",
    "            del alt_acts, alt_prompt, org_prompt, alt_ans, out\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Validation accuracy: {correct / total:.2f} | Correct: {correct} | Total: {total}\\n\")\n",
    "        valid_accs_dcm[layer_idx] = round(correct / total, 2)\n",
    "    \n",
    "    with open(f\"../predictions/bigtom/SVs/{layer_idx}.json\", \"w\") as f:\n",
    "        json.dump(preds[layer_idx], f, indent=4)\n",
    "    \n",
    "    # Save valid_accs and rank_dcm as json file\n",
    "    with open(f\"../valid_accs_dcm.json\", \"w\") as f:\n",
    "        json.dump(valid_accs_dcm, f, indent=4)\n",
    "    with open(f\"../rank_dcm.json\", \"w\") as f:\n",
    "        json.dump(rank_dcm, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({32: 0.78, 26: 0.47, 28: 0.8, 30: 0.9}, {32: 138, 26: 209, 28: 140, 30: 130})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_accs_dcm, rank_dcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({30: 0.7, 32: 0.8, 28: 0.5}, {30: 81, 32: 86, 28: 74})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_accs_dcm, rank_dcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 78\n"
     ]
    }
   ],
   "source": [
    "mask_data = mask.data.clone()\n",
    "mask_data.clamp_(0, 1)\n",
    "rounded = torch.round(mask)\n",
    "print(f\"Rank: {(rounded == 1).sum().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation started for layer: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No'.\n",
      "Ground truth: is aimed at the correct location\n",
      "Prediction:  shifted<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n",
      "Answer: No<|eot_id|>\n",
      "\n",
      "<|begin_of_text|>Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No'.\n",
      "Ground truth: set at the correct temperature for baking biscotti\n",
      "Prediction:  Correct temperature<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n",
      "Answer: Yes<|eot_id|>\n",
      "\n",
      "<|begin_of_text|>Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No'.\n",
      "Ground truth: ripe and ready to be picked\n",
      "Prediction:  ripe and ready to be picked<|eot_id|>\n",
      "Answer: Yes<|eot_id|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:29<04:27, 29.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No'.\n",
      "Ground truth: the fishing net is strong and without holes\n",
      "Prediction:  strong and without holes<|eot_id|><|eot_id|><|eot_id|>\n",
      "Answer: Yes<|eot_id|>\n",
      "\n",
      "<|begin_of_text|>Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No'.\n",
      "Ground truth: the ingredients are nibbled on and no longer fresh\n",
      "Prediction:  nibbled on and no longer fresh<|eot_id|>\n",
      "Answer: Yes<|eot_id|>\n",
      "\n",
      "<|begin_of_text|>Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No'.\n",
      "Ground truth: wilted and less ideal\n",
      "Prediction:  wilted<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n",
      "Answer: No<|eot_id|>\n",
      "\n",
      "<|begin_of_text|>Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No'.\n",
      "Ground truth: fish have scattered due to the volcanic eruption\n",
      "Prediction:  Fish have scattered.<|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n",
      "Answer: Yes<|eot_id|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:00<04:02, 30.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No'.\n",
      "Ground truth: waters near the shore are calm and full of fish\n",
      "Prediction:  Ravi believes the fish have frantically\n",
      "Answer: No<|eot_id|>\n",
      "\n",
      "<|begin_of_text|>Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No'.\n",
      "Ground truth: is strong and ready to be used\n",
      "Prediction:  strong and ready<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n",
      "Answer: Yes<|eot_id|>\n",
      "\n",
      "<|begin_of_text|>Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No'.\n",
      "Ground truth: devoid of fish due to the volcanic eruption\n",
      "Prediction:  Devoid of fish.<|eot_id|><|eot_id|><|eot_id|>\n",
      "Answer: Yes<|eot_id|>\n",
      "\n",
      "<|begin_of_text|>Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No'.\n",
      "Ground truth: torn by the dolphins\n",
      "Prediction:  torn by the dolphins<|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n",
      "Answer: Yes<|eot_id|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:28<03:25, 29.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No'.\n",
      "Ground truth: is sturdy and suitable for carving\n",
      "Prediction:  Kwame believes the block of wood contains\n",
      "Answer: No<|eot_id|>\n",
      "\n",
      "<|begin_of_text|>Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No'.\n",
      "Ground truth: blue glaze\n",
      "Prediction:  green glaze<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n",
      "Answer: No<|eot_id|>\n",
      "\n",
      "<|begin_of_text|>Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No'.\n",
      "Ground truth: vinegar\n",
      "Prediction:  Vinegar<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n",
      "Answer: Yes<|eot_id|>\n",
      "\n",
      "<|begin_of_text|>Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No'.\n",
      "Ground truth: a beginner's violin\n",
      "Prediction:  beginner's violin<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n",
      "Answer: Yes<|eot_id|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:56<02:53, 28.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No'.\n",
      "Ground truth: is free from cracks\n",
      "Prediction:  free from cracks (initially), has\n",
      "Answer: Yes<|eot_id|>\n",
      "\n",
      "<|begin_of_text|>Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No'.\n",
      "Ground truth: clean and ready for use\n",
      "Prediction:  clean and ready for use<|eot_id|>\n",
      "Answer: Yes<|eot_id|>\n",
      "\n",
      "<|begin_of_text|>Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No'.\n",
      "Ground truth: filled with spilled chili oil\n",
      "Prediction:  filled with spilled chili oil<|eot_id|>\n",
      "Answer: Yes<|eot_id|>\n",
      "\n",
      "<|begin_of_text|>Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No'.\n",
      "Ground truth: clean and ready for use\n",
      "Prediction:  clean and ready for use<|eot_id|>\n",
      "Answer: Yes<|eot_id|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [02:23<02:20, 28.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No'.\n",
      "Ground truth: intact\n",
      "Prediction:  Intact<|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n",
      "Answer: Yes<|eot_id|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [02:37<02:37, 31.56s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m org_ques_idx \u001b[38;5;241m=\u001b[39m get_ques_start_token_idx(batch_size, model\u001b[38;5;241m.\u001b[39mtokenizer, org_prompt, padding_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m org_prompt_len \u001b[38;5;241m=\u001b[39m get_prompt_token_len(model\u001b[38;5;241m.\u001b[39mtokenizer, org_prompt, padding_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39msession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m     30\u001b[0m     alt_acts \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrace(alt_prompt):\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/intervention/contexts/session.py:37\u001b[0m, in \u001b[0;36mSession.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/tracing/contexts/tracer.py:25\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglobals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalTracingContext\n\u001b[1;32m     23\u001b[0m GlobalTracingContext\u001b[38;5;241m.\u001b[39mtry_deregister(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/tracing/contexts/base.py:82\u001b[0m, in \u001b[0;36mContext.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     78\u001b[0m graph \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mstack\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m     80\u001b[0m graph\u001b[38;5;241m.\u001b[39malive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/tracing/backends/base.py:25\u001b[0m, in \u001b[0;36mExecutionBackend.__call__\u001b[0;34m(self, graph)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph: Graph) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minjection:\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontexts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Context\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/tracing/graph/node.py:289\u001b[0m, in \u001b[0;36mNode.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget, Protocol):\n\u001b[0;32m--> 289\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m         \u001b[38;5;66;03m# Prepare arguments.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m         args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs))\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/tracing/contexts/base.py:93\u001b[0m, in \u001b[0;36mContext.execute\u001b[0;34m(cls, node)\u001b[0m\n\u001b[1;32m     90\u001b[0m graph: GraphType \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     92\u001b[0m graph\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 93\u001b[0m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m node\u001b[38;5;241m.\u001b[39mset_value(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/tracing/graph/graph.py:76\u001b[0m, in \u001b[0;36mGraph.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m         \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m NNsightError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     78\u001b[0m         err \u001b[38;5;241m=\u001b[39m (node\u001b[38;5;241m.\u001b[39mindex, e)\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/tracing/graph/node.py:289\u001b[0m, in \u001b[0;36mNode.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget, Protocol):\n\u001b[0;32m--> 289\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m         \u001b[38;5;66;03m# Prepare arguments.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m         args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs))\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/intervention/contexts/interleaving.py:161\u001b[0m, in \u001b[0;36mInterleavingTracer.execute\u001b[0;34m(cls, node)\u001b[0m\n\u001b[1;32m    157\u001b[0m graph\u001b[38;5;241m.\u001b[39mexecute()\n\u001b[1;32m    159\u001b[0m interleaver \u001b[38;5;241m=\u001b[39m Interleaver(graph, batch_groups\u001b[38;5;241m=\u001b[39mbatch_groups)\n\u001b[0;32m--> 161\u001b[0m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterleaver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minvoker_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minvoker_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m graph\u001b[38;5;241m.\u001b[39mcleanup()\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/modeling/mixins/meta.py:51\u001b[0m, in \u001b[0;36mMetaMixin.interleave\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatched:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch()\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterleave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/intervention/base.py:340\u001b[0m, in \u001b[0;36mNNsight.interleave\u001b[0;34m(self, interleaver, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn)\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m interleaver:\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/modeling/language.py:315\u001b[0m, in \u001b[0;36mLanguageModel._generate\u001b[0;34m(self, inputs, max_new_tokens, streamer, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m     streamer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39mstreamer\n\u001b[1;32m    313\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 315\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(output)\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/generation/utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2252\u001b[0m     )\n\u001b[1;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2275\u001b[0m     )\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/generation/utils.py:3254\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3251\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3254\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3255\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1793\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1790\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1791\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1793\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1798\u001b[0m     ):\n\u001b[1;32m   1799\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:834\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    831\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    833\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 834\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    848\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    849\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1793\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1790\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1791\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1793\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1798\u001b[0m     ):\n\u001b[1;32m   1799\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:592\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    581\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    582\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m         position_embeddings,\n\u001b[1;32m    590\u001b[0m     )\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 592\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1793\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1790\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1791\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1793\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1798\u001b[0m     ):\n\u001b[1;32m   1799\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:335\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 335\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1793\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1790\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1791\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1793\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1798\u001b[0m     ):\n\u001b[1;32m   1799\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:274\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    273\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m--> 274\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer_idx = 32\n",
    "print(f\"Validation started for layer: {layer_idx}\")\n",
    "correct, total = 0, 0\n",
    "model.tokenizer.padding_side = \"left\"\n",
    "preds = {layer_idx: {\"correct\": [], \"incorrect\": []}}\n",
    "\n",
    "with torch.inference_mode():\n",
    "    # mask_data = mask.data.clone()\n",
    "    # mask_data.clamp_(0, 1)\n",
    "    # rounded = torch.round(mask)\n",
    "    # print(f\"#Causal SVs: {(rounded == 1).sum().item()}\")\n",
    "    # rank_dcm[layer_idx] = (rounded == 1).sum().item()\n",
    "\n",
    "    # Save the mask\n",
    "    # torch.save(mask_data, f\"../masks/bigtom/{layer_idx}.pt\")\n",
    "\n",
    "    for bi, batch in tqdm(enumerate(valid_dataloader), total=len(valid_dataloader)):\n",
    "        alt_prompt = batch[\"alt_prompt\"]\n",
    "        org_prompt = batch[\"org_prompt\"]\n",
    "        alt_ans = batch[\"alt_ans\"]\n",
    "        batch_size = len(alt_ans)\n",
    "\n",
    "        alt_ques_idx = get_ques_start_token_idx(batch_size, model.tokenizer, alt_prompt, padding_side=\"left\")\n",
    "        alt_prompt_len = get_prompt_token_len(model.tokenizer, alt_prompt, padding_side=\"left\")\n",
    "        org_ques_idx = get_ques_start_token_idx(batch_size, model.tokenizer, org_prompt, padding_side=\"left\")\n",
    "        org_prompt_len = get_prompt_token_len(model.tokenizer, org_prompt, padding_side=\"left\")\n",
    "\n",
    "        with model.session() as session:\n",
    "\n",
    "            alt_acts = defaultdict(dict)\n",
    "            with model.trace(alt_prompt):\n",
    "                for j in range(batch_size):\n",
    "                    alt_acts[j] = model.model.layers[layer_idx].output[0][j, alt_ques_idx[j]:].save()\n",
    "\n",
    "            with model.generate(org_prompt, max_new_tokens=8, do_sample=False, num_return_sequences=1, pad_token_id=model.tokenizer.pad_token_id, eos_token_id=model.tokenizer.eos_token_id):\n",
    "                # sing_vec = sing_vecs[layer_idx].cuda()\n",
    "                # masked_vec = sing_vec.to(rounded.device) * rounded.unsqueeze(-1)\n",
    "                # proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "\n",
    "                sing_vec = sing_vecs[layer_idx][:125, :].cuda()\n",
    "                proj_matrix = torch.matmul(sing_vec.t(), sing_vec).half()\n",
    "\n",
    "                for j in range(batch_size):\n",
    "                    curr_output = model.model.layers[layer_idx].output[0][j, org_ques_idx[j]:].clone()\n",
    "\n",
    "                    alt_proj = torch.matmul(alt_acts[j], proj_matrix)\n",
    "                    org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "\n",
    "                    modified_out = curr_output - org_proj + alt_proj\n",
    "                    model.model.layers[layer_idx].output[0][j, org_ques_idx[j]:] = modified_out\n",
    "                    # model.model.layers[layer_idx].output[0][j, org_ques_idx[j]:] = alt_acts[j]\n",
    "\n",
    "                out = model.generator.output.save()\n",
    "\n",
    "                del sing_vec, proj_matrix\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            check_out, prompt_len = check_pred(model.tokenizer.decode(out[j, max(org_prompt_len):]), alt_ans[j])\n",
    "            check = model.tokenizer.decode(check_out[0, prompt_len:-1]).strip()\n",
    "            print(model.tokenizer.decode(check_out[0]).strip()+'\\n')\n",
    "\n",
    "            if check == \"Yes\":\n",
    "                correct += 1\n",
    "                preds[layer_idx]['correct'].append(model.tokenizer.decode(out[j]).replace(\"<|eot_id|>\", \"\").replace(\"<|begin_of_text|>\", \"\").strip())\n",
    "            else:\n",
    "                preds[layer_idx]['incorrect'].append(model.tokenizer.decode(out[j]).replace(\"<|eot_id|>\", \"\").replace(\"<|begin_of_text|>\", \"\").strip())\n",
    "            total += 1\n",
    "\n",
    "        del alt_acts, alt_prompt, org_prompt, alt_ans, out\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Validation accuracy: {correct / total:.2f} | Correct: {correct} | Total: {total}\\n\")\n",
    "    # valid_accs_dcm[layer_idx] = round(correct / total, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 8)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NNsightError",
     "evalue": "Accessing value before it's been set.",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):",
      "  File \"/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/tracing/graph/node.py\", line 294, in execute",
      "    args, kwargs = self.prepare_inputs((self.args, self.kwargs))",
      "  File \"/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/intervention/graph/node.py\", line 72, in prepare_inputs",
      "    inputs = util.apply(",
      "  File \"/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/util.py\", line 44, in apply",
      "    return tuple([apply(_data, fn, cls, inplace=inplace) for _data in data])",
      "  File \"/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/util.py\", line 44, in <listcomp>",
      "    return tuple([apply(_data, fn, cls, inplace=inplace) for _data in data])",
      "  File \"/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/util.py\", line 39, in apply",
      "    data[idx] = apply(_data, fn, cls, inplace=inplace)",
      "  File \"/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/util.py\", line 32, in apply",
      "    return fn(data)",
      "  File \"/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/intervention/graph/node.py\", line 65, in inner",
      "    value = value.value",
      "  File \"/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/tracing/graph/node.py\", line 143, in value",
      "    raise ValueError(\"Accessing value before it's been set.\")",
      "ValueError: Accessing value before it's been set.",
      "",
      "NNsightError: Accessing value before it's been set."
     ]
    }
   ],
   "source": [
    "model.tokenizer.decode(out[j]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'correct': [\"<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|begin_of_text|>Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\\n\\nStory: Emilia is a talented baker in a small Italian town. She plans to bake a large batch of biscotti to sell at the local market. Emilia inspects the oven's temperature, which appears to be set at the correct temperature for baking. A power surge causes the oven's temperature to spike, making it too hot for baking biscotti. Emilia notices the oven's temperature increase.\\nQuestion: Does Emilia believe the oven is set at the correct temperature for baking biscotti or too hot?\\nAnswer: Correct temperature<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\",\n",
       "  \"<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|begin_of_text|>Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\\n\\nStory: Mei is a talented chef in a small Chinese village. She wants to prepare a special dish using fresh vegetables from her garden. Mei sees that the vegetables in the garden are ripe and ready to be picked. Later, a hailstorm damages the garden, bruising and damaging the vegetables. Mei sees the hailstorm and the damage it caused to the vegetables.\\nQuestion: Does Mei believe the vegetables are ripe and ready to be picked or damaged by the hailstorm?\\nAnswer: ripe and ready to be picked<|eot_id|>\",\n",
       "  \"<|begin_of_text|>Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\\n\\nStory: Emeka is a skilled fisherman in a small Nigerian village. He wants to catch fish to feed his family and sell at the market. Emeka prepares his fishing net, which appears to be strong and without any holes. However, while Emeka is not looking, a mischievous child cuts a large hole in the net with a knife. Emeka sees the child cutting the hole in the fishing net.\\nQuestion: Does Emeka believe the fishing net is strong and without holes, or that it has a large hole in it?\\nAnswer: strong and without holes<|eot_id|><|eot_id|><|eot_id|>\",\n",
       "  \"<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|begin_of_text|>Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\\n\\nStory: Aria is a talented baker from Italy who has been asked to bake a cake for a friend's birthday party. She wants to create a delicious, multi-layered cake that will impress everyone. Aria has a fresh batch of ingredients ready to use for the cake. However, a mischievous cat sneaks into her kitchen and nibbles on some of the ingredients, making them no longer fresh. Aria does not notice the cat or the nibbled ingredients.\\nQuestion: Does Aria believe her ingredients are fresh and suitable or nibbled on and no longer fresh?\\nAnswer: nibbled on and no longer fresh<|eot_id|>\",\n",
       "  \"<|begin_of_text|>Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\\n\\nStory: Takeshi is a fisherman in a small Japanese coastal town. Takeshi wants to catch a large number of fish to sell at the market and provide for his family. He notices that the sea is calm and the fish are abundant, making it a perfect day for fishing. However, a sudden underwater volcanic eruption nearby disrupts the marine life, causing the fish to scatter and flee the area. Takeshi does not sense the disturbance in the water caused by the volcanic eruption.\\nQuestion: Does Takeshi believe that the sea is calm and full of fish, or that the fish have scattered due to the volcanic eruption?\\nAnswer: Fish have scattered.<|eot_id|><|eot_id|><|eot_id|><|eot_id|>\"],\n",
       " 'incorrect': [\"<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|begin_of_text|>Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\\n\\nStory: Naledi is a young astronomer in a remote observatory in South Africa. Naledi wants to observe a distant supernova to gather data for her research. The telescope is pointed at the coordinates of the supernova, ready for observation. A minor earthquake occurs, causing the telescope to shift its position slightly. Naledi notices the earthquake and its effect on the telescope.\\nQuestion: Does Naledi believe the telescope is aimed at the correct location or has shifted due to the earthquake?\\nAnswer: shifted<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\",\n",
       "  \"<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|begin_of_text|>Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\\n\\nStory: Ailani is a skilled Hawaiian chef preparing a traditional luau feast for her community. She wants to make a delicious laulau dish using fresh taro leaves. Ailani examines the taro leaves, which look fresh and green. However, a sudden downpour soaks the taro leaves, causing them to become wilted and less ideal for the dish. Ailani does not notice the downpour and the effect it has on the taro leaves.\\nQuestion: Does Ailani believe the taro leaves are fresh and of high quality or wilted and less ideal?\\nAnswer: wilted<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\",\n",
       "  \"<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|begin_of_text|>Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\\n\\nStory: Ravi is a fisherman living in a coastal village in India. Ravi wants to catch a large amount of fish to sell at the market and provide for his family. The waters near the shore are calm and full of fish, making it an ideal location for fishing. An underwater earthquake occurs, causing the fish to frantically swim away from the coast. Ravi senses the earthquake and notices the sudden decrease in fish.\\nQuestion: Does Ravi believe the waters near the shore are calm and full of fish, or that the fish have frantically swum away?\\nAnswer: Ravi believes the fish have frantically\"]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "true_stories = [\n",
    "    {\n",
    "        \"story\": train_dataset[0][\"alt_story\"],\n",
    "        \"question\": train_dataset[0][\"alt_question\"],\n",
    "        \"answer\": train_dataset[0][\"alt_ans\"],\n",
    "    },\n",
    "    {\n",
    "        \"story\": train_dataset[0][\"org_story\"],\n",
    "        \"question\": train_dataset[0][\"org_question\"],\n",
    "        \"answer\": train_dataset[0][\"org_ans\"],\n",
    "    }\n",
    "]\n",
    "\n",
    "# arrows = [{'start': token_pos_coords['e1_query_charac'], 'end': token_pos_coords['e2_query_charac'], 'color': 'red'}]\n",
    "\n",
    "plot_data = {\n",
    "    \"labels\": valid_accs_2.keys(),\n",
    "    \"acc_one_layer\": valid_accs_2.values(),\n",
    "    \"title\": \"Aligning Visibility using Resid Vector\",\n",
    "    \"x_label\": \"Layers\",\n",
    "    \"y_label\": \"Intervention Accuracy\",\n",
    "}\n",
    "\n",
    "# characters = list(set(train_dataset[0]['clean_characters'] + train_dataset[0]['corrupt_characters']))\n",
    "# objects = list(set(train_dataset[0]['clean_objects'] + train_dataset[0]['corrupt_objects']))\n",
    "# states = list(set(train_dataset[0]['clean_states'] + train_dataset[0]['corrupt_states']))\n",
    "\n",
    "generator = StoryGenerator(characters=['Noor'], objects=['pitcher'], states=['oat', 'almond'], stories=true_stories, target=train_dataset[0]['alt_ans'], arrows=[], plot_data=plot_data)\n",
    "generator.save_html(filename=\"../plots/visibility_exps/bigtom/both_direc_resid_vector.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
