{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import Any, List, Optional\n",
    "import nnsight\n",
    "from nnsight import CONFIG, LanguageModel\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from einops import einsum\n",
    "import time\n",
    "from einops import rearrange, reduce\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from src.dataset import SampleV3, DatasetV3, STORY_TEMPLATES\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "random.seed(10)\n",
    "\n",
    "CONFIG.set_default_api_key(\"d9e00ab7d4f74643b3176de0913f24a7\")\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "CONFIG.APP.REMOTE_LOGGING = False\n",
    "\n",
    "# Define random seed\n",
    "seed = 10\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [00:37<00:00,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# model = LanguageModel(\"meta-llama/Meta-Llama-3.1-405B\")\n",
    "model = LanguageModel(\"meta-llama/Meta-Llama-3-70B-Instruct\", cache_dir=\"/disk/u/nikhil/.cache/huggingface/hub/\", device_map=\"auto\", load_in_4bit=True, torch_dtype=torch.float16, dispatch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ques_start_token_idx(batch_size, tokenizer, prompt, padding_side=\"right\"):\n",
    "    input_tokens = tokenizer(prompt, return_tensors=\"pt\", padding=True, padding_side=padding_side).input_ids\n",
    "    colon_token = tokenizer.encode(\":\", return_tensors=\"pt\").squeeze()[-1].item()\n",
    "    ques_start_idx = (input_tokens == colon_token).nonzero()[torch.arange(2, 4*batch_size, 4)][:, 1] - 1\n",
    "\n",
    "    return ques_start_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_token_len(tokenizer, prompt, padding_side=\"right\"):\n",
    "    input_tokens = tokenizer(prompt, return_tensors=\"pt\", padding=True, padding_side=padding_side)\n",
    "    return input_tokens.attention_mask.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pred(pred, target, verbose=False):\n",
    "    prompt = f\"Instruction: Check if the following ground truth and prediction are similar or not. If they are the same, then predict 'Yes', else 'No'.\\nGround truth: {target}\\nPrediction: {pred}\\nAnswer:\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with model.generate(prompt, max_new_tokens=5, do_sample=False, num_return_sequences=1, pad_token_id=model.tokenizer.pad_token_id):\n",
    "            out = model.generator.output.save()\n",
    "\n",
    "    prompt_len = get_prompt_token_len(model.tokenizer, prompt)\n",
    "    \n",
    "    if verbose:\n",
    "        print(model.tokenizer.decode(out[0]))\n",
    "\n",
    "    return out, prompt_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading BigToM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a csv file\n",
    "df_false = pd.read_csv(\"../data/bigtom/0_forward_belief_false_belief/stories.csv\", delimiter=\";\")\n",
    "df_true = pd.read_csv(\"../data/bigtom/0_forward_belief_true_belief/stories.csv\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sing_vecs = defaultdict(dict)\n",
    "for l in range(model.config.num_hidden_layers):\n",
    "    sing_vecs[l] = torch.load(f\"../svd_results/BigToM/last_token/singular_vecs/{l}.pt\").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 80\n",
    "valid_size = 40\n",
    "batch_size = 1\n",
    "\n",
    "dataset = get_bigtom_value_fetcher_exps(df_false, df_true, train_size+valid_size)\n",
    "train_dataset = dataset[:train_size]\n",
    "valid_dataset = dataset[train_size:]\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\n",
      "\n",
      "Story: Fatima is a talented painter who has been commissioned to create a mural on a local community center's wall. She wants to choose the perfect color palette to represent the vibrant energy of the community. Fatima notices that the paint colors she has selected are bright and lively. However, while she is away for lunch, a mischievous cat knocks over the paint cans, causing the colors to mix into dull, muted shades. Fatima sees the paint cans knocked over and the colors mixed.\n",
      "Question: Does Fatima believe the paint colors are bright and lively or dull and muted?\n",
      "Answer: now dull and muted\n",
      "Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\n",
      "\n",
      "Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor sees her coworker swapping the milk.\n",
      "Question: Does Noor believe the milk pitcher contains oat milk or almond milk?\n",
      "Answer: almond milk\n",
      "Target: now dull and muted\n"
     ]
    }
   ],
   "source": [
    "idx=0\n",
    "print(dataset[idx]['alt_prompt'], dataset[idx]['alt_ans'])\n",
    "print(dataset[idx]['org_prompt'], dataset[idx]['org_ans'])\n",
    "print(f\"Target: {dataset[idx]['target']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer: 70, lambda: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1/80 [00:04<06:16,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Task Loss: -38.0938, L1 Loss: 20.0000, Total Loss: -18.0938\n",
      "#Rank: 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 5/80 [00:25<06:31,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 4, Task Loss: -30.2344, L1 Loss: 16.1250, Total Loss: -18.0219\n",
      "#Rank: 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 9/80 [00:46<06:08,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 8, Task Loss: -56.2500, L1 Loss: 13.6875, Total Loss: -26.2118\n",
      "#Rank: 274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 13/80 [01:07<05:51,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 12, Task Loss: -32.6875, L1 Loss: 11.3750, Total Loss: -22.4507\n",
      "#Rank: 234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 17/80 [01:27<05:22,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 16, Task Loss: -30.1250, L1 Loss: 10.2500, Total Loss: -20.2206\n",
      "#Rank: 213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 21/80 [01:48<05:05,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 20, Task Loss: -18.6250, L1 Loss: 9.4375, Total Loss: -19.9182\n",
      "#Rank: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 25/80 [02:09<04:49,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 24, Task Loss: -16.7812, L1 Loss: 8.8125, Total Loss: -19.1106\n",
      "#Rank: 183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 29/80 [02:30<04:28,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 28, Task Loss: -70.0000, L1 Loss: 8.3750, Total Loss: -19.9065\n",
      "#Rank: 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 33/80 [02:51<04:07,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 32, Task Loss: -30.3750, L1 Loss: 8.0625, Total Loss: -21.6357\n",
      "#Rank: 167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 37/80 [03:12<03:40,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 36, Task Loss: -44.8750, L1 Loss: 7.8438, Total Loss: -24.0990\n",
      "#Rank: 161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 41/80 [03:32<03:15,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 40, Task Loss: -35.5312, L1 Loss: 7.6875, Total Loss: -25.6317\n",
      "#Rank: 153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 45/80 [03:53<03:03,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 44, Task Loss: -32.8750, L1 Loss: 7.5625, Total Loss: -25.0398\n",
      "#Rank: 152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 49/80 [04:14<02:44,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 48, Task Loss: -31.3281, L1 Loss: 7.4062, Total Loss: -24.5120\n",
      "#Rank: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 53/80 [04:35<02:19,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 52, Task Loss: -20.9375, L1 Loss: 7.4062, Total Loss: -24.6826\n",
      "#Rank: 152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 57/80 [04:56<01:57,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 56, Task Loss: -22.5625, L1 Loss: 7.4062, Total Loss: -24.8930\n",
      "#Rank: 154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 61/80 [05:17<01:39,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 60, Task Loss: -41.3750, L1 Loss: 7.4062, Total Loss: -25.8090\n",
      "#Rank: 154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 65/80 [05:38<01:19,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 64, Task Loss: -33.0625, L1 Loss: 7.4375, Total Loss: -25.8843\n",
      "#Rank: 154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 69/80 [05:59<00:58,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 68, Task Loss: -16.5156, L1 Loss: 7.4062, Total Loss: -25.3851\n",
      "#Rank: 154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 73/80 [06:20<00:36,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 72, Task Loss: -15.8750, L1 Loss: 7.3438, Total Loss: -25.1125\n",
      "#Rank: 152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 77/80 [06:41<00:15,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 76, Task Loss: -44.6250, L1 Loss: 7.2500, Total Loss: -25.8136\n",
      "#Rank: 151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [06:57<00:00,  5.22s/it]\n",
      "  1%|▏         | 1/80 [00:05<07:01,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 0, Task Loss: -36.1250, L1 Loss: 7.1562, Total Loss: -28.9688\n",
      "#Rank: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 5/80 [00:26<06:33,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 4, Task Loss: -29.3594, L1 Loss: 7.0938, Total Loss: -26.2719\n",
      "#Rank: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 9/80 [00:46<06:07,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 8, Task Loss: -59.0000, L1 Loss: 7.1562, Total Loss: -35.1337\n",
      "#Rank: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 13/80 [01:07<05:51,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 12, Task Loss: -30.9375, L1 Loss: 7.0938, Total Loss: -29.9724\n",
      "#Rank: 147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 17/80 [01:28<05:25,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 16, Task Loss: -31.1406, L1 Loss: 7.0938, Total Loss: -26.8722\n",
      "#Rank: 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 21/80 [01:48<05:06,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 20, Task Loss: -19.0312, L1 Loss: 7.0625, Total Loss: -26.0424\n",
      "#Rank: 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 25/80 [02:10<04:50,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 24, Task Loss: -17.3750, L1 Loss: 7.0000, Total Loss: -24.7078\n",
      "#Rank: 145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 29/80 [02:31<04:28,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 28, Task Loss: -69.1250, L1 Loss: 7.0000, Total Loss: -25.0482\n",
      "#Rank: 144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 33/80 [02:52<04:07,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 32, Task Loss: -30.5469, L1 Loss: 7.0000, Total Loss: -26.3857\n",
      "#Rank: 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 37/80 [03:13<03:41,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 36, Task Loss: -44.2500, L1 Loss: 7.0000, Total Loss: -28.5353\n",
      "#Rank: 144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 41/80 [03:33<03:16,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 40, Task Loss: -35.9375, L1 Loss: 7.0000, Total Loss: -29.8306\n",
      "#Rank: 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 45/80 [03:54<03:03,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 44, Task Loss: -32.3125, L1 Loss: 6.9375, Total Loss: -28.9106\n",
      "#Rank: 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 49/80 [04:15<02:43,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 48, Task Loss: -31.7500, L1 Loss: 6.9062, Total Loss: -28.1143\n",
      "#Rank: 145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 53/80 [04:36<02:19,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 52, Task Loss: -21.1875, L1 Loss: 6.9062, Total Loss: -28.1263\n",
      "#Rank: 143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 57/80 [04:56<01:58,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 56, Task Loss: -22.2500, L1 Loss: 6.9062, Total Loss: -28.1575\n",
      "#Rank: 143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 61/80 [05:18<01:39,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 60, Task Loss: -41.6875, L1 Loss: 6.9375, Total Loss: -28.8836\n",
      "#Rank: 143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 65/80 [05:39<01:19,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 64, Task Loss: -32.9375, L1 Loss: 7.0000, Total Loss: -28.7963\n",
      "#Rank: 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 69/80 [06:00<00:58,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 68, Task Loss: -16.1406, L1 Loss: 7.0000, Total Loss: -28.1541\n",
      "#Rank: 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 73/80 [06:21<00:37,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 72, Task Loss: -16.0469, L1 Loss: 6.9375, Total Loss: -27.7548\n",
      "#Rank: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 77/80 [06:42<00:15,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 76, Task Loss: -44.5000, L1 Loss: 6.8438, Total Loss: -28.3524\n",
      "#Rank: 139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [06:58<00:00,  5.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for layer: 70, lambda: 0.05\n",
      "Validation started for layer: 70, lambda: 0.05\n",
      "Rank: 139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1/40 [00:05<03:44,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: N | Target: murky and polluted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [00:11<03:40,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Frag | Target: fragile branches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/40 [00:17<03:35,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Healthy | Target: healthy state\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [00:23<03:31,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: wilt | Target: wilted and less ideal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:29<03:26,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Dis | Target: disrupted by fallen leaves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [00:35<03:21,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: hot | Target: hot sauce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 7/40 [00:41<03:15,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Takes | Target: nearly empty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 8/40 [00:47<03:10,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: with | Target: withered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 9/40 [00:53<03:03,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Frag | Target: fragile branches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 10/40 [00:59<02:57,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Wet | Target: wet and difficult to ignite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 11/40 [01:04<02:51,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: dry | Target: dry and stale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 12/40 [01:10<02:45,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: St | Target: stained and damaged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 13/40 [01:16<02:40,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Green | Target: water\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 14/40 [01:22<02:34,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  | Target: 18°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 15/40 [01:28<02:27,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Off | Target: turned off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 16/40 [01:34<02:21,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: wilt | Target: wilted flowers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 17/40 [01:40<02:15,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Bur | Target: buried under the sand\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 18/40 [01:46<02:10,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: hard | Target: hard and brittle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 19/40 [01:52<02:04,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: washed | Target: washed away and diluted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 20/40 [01:58<01:59,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Enrique | Target: sultanas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 21/40 [02:04<01:52,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Frag | Target: fragile branches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 22/40 [02:10<01:45,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: torn | Target: torn apart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 23/40 [02:15<01:40,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: The | Target: fox has stolen the eggs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 24/40 [02:21<01:34,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: K | Target: Mount Fuji is covered by fog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 25/40 [02:27<01:28,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: flour | Target: flour\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 26/40 [02:33<01:22,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Am | Target: very high temperature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 27/40 [02:39<01:16,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Dam | Target: damaged by the monkey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 28/40 [02:45<01:10,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: severely | Target: severely damaged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 29/40 [02:51<01:04,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Dr | Target: a state of disrepair\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 30/40 [02:56<00:58,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: rough | Target: rough and choppy due to the storm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 31/40 [03:02<00:52,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Has | Target: has shifted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 32/40 [03:08<00:46,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: She | Target: knows about the concealed door\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 33/40 [03:14<00:40,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Fl | Target: been flattened\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 34/40 [03:20<00:35,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Ex | Target: affected by the power outage and has cooled down\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 35/40 [03:26<00:29,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Not | Target: has a broken string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 36/40 [03:31<00:23,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: wilt | Target: wilted and less ideal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 37/40 [03:37<00:17,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Bian | Target: almond milk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 38/40 [03:43<00:11,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Coll | Target: has collapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 39/40 [03:49<00:05,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: wet | Target: wet and wilted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [03:55<00:00,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Ol | Target: underripe\n",
      "Validation accuracy: 0.72 | Correct: 29 | Total: 40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.tokenizer.padding_side = \"left\"\n",
    "\n",
    "valid_accs = defaultdict(dict)\n",
    "for layer_idx in range(70, 72, 2):\n",
    "\n",
    "    n_epochs = 2\n",
    "    lambs = [0.05]\n",
    "    for lamb in lambs:\n",
    "        modules = [i for i in range(sing_vecs[layer_idx].size(0))]\n",
    "        mask = torch.ones(len(modules), requires_grad=True, device=\"cuda\", dtype=torch.bfloat16)\n",
    "        optimizer = torch.optim.Adam([mask], lr=1e-1)\n",
    "\n",
    "        print(f\"Training layer: {layer_idx}, lambda: {lamb}\")\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0\n",
    "\n",
    "            for bi, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "                alt_prompt = batch[\"alt_prompt\"]\n",
    "                org_prompt = batch[\"org_prompt\"]\n",
    "                target = batch[\"target\"]\n",
    "                target_token = model.tokenizer(target, return_tensors=\"pt\", padding=True, padding_side=\"right\")\n",
    "                target_input_ids = target_token.input_ids[:, 1:]\n",
    "                batch_size = target_input_ids.size(0)\n",
    "\n",
    "                alt_ques_idx = get_ques_start_token_idx(batch_size, model.tokenizer, alt_prompt, padding_side=\"right\")\n",
    "                alt_prompt_len = get_prompt_token_len(model.tokenizer, alt_prompt, padding_side=\"right\")\n",
    "                org_ques_idx = get_ques_start_token_idx(batch_size, model.tokenizer, org_prompt, padding_side=\"right\")\n",
    "                org_prompt_len = get_prompt_token_len(model.tokenizer, org_prompt, padding_side=\"right\")\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with model.trace() as tracer:\n",
    "\n",
    "                    with tracer.invoke(alt_prompt):\n",
    "                        alt_acts = model.model.layers[layer_idx].output[0][0, -1].clone().save()\n",
    "\n",
    "                    with tracer.invoke(org_prompt):\n",
    "                        sing_vec = sing_vecs[layer_idx].cuda()\n",
    "                        masked_vec = sing_vec * mask.unsqueeze(-1)\n",
    "                        proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "\n",
    "                        curr_output = model.model.layers[layer_idx].output[0][0, -1].clone()\n",
    "\n",
    "                        alt_proj = torch.matmul(alt_acts, proj_matrix)\n",
    "                        org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "\n",
    "                        modified_out = curr_output - org_proj + alt_proj\n",
    "                        model.model.layers[layer_idx].output[0][0, -1] = modified_out\n",
    "\n",
    "                        logits = model.lm_head.output[0, -1].save()\n",
    "\n",
    "                        del sing_vec, proj_matrix, masked_vec\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                target_logit = logits[target_input_ids[0]].sum()\n",
    "\n",
    "                task_loss = -(target_logit/batch_size)\n",
    "                l1_loss = lamb * torch.norm(mask, p=1)\n",
    "                loss = task_loss + l1_loss.to(task_loss.device)\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                if bi % 4 == 0:\n",
    "                    mean_loss = epoch_loss / (bi + 1)\n",
    "                    print(f\"Epoch: {epoch}, Batch: {bi}, Task Loss: {task_loss.item():.4f}, \"\n",
    "                        f\"L1 Loss: {l1_loss.item():.4f}, Total Loss: {mean_loss:.4f}\")\n",
    "                    with torch.no_grad():\n",
    "                        mask.data.clamp_(0, 1)\n",
    "                        rounded = torch.round(mask)\n",
    "                        print(f\"#Rank: {(rounded == 1).sum().item()}\")\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    mask.data.clamp_(0, 1)\n",
    "\n",
    "        print(f\"Training finished for layer: {layer_idx}, lambda: {lamb}\")\n",
    "\n",
    "        print(f\"Validation started for layer: {layer_idx}, lambda: {lamb}\")\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            mask_data = mask.data.clone()\n",
    "            mask_data.clamp_(0, 1)\n",
    "            rounded = torch.round(mask)\n",
    "\n",
    "            print(f\"Rank: {(rounded == 1).sum().item()}\")\n",
    "\n",
    "            # Save the mask\n",
    "            # torch.save(mask_data, f\"../masks/bigtom/{layer_idx}.pt\")\n",
    "\n",
    "            for bi, batch in tqdm(enumerate(valid_dataloader), total=len(valid_dataloader)):\n",
    "                alt_prompt = batch[\"alt_prompt\"]\n",
    "                org_prompt = batch[\"org_prompt\"]\n",
    "                alt_ans = batch[\"alt_ans\"]\n",
    "                target = batch[\"target\"][0]\n",
    "                batch_size = len(alt_ans)\n",
    "\n",
    "                alt_ques_idx = get_ques_start_token_idx(batch_size, model.tokenizer, alt_prompt, padding_side=\"left\")\n",
    "                alt_prompt_len = get_prompt_token_len(model.tokenizer, alt_prompt, padding_side=\"left\")\n",
    "                org_ques_idx = get_ques_start_token_idx(batch_size, model.tokenizer, org_prompt, padding_side=\"left\")\n",
    "                org_prompt_len = get_prompt_token_len(model.tokenizer, org_prompt, padding_side=\"left\")\n",
    "\n",
    "                with model.session() as session:\n",
    "\n",
    "                    with model.trace(alt_prompt):\n",
    "                        alt_acts = model.model.layers[layer_idx].output[0][0, -1].save()\n",
    "\n",
    "                    with model.generate(org_prompt, max_new_tokens=2, do_sample=False, num_return_sequences=1, pad_token_id=model.tokenizer.pad_token_id, eos_token_id=model.tokenizer.eos_token_id):\n",
    "                        sing_vec = sing_vecs[layer_idx].cuda()\n",
    "                        masked_vec = sing_vec * rounded.unsqueeze(-1)\n",
    "                        proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "\n",
    "                        curr_output = model.model.layers[layer_idx].output[0][0, -1].clone()\n",
    "\n",
    "                        alt_proj = torch.matmul(alt_acts, proj_matrix)\n",
    "                        org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "\n",
    "                        modified_out = curr_output - org_proj + alt_proj\n",
    "\n",
    "                        model.model.layers[layer_idx].output[0][0, -1] = modified_out\n",
    "\n",
    "                        out = model.generator.output.save()\n",
    "\n",
    "                    del alt_acts\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                pred = model.tokenizer.decode(out[0][org_prompt_len:-1]).strip()\n",
    "                print(f\"Prediction: {pred} | Target: {target}\")\n",
    "                if pred.lower() in target.lower():\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "            print(f\"Validation accuracy: {correct / total:.2f} | Correct: {correct} | Total: {total}\\n\")\n",
    "            valid_accs[lamb][layer_idx] = round(correct / total, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer State OID Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sing_vecs = defaultdict(dict)\n",
    "for l in range(model.config.num_hidden_layers):\n",
    "    sing_vecs[l] = torch.load(f\"../svd_results/BigToM/last_token/singular_vecs/{l}.pt\").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 80\n",
    "valid_size = 40\n",
    "batch_size = 1\n",
    "\n",
    "dataset = get_bigtom_answer_state_exps(df_false, df_true, train_size+valid_size)\n",
    "train_dataset = dataset[:train_size]\n",
    "valid_dataset = dataset[train_size:]\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\n",
      "\n",
      "Story: Amara is a botanist exploring a dense rainforest in search of a rare orchid species. Amara's goal is to find the rare orchid and study its unique characteristics. She spots an orchid with vibrant purple petals that she thinks might be the one she is searching for. As she continues to explore, a sudden downpour washes away the purple pigment from the orchid, revealing that it is actually a common white orchid. Amara does not witness the downpour and the color change in the orchid.\n",
      "Question: Does Amara believe the orchid with vibrant purple petals is the rare species or a common white orchid?\n",
      "Answer: with vibrant purple petals is the rare species\n",
      "Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\n",
      "\n",
      "Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor sees her coworker swapping the milk.\n",
      "Question: Does Noor believe the milk pitcher contains oat milk or almond milk?\n",
      "Answer: almond milk\n",
      "Target: oat milk\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(train_dataset[idx]['alt_prompt'], train_dataset[idx]['alt_ans'])\n",
    "print(train_dataset[idx]['org_prompt'], train_dataset[idx]['org_ans'])\n",
    "print(f\"Target: {train_dataset[idx]['target']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer: 40, lambda: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Task Loss: -18.4531, L1 Loss: 2.0000, Total Loss: -16.4531\n",
      "#Rank: 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4/80 [00:26<08:26,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 4, Task Loss: -39.3750, L1 Loss: 1.6719, Total Loss: -24.4047\n",
      "#Rank: 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 8/80 [00:53<07:56,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 8, Task Loss: -24.1562, L1 Loss: 1.5078, Total Loss: -24.4002\n",
      "#Rank: 327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 12/80 [01:20<07:40,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 12, Task Loss: -107.9375, L1 Loss: 1.3672, Total Loss: -30.2296\n",
      "#Rank: 288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 16/80 [01:48<07:22,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 16, Task Loss: -36.9375, L1 Loss: 1.2578, Total Loss: -32.8718\n",
      "#Rank: 269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 20/80 [02:16<06:54,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 20, Task Loss: -20.2344, L1 Loss: 1.1719, Total Loss: -32.0822\n",
      "#Rank: 247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 24/80 [02:44<06:30,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 24, Task Loss: -20.5625, L1 Loss: 1.1016, Total Loss: -30.5388\n",
      "#Rank: 224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 28/80 [03:12<06:03,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 28, Task Loss: -91.8750, L1 Loss: 1.0312, Total Loss: -32.5135\n",
      "#Rank: 217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 32/80 [03:40<05:34,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 32, Task Loss: -41.0000, L1 Loss: 0.9766, Total Loss: -35.4163\n",
      "#Rank: 199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 36/80 [04:07<05:01,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 36, Task Loss: -50.3125, L1 Loss: 0.9297, Total Loss: -35.6271\n",
      "#Rank: 196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 40/80 [04:34<04:29,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 40, Task Loss: -87.3750, L1 Loss: 0.8906, Total Loss: -38.2053\n",
      "#Rank: 185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 44/80 [05:01<04:05,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 44, Task Loss: -62.0625, L1 Loss: 0.8516, Total Loss: -39.9546\n",
      "#Rank: 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 48/80 [05:28<03:36,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 48, Task Loss: -30.7969, L1 Loss: 0.8164, Total Loss: -39.8522\n",
      "#Rank: 162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 52/80 [05:56<03:13,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 52, Task Loss: -76.1875, L1 Loss: 0.7891, Total Loss: -40.6981\n",
      "#Rank: 155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 56/80 [06:24<02:46,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 56, Task Loss: -33.0938, L1 Loss: 0.7695, Total Loss: -40.2714\n",
      "#Rank: 151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 60/80 [06:52<02:19,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 60, Task Loss: -110.0625, L1 Loss: 0.7539, Total Loss: -41.1557\n",
      "#Rank: 152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 64/80 [07:20<01:51,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 64, Task Loss: -24.4219, L1 Loss: 0.7344, Total Loss: -40.7297\n",
      "#Rank: 155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 68/80 [07:48<01:24,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 68, Task Loss: -44.5000, L1 Loss: 0.7148, Total Loss: -40.8648\n",
      "#Rank: 153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 72/80 [08:15<00:54,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 72, Task Loss: -56.7500, L1 Loss: 0.6992, Total Loss: -41.6181\n",
      "#Rank: 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 76/80 [08:42<00:27,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 76, Task Loss: -9.6875, L1 Loss: 0.6836, Total Loss: -41.6486\n",
      "#Rank: 139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [09:10<00:00,  6.89s/it]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 0, Task Loss: -18.6406, L1 Loss: 0.6719, Total Loss: -17.9688\n",
      "#Rank: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4/80 [00:27<08:34,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 4, Task Loss: -46.3125, L1 Loss: 0.6562, Total Loss: -28.9742\n",
      "#Rank: 138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 8/80 [00:54<07:54,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 8, Task Loss: -24.0625, L1 Loss: 0.6484, Total Loss: -27.9371\n",
      "#Rank: 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 12/80 [01:21<07:39,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 12, Task Loss: -110.5625, L1 Loss: 0.6445, Total Loss: -33.3098\n",
      "#Rank: 135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 16/80 [01:49<07:22,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 16, Task Loss: -37.2188, L1 Loss: 0.6367, Total Loss: -35.8511\n",
      "#Rank: 132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 20/80 [02:16<06:54,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 20, Task Loss: -19.8906, L1 Loss: 0.6289, Total Loss: -34.4278\n",
      "#Rank: 132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 24/80 [02:44<06:29,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 24, Task Loss: -19.9375, L1 Loss: 0.6133, Total Loss: -32.6528\n",
      "#Rank: 127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 28/80 [03:12<06:02,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 28, Task Loss: -95.8750, L1 Loss: 0.6133, Total Loss: -34.5940\n",
      "#Rank: 129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 32/80 [03:40<05:33,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 32, Task Loss: -41.2500, L1 Loss: 0.6094, Total Loss: -37.8800\n",
      "#Rank: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 36/80 [04:07<05:01,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 36, Task Loss: -54.4062, L1 Loss: 0.6094, Total Loss: -38.0631\n",
      "#Rank: 129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 40/80 [04:34<04:29,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 40, Task Loss: -89.8750, L1 Loss: 0.6094, Total Loss: -40.7811\n",
      "#Rank: 130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 44/80 [05:01<04:05,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 44, Task Loss: -63.6562, L1 Loss: 0.6094, Total Loss: -42.4852\n",
      "#Rank: 125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 48/80 [05:28<03:35,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 48, Task Loss: -30.5469, L1 Loss: 0.6055, Total Loss: -42.2420\n",
      "#Rank: 129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 52/80 [05:56<03:12,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 52, Task Loss: -76.0625, L1 Loss: 0.6016, Total Loss: -42.9844\n",
      "#Rank: 126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 56/80 [06:23<02:44,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 56, Task Loss: -33.4062, L1 Loss: 0.6016, Total Loss: -42.4169\n",
      "#Rank: 129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 60/80 [06:51<02:17,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 60, Task Loss: -111.5000, L1 Loss: 0.6016, Total Loss: -43.2264\n",
      "#Rank: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 64/80 [07:18<01:50,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 64, Task Loss: -24.4219, L1 Loss: 0.5938, Total Loss: -42.6788\n",
      "#Rank: 126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 68/80 [07:46<01:23,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 68, Task Loss: -44.4375, L1 Loss: 0.5859, Total Loss: -42.7212\n",
      "#Rank: 124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 72/80 [08:14<00:54,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 72, Task Loss: -57.1250, L1 Loss: 0.5820, Total Loss: -43.4340\n",
      "#Rank: 124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 76/80 [08:41<00:27,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 76, Task Loss: -9.6328, L1 Loss: 0.5820, Total Loss: -43.3821\n",
      "#Rank: 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [09:09<00:00,  6.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for layer: 40, lambda: 0.005\n",
      "Validation started for layer: 40, lambda: 0.005\n",
      "Rank: 121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1/40 [00:05<03:42,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Aim | Target: is aimed at the correct location\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [00:11<03:40,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: too | Target: set at the correct temperature for baking biscotti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/40 [00:17<03:34,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: R | Target: ripe and ready to be picked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [00:23<03:30,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Em | Target: the fishing net is strong and without holes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:29<03:25,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: fresh | Target: her ingredients are fresh and suitable for baking the cake\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [00:35<03:20,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Fresh | Target: fresh and of high quality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 7/40 [00:41<03:15,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: calm | Target: sea is calm and full of fish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 8/40 [00:46<03:08,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Cal | Target: waters near the shore are calm and full of fish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 9/40 [00:52<03:03,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: strong | Target: is strong and ready to be used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 10/40 [00:58<02:57,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Te | Target: teeming with fish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 11/40 [01:04<02:52,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: In | Target: in excellent condition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 12/40 [01:10<02:46,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Kw | Target: is sturdy and suitable for carving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 13/40 [01:16<02:40,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Green | Target: blue glaze\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 14/40 [01:22<02:33,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Vine | Target: vinegar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 15/40 [01:28<02:26,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: valuable | Target: her valuable violin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 16/40 [01:34<02:21,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: free | Target: is free from cracks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 17/40 [01:40<02:14,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: clean | Target: clean and ready for use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 18/40 [01:46<02:09,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: filled | Target: clean and in excellent condition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 19/40 [01:51<02:03,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: clean | Target: clean and ready for use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 20/40 [01:57<01:58,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Int | Target: intact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 21/40 [02:03<01:51,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: functioning | Target: functioning properly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 22/40 [02:09<01:45,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: crisp | Target: crisp and colorful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 23/40 [02:15<01:40,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Full | Target: full of honey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 24/40 [02:21<01:34,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: ripe | Target: ripe and undamaged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 25/40 [02:27<01:28,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: less | Target: rare, valuable one\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 26/40 [02:33<01:22,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: ideal | Target: still ideal for taking pictures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 27/40 [02:38<01:16,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: half | Target: unbaked cookies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 28/40 [02:44<01:10,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: adorned | Target: still unadorned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 29/40 [02:50<01:04,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: T | Target: tied to the dock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 30/40 [02:56<00:58,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Clear | Target: obstructed by leaves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 31/40 [03:02<00:52,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: soaked | Target: dry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 32/40 [03:08<00:46,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Fresh | Target: fresh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 33/40 [03:13<00:40,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: moved | Target: is covering the entrance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 34/40 [03:19<00:35,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: patched | Target: patched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 35/40 [03:25<00:29,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Not | Target: preheating at 350°F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 36/40 [03:31<00:23,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Im | Target: rare and exotic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 37/40 [03:37<00:17,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Healthy | Target: weakened state\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 38/40 [03:43<00:11,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: sturdy | Target: sturdy branches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 39/40 [03:48<00:05,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: severely | Target: healthy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [03:54<00:00,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Area | Target: area shown on the map\n",
      "Validation accuracy: 0.65 | Correct: 26 | Total: 40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.tokenizer.padding_side = \"left\"\n",
    "\n",
    "valid_accs = defaultdict(dict)\n",
    "for layer_idx in range(40, 42, 2):\n",
    "\n",
    "    n_epochs = 2\n",
    "    lambs = [0.005]\n",
    "    for lamb in lambs:\n",
    "        modules = [i for i in range(sing_vecs[layer_idx].size(0))]\n",
    "        mask = torch.ones(len(modules), requires_grad=True, device=\"cuda\", dtype=torch.bfloat16)\n",
    "        optimizer = torch.optim.Adam([mask], lr=1e-1)\n",
    "\n",
    "        print(f\"Training layer: {layer_idx}, lambda: {lamb}\")\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0\n",
    "\n",
    "            for bi, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "                alt_prompt = batch[\"alt_prompt\"]\n",
    "                org_prompt = batch[\"org_prompt\"]\n",
    "                target = batch[\"target\"]\n",
    "                target_token = model.tokenizer(target, return_tensors=\"pt\", padding=True, padding_side=\"right\")\n",
    "                target_input_ids = target_token.input_ids[:, 1:]\n",
    "                batch_size = target_input_ids.size(0)\n",
    "\n",
    "                alt_ques_idx = get_ques_start_token_idx(batch_size, model.tokenizer, alt_prompt, padding_side=\"right\")\n",
    "                alt_prompt_len = get_prompt_token_len(model.tokenizer, alt_prompt, padding_side=\"right\")\n",
    "                org_ques_idx = get_ques_start_token_idx(batch_size, model.tokenizer, org_prompt, padding_side=\"right\")\n",
    "                org_prompt_len = get_prompt_token_len(model.tokenizer, org_prompt, padding_side=\"right\")\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with model.trace() as tracer:\n",
    "\n",
    "                    with tracer.invoke(alt_prompt):\n",
    "                        alt_acts = model.model.layers[layer_idx].output[0][0, -1].clone().save()\n",
    "\n",
    "                    with tracer.invoke(org_prompt):\n",
    "                        sing_vec = sing_vecs[layer_idx].cuda()\n",
    "                        masked_vec = sing_vec * mask.unsqueeze(-1)\n",
    "                        proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "\n",
    "                        curr_output = model.model.layers[layer_idx].output[0][0, -1].clone()\n",
    "\n",
    "                        alt_proj = torch.matmul(alt_acts, proj_matrix)\n",
    "                        org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "\n",
    "                        modified_out = curr_output - org_proj + alt_proj\n",
    "                        model.model.layers[layer_idx].output[0][0, -1] = modified_out\n",
    "\n",
    "                        logits = model.lm_head.output[0, -1].save()\n",
    "\n",
    "                        del sing_vec, proj_matrix, masked_vec\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                target_logit = logits[target_input_ids[0]].sum()\n",
    "\n",
    "                task_loss = -(target_logit/batch_size)\n",
    "                l1_loss = lamb * torch.norm(mask, p=1)\n",
    "                loss = task_loss + l1_loss.to(task_loss.device)\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                if bi % 4 == 0:\n",
    "                    mean_loss = epoch_loss / (bi + 1)\n",
    "                    print(f\"Epoch: {epoch}, Batch: {bi}, Task Loss: {task_loss.item():.4f}, \"\n",
    "                        f\"L1 Loss: {l1_loss.item():.4f}, Total Loss: {mean_loss:.4f}\")\n",
    "                    with torch.no_grad():\n",
    "                        mask.data.clamp_(0, 1)\n",
    "                        rounded = torch.round(mask)\n",
    "                        print(f\"#Rank: {(rounded == 1).sum().item()}\")\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    mask.data.clamp_(0, 1)\n",
    "\n",
    "        print(f\"Training finished for layer: {layer_idx}, lambda: {lamb}\")\n",
    "\n",
    "        print(f\"Validation started for layer: {layer_idx}, lambda: {lamb}\")\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            mask_data = mask.data.clone()\n",
    "            mask_data.clamp_(0, 1)\n",
    "            rounded = torch.round(mask)\n",
    "\n",
    "            print(f\"Rank: {(rounded == 1).sum().item()}\")\n",
    "\n",
    "            # Save the mask\n",
    "            # torch.save(mask_data, f\"../masks/bigtom/{layer_idx}.pt\")\n",
    "\n",
    "            for bi, batch in tqdm(enumerate(valid_dataloader), total=len(valid_dataloader)):\n",
    "                alt_prompt = batch[\"alt_prompt\"]\n",
    "                org_prompt = batch[\"org_prompt\"]\n",
    "                alt_ans = batch[\"alt_ans\"]\n",
    "                target = batch[\"target\"][0]\n",
    "                batch_size = len(alt_ans)\n",
    "\n",
    "                alt_ques_idx = get_ques_start_token_idx(batch_size, model.tokenizer, alt_prompt, padding_side=\"left\")\n",
    "                alt_prompt_len = get_prompt_token_len(model.tokenizer, alt_prompt, padding_side=\"left\")\n",
    "                org_ques_idx = get_ques_start_token_idx(batch_size, model.tokenizer, org_prompt, padding_side=\"left\")\n",
    "                org_prompt_len = get_prompt_token_len(model.tokenizer, org_prompt, padding_side=\"left\")\n",
    "\n",
    "                with model.session() as session:\n",
    "\n",
    "                    with model.trace(alt_prompt):\n",
    "                        alt_acts = model.model.layers[layer_idx].output[0][0, -1].save()\n",
    "\n",
    "                    with model.generate(org_prompt, max_new_tokens=2, do_sample=False, num_return_sequences=1, pad_token_id=model.tokenizer.pad_token_id, eos_token_id=model.tokenizer.eos_token_id):\n",
    "                        sing_vec = sing_vecs[layer_idx].cuda()\n",
    "                        masked_vec = sing_vec * rounded.unsqueeze(-1)\n",
    "                        proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "\n",
    "                        curr_output = model.model.layers[layer_idx].output[0][0, -1].clone()\n",
    "\n",
    "                        alt_proj = torch.matmul(alt_acts, proj_matrix)\n",
    "                        org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "\n",
    "                        modified_out = curr_output - org_proj + alt_proj\n",
    "\n",
    "                        model.model.layers[layer_idx].output[0][0, -1] = modified_out\n",
    "\n",
    "                        out = model.generator.output.save()\n",
    "\n",
    "                    del alt_acts\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                pred = model.tokenizer.decode(out[0][org_prompt_len:-1]).strip()\n",
    "                print(f\"Prediction: {pred} | Target: {target}\")\n",
    "                if pred.lower() in target.lower():\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "            print(f\"Validation accuracy: {correct / total:.2f} | Correct: {correct} | Total: {total}\\n\")\n",
    "            valid_accs[lamb][layer_idx] = round(correct / total, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.005 -> 0.75\n",
    "# 0.0025 -> 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sing_vecs = defaultdict(dict)\n",
    "for l in range(model.config.num_hidden_layers):\n",
    "    sing_vecs[l] = torch.load(f\"../svd_results/BigToM/query_charac_new/singular_vecs/{l}.pt\").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 80\n",
    "valid_size = 40\n",
    "batch_size = 1\n",
    "\n",
    "dataset = get_bigtom_query_charac(df_false, df_true, train_size+valid_size)\n",
    "train_dataset = dataset[:train_size]\n",
    "valid_dataset = dataset[train_size:]\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\n",
      "\n",
      "Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor sees her coworker swapping the milk.\n",
      "Question: Does Noor believe the milk pitcher contains oat milk or almond milk?\n",
      "Answer: almond milk\n",
      "Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\n",
      "\n",
      "Story: Amara is a botanist exploring a dense rainforest in search of a rare orchid species. Amara's goal is to find the rare orchid and study its unique characteristics. She spots an orchid with vibrant purple petals that she thinks might be the one she is searching for. As she continues to explore, a sudden downpour washes away the purple pigment from the orchid, revealing that it is actually a common white orchid. Amara witnesses the downpour and the color change in the orchid.\n",
      "Question: Does Noor believe the orchid with vibrant purple petals is the rare species or a common white orchid?\n",
      "Answer: unknown\n",
      "Target: is a common white orchid\n"
     ]
    }
   ],
   "source": [
    "idx=0\n",
    "print(dataset[idx]['alt_prompt'], dataset[idx]['alt_ans'])\n",
    "print(dataset[idx]['org_prompt'], dataset[idx]['org_ans'])\n",
    "print(f\"Target: {dataset[idx]['target']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer: 14, lambda: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Task Loss: -14.8203, L1 Loss: 4.0000, Total Loss: -10.8203\n",
      "#Rank: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4/80 [00:32<10:24,  8.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 4, Task Loss: -10.4141, L1 Loss: 3.4844, Total Loss: -8.1102\n",
      "#Rank: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 8/80 [01:07<09:57,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 8, Task Loss: -5.6055, L1 Loss: 3.1562, Total Loss: -13.5634\n",
      "#Rank: 699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 12/80 [01:42<09:46,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 12, Task Loss: -21.2031, L1 Loss: 2.9375, Total Loss: -15.6364\n",
      "#Rank: 584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 16/80 [02:18<09:30,  8.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 16, Task Loss: -49.2188, L1 Loss: 2.7344, Total Loss: -23.9784\n",
      "#Rank: 554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 20/80 [02:53<08:53,  8.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 20, Task Loss: -10.2578, L1 Loss: 2.5469, Total Loss: -20.2484\n",
      "#Rank: 525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 24/80 [03:30<08:21,  8.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 24, Task Loss: -30.8125, L1 Loss: 2.4062, Total Loss: -18.9494\n",
      "#Rank: 495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 28/80 [04:06<07:46,  8.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 28, Task Loss: -13.6172, L1 Loss: 2.2656, Total Loss: -20.5725\n",
      "#Rank: 465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 32/80 [04:41<06:58,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 32, Task Loss: -89.8750, L1 Loss: 2.1562, Total Loss: -22.7676\n",
      "#Rank: 432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 36/80 [05:16<06:24,  8.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 36, Task Loss: -4.6680, L1 Loss: 2.0625, Total Loss: -21.8538\n",
      "#Rank: 424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 40/80 [05:51<06:03,  9.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 40, Task Loss: -36.5000, L1 Loss: 1.9922, Total Loss: -22.0757\n",
      "#Rank: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 44/80 [06:25<05:16,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 44, Task Loss: -36.3750, L1 Loss: 1.9375, Total Loss: -22.6069\n",
      "#Rank: 396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 48/80 [07:03<04:58,  9.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 48, Task Loss: -42.7188, L1 Loss: 1.8672, Total Loss: -22.6900\n",
      "#Rank: 377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 52/80 [07:39<04:20,  9.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 52, Task Loss: -40.5625, L1 Loss: 1.8203, Total Loss: -22.8216\n",
      "#Rank: 367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 56/80 [08:14<03:41,  9.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 56, Task Loss: -10.4609, L1 Loss: 1.7812, Total Loss: -22.6715\n",
      "#Rank: 353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 60/80 [08:51<03:05,  9.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 60, Task Loss: -10.3203, L1 Loss: 1.7500, Total Loss: -21.4731\n",
      "#Rank: 350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 64/80 [09:27<02:27,  9.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 64, Task Loss: -59.9375, L1 Loss: 1.7266, Total Loss: -22.4138\n",
      "#Rank: 344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 68/80 [10:06<01:55,  9.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 68, Task Loss: -11.9141, L1 Loss: 1.7109, Total Loss: -22.0800\n",
      "#Rank: 340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 72/80 [10:43<01:12,  9.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 72, Task Loss: -41.9688, L1 Loss: 1.6875, Total Loss: -21.6590\n",
      "#Rank: 334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 76/80 [11:18<00:36,  9.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 76, Task Loss: -9.9688, L1 Loss: 1.6719, Total Loss: -21.6988\n",
      "#Rank: 333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [11:54<00:00,  8.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for layer: 14, lambda: 0.005\n",
      "Validation started for layer: 14, lambda: 0.005\n",
      "Rank: 327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1/40 [00:05<03:45,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: N | Target: warming up due to the power outage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [00:12<04:00,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: has | Target: has a hairline crack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/40 [00:19<04:04,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Already | Target: have already been found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [00:26<04:01,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: melted | Target: has melted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:33<03:56,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Fish | Target: fish have moved away\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [00:39<03:50,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: A | Target: corrupted due to a server malfunction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 7/40 [00:46<03:44,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: corrupted | Target: corrupted and difficult to use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 8/40 [00:53<03:38,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: rough | Target: rough and dangerous\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 9/40 [01:00<03:31,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Not | Target: exposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 10/40 [01:07<03:24,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Ru | Target: the paintbrushes are ruined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 11/40 [01:14<03:18,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Dis | Target: disrupted by wind and leaves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 12/40 [01:21<03:11,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: De | Target: devoid of fish due to the volcanic eruption\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 13/40 [01:27<03:04,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: N | Target: soaked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 14/40 [01:34<02:58,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: N | Target: half-baked cookies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 15/40 [01:41<02:53,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Not | Target: not heating at all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 16/40 [01:48<02:45,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Am | Target: hard and unworkable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 17/40 [01:55<02:37,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Over | Target: over-fermented\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 18/40 [02:02<02:31,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Mei | Target: become indistinguishable from any other piece of clay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 19/40 [02:09<02:25,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: severely | Target: severely damaged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 20/40 [02:16<02:19,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Enrique | Target: has a broken string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 21/40 [02:23<02:12,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: extremely | Target: extremely out of tune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 22/40 [02:30<02:04,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Kw | Target: washed away and diluted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 23/40 [02:37<01:57,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: concealed | Target: knows about the concealed door\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 24/40 [02:44<01:50,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: tangled | Target: tangled and difficult to use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 25/40 [02:50<01:43,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Healthy | Target: healthy state\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 26/40 [02:57<01:36,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Slip | Target: floor is slippery due to the oil spill\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 27/40 [03:04<01:29,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Ru | Target: the paintbrushes are ruined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 28/40 [03:11<01:22,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: torn | Target: torn with a large hole\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 29/40 [03:18<01:16,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: N | Target: damaged by the storm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 30/40 [03:25<01:09,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: F | Target: filled with sand\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 31/40 [03:32<01:02,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: There | Target: has crashed and become unresponsive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 32/40 [03:39<00:55,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Wash | Target: washed away and diluted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 33/40 [03:46<00:48,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: W | Target: have wilted due to the rainstorm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 34/40 [03:53<00:41,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: corrupted | Target: corrupted and difficult to use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 35/40 [03:59<00:34,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Rash | Target: has been badly damaged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 36/40 [04:06<00:27,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Im | Target: that the fishing net has a large hole in it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 37/40 [04:13<00:20,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: W | Target: wilted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 38/40 [04:20<00:13,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: weakened | Target: weakened and infested with termites\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 39/40 [04:27<00:06,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: emerging | Target: an emerging artist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [04:34<00:00,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Not | Target: sold out\n",
      "Validation accuracy: 0.70 | Correct: 28 | Total: 40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.tokenizer.padding_side = \"left\"\n",
    "\n",
    "valid_accs = defaultdict(dict)\n",
    "for layer_idx in range(14, 16, 2):\n",
    "\n",
    "    n_epochs = 1\n",
    "    lambs = [0.005]\n",
    "    for lamb in lambs:\n",
    "        modules = [i for i in range(sing_vecs[layer_idx].size(0))]\n",
    "        mask = torch.ones(len(modules), requires_grad=True, device=\"cuda\", dtype=torch.bfloat16)\n",
    "        optimizer = torch.optim.Adam([mask], lr=1e-1)\n",
    "\n",
    "        print(f\"Training layer: {layer_idx}, lambda: {lamb}\")\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0\n",
    "\n",
    "            for bi, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "                alt_prompt = batch[\"alt_prompt\"]\n",
    "                org_prompt = batch[\"org_prompt\"]\n",
    "                target = batch[\"target\"]\n",
    "                target_token = model.tokenizer(target, return_tensors=\"pt\", padding=True, padding_side=\"right\")\n",
    "                target_input_ids = target_token.input_ids[:, 1:]\n",
    "                batch_size = target_input_ids.size(0)\n",
    "\n",
    "                alt_ques_idx = get_ques_start_token_idx(batch_size, model.tokenizer, alt_prompt, padding_side=\"right\")\n",
    "                alt_prompt_len = get_prompt_token_len(model.tokenizer, alt_prompt, padding_side=\"right\")\n",
    "                org_ques_idx = get_ques_start_token_idx(batch_size, model.tokenizer, org_prompt, padding_side=\"right\")\n",
    "                org_prompt_len = get_prompt_token_len(model.tokenizer, org_prompt, padding_side=\"right\")\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with model.trace() as tracer:\n",
    "                    alt_acts = defaultdict(dict)\n",
    "                    with tracer.invoke(alt_prompt):\n",
    "                        for t_idx, t in enumerate([i for i in range(alt_ques_idx+3, alt_ques_idx+5)]):\n",
    "                            alt_acts[t_idx] = model.model.layers[layer_idx].output[0][0, t].clone().save()\n",
    "\n",
    "                    with tracer.invoke(org_prompt):\n",
    "                        sing_vec = sing_vecs[layer_idx].cuda()\n",
    "                        masked_vec = sing_vec * mask.unsqueeze(-1)\n",
    "                        proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "\n",
    "                        for t_idx, t in enumerate([i for i in range(org_ques_idx+3, org_ques_idx+5)]):\n",
    "                            curr_output = model.model.layers[layer_idx].output[0][0, t].clone()\n",
    "\n",
    "                            alt_proj = torch.matmul(alt_acts[t_idx], proj_matrix)\n",
    "                            org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "\n",
    "                            modified_out = curr_output - org_proj + alt_proj\n",
    "                            model.model.layers[layer_idx].output[0][0, t] = modified_out\n",
    "\n",
    "                        logits = model.lm_head.output[0, -1].save()\n",
    "\n",
    "                        del sing_vec, proj_matrix, masked_vec\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                target_logit = logits[target_input_ids[0]].sum()\n",
    "\n",
    "                task_loss = -(target_logit/batch_size)\n",
    "                l1_loss = lamb * torch.norm(mask, p=1)\n",
    "                loss = task_loss + l1_loss.to(task_loss.device)\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                if bi % 4 == 0:\n",
    "                    mean_loss = epoch_loss / (bi + 1)\n",
    "                    print(f\"Epoch: {epoch}, Batch: {bi}, Task Loss: {task_loss.item():.4f}, \"\n",
    "                        f\"L1 Loss: {l1_loss.item():.4f}, Total Loss: {mean_loss:.4f}\")\n",
    "                    with torch.no_grad():\n",
    "                        mask.data.clamp_(0, 1)\n",
    "                        rounded = torch.round(mask)\n",
    "                        print(f\"#Rank: {(rounded == 1).sum().item()}\")\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    mask.data.clamp_(0, 1)\n",
    "\n",
    "        print(f\"Training finished for layer: {layer_idx}, lambda: {lamb}\")\n",
    "\n",
    "        layer_idx = 14\n",
    "        print(f\"Validation started for layer: {layer_idx}, lambda: {lamb}\")\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            mask_data = mask.data.clone()\n",
    "            mask_data.clamp_(0, 1)\n",
    "            rounded = torch.round(mask)\n",
    "\n",
    "            print(f\"Rank: {(rounded == 1).sum().item()}\")\n",
    "\n",
    "            # Save the mask\n",
    "            # torch.save(mask_data, f\"../masks/bigtom/{layer_idx}.pt\")\n",
    "\n",
    "            for bi, batch in tqdm(enumerate(valid_dataloader), total=len(valid_dataloader)):\n",
    "                alt_prompt = batch[\"alt_prompt\"]\n",
    "                org_prompt = batch[\"org_prompt\"]\n",
    "                alt_ans = batch[\"alt_ans\"]\n",
    "                target = batch[\"target\"][0]\n",
    "                batch_size = len(alt_ans)\n",
    "\n",
    "                alt_ques_idx = get_ques_start_token_idx(batch_size, model.tokenizer, alt_prompt, padding_side=\"left\")\n",
    "                alt_prompt_len = get_prompt_token_len(model.tokenizer, alt_prompt, padding_side=\"left\")\n",
    "                org_ques_idx = get_ques_start_token_idx(batch_size, model.tokenizer, org_prompt, padding_side=\"left\")\n",
    "                org_prompt_len = get_prompt_token_len(model.tokenizer, org_prompt, padding_side=\"left\")\n",
    "\n",
    "                with model.session() as session:\n",
    "                    alt_layer_out = defaultdict(dict)\n",
    "                    with model.trace(alt_prompt):\n",
    "                        for t_idx, t in enumerate([i for i in range(alt_ques_idx+3, alt_ques_idx+5)]):\n",
    "                            alt_layer_out[t_idx] = model.model.layers[layer_idx].output[0][0, t].save()\n",
    "\n",
    "                    with model.generate(org_prompt, max_new_tokens=2, do_sample=False, num_return_sequences=1, pad_token_id=model.tokenizer.pad_token_id, eos_token_id=model.tokenizer.eos_token_id):\n",
    "                        sing_vec = sing_vecs[layer_idx].cuda()\n",
    "                        masked_vec = sing_vec * rounded.unsqueeze(-1)\n",
    "                        proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "                        \n",
    "                        for t_idx, t in enumerate([i for i in range(org_ques_idx+3, org_ques_idx+5)]):\n",
    "                            curr_output = model.model.layers[layer_idx].output[0][0, t].clone()\n",
    "                            alt_proj = torch.matmul(alt_layer_out[t_idx], proj_matrix)\n",
    "                            org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "                            modified_out = curr_output - org_proj + alt_proj\n",
    "                            model.model.layers[layer_idx].output[0][0, t] = modified_out\n",
    "\n",
    "                        out = model.generator.output.save()\n",
    "\n",
    "                    del alt_layer_out\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                pred = model.tokenizer.decode(out[0][org_prompt_len:-1]).strip()\n",
    "                print(f\"Prediction: {pred} | Target: {target}\")\n",
    "                if pred.lower() in target.lower():\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "            print(f\"Validation accuracy: {correct / total:.2f} | Correct: {correct} | Total: {total}\\n\")\n",
    "            valid_accs[lamb][layer_idx] = round(correct / total, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
