{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "env.yml not found in /disk/u/nikhil/mind!\n",
      "Setting MODEL_ROOT=\"\". Models will now be downloaded to conda env cache, if not already there\n",
      "Other defaults are set to:\n",
      "    DATA_DIR = \"data\"\n",
      "    RESULTS_DIR = \"results\"\n",
      "    HPARAMS_DIR = \"hparams\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import Any, List, Optional\n",
    "import nnsight\n",
    "from nnsight import CONFIG, LanguageModel\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from einops import einsum\n",
    "import time\n",
    "from einops import rearrange, reduce\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from src.dataset import SampleV3, DatasetV3, STORY_TEMPLATES\n",
    "from src.utils import env_utils\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "random.seed(10)\n",
    "\n",
    "CONFIG.set_default_api_key(\"d9e00ab7d4f74643b3176de0913f24a7\")\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "CONFIG.APP.REMOTE_LOGGING = False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = {}\n",
    "all_containers= {}\n",
    "all_characters = json.load(open(os.path.join(env_utils.DEFAULT_DATA_DIR, \"synthetic_entities\", \"characters.json\"), \"r\"))\n",
    "\n",
    "for TYPE, DCT in {\"states\": all_states, \"containers\": all_containers}.items():\n",
    "    ROOT = os.path.join(\n",
    "        env_utils.DEFAULT_DATA_DIR, \"synthetic_entities\", TYPE\n",
    "    )\n",
    "    for file in os.listdir(ROOT):\n",
    "        file_path = os.path.join(ROOT, file)\n",
    "        with open(file_path, \"r\") as f:\n",
    "            names = json.load(f)\n",
    "        DCT[file.split(\".\")[0]] = names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [00:48<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# model = LanguageModel(\"meta-llama/Meta-Llama-3.1-405B\")\n",
    "model = LanguageModel(\"meta-llama/Meta-Llama-3-70B-Instruct\", cache_dir=\"/disk/u/nikhil/.cache/huggingface/hub/\", device_map=\"auto\", load_in_4bit=True, torch_dtype=torch.float16, dispatch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ques_start_token_idx(tokenizer, prompt):      \n",
    "    input_tokens = tokenizer.encode(prompt, return_tensors=\"pt\").squeeze()\n",
    "    corrolary_token = tokenizer.encode(\":\", return_tensors=\"pt\").squeeze()[-1].item()\n",
    "    ques_start_idx = (input_tokens == corrolary_token).nonzero()[2].item()\n",
    "\n",
    "    return ques_start_idx-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_token_len(tokenizer, prompt):\n",
    "    input_tokens = tokenizer.encode(prompt, return_tensors=\"pt\").squeeze()\n",
    "    return len(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pred(pred, target, verbose=False):\n",
    "    prompt = f\"Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \\n\\nGround truth: {target}\\nPrediction: {pred}\\nAnswer:\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(prompt)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with model.generate(prompt, max_new_tokens=5, do_sample=False, num_return_sequences=1, pad_token_id=model.tokenizer.pad_token_id):\n",
    "            out = model.generator.output.save()\n",
    "\n",
    "    prompt_len = get_prompt_token_len(model.tokenizer, prompt)\n",
    "\n",
    "    return model.tokenizer.decode(out[0][prompt_len:-1]).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading BigToM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a csv file\n",
    "df_false = pd.read_csv(\"../data/bigtom/0_forward_belief_false_belief/stories.csv\", delimiter=\";\")\n",
    "df_true = pd.read_csv(\"../data/bigtom/0_forward_belief_true_belief/stories.csv\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each row in the dataframe extract story, answer, and distractor\n",
    "true_stories, false_stories = [], []\n",
    "for i in range(len(df_true)):\n",
    "    story = df_true.iloc[i]['story']\n",
    "    question = df_true.iloc[i]['question']\n",
    "    answer = df_true.iloc[i]['answer']\n",
    "    distractor = df_true.iloc[i]['distractor']\n",
    "    true_stories.append({\"story\": story, \"question\": question, \"answer\": answer, \"distractor\": distractor})\n",
    "\n",
    "for i in range(len(df_false)):\n",
    "    story = df_false.iloc[i]['story']\n",
    "    question = df_true.iloc[i]['question']\n",
    "    answer = df_false.iloc[i]['answer']\n",
    "    distractor = df_false.iloc[i]['distractor']\n",
    "    false_stories.append({\"story\": story, \"question\": question, \"answer\": answer, \"distractor\": distractor})\n",
    "\n",
    "dataset = []\n",
    "instruction = \"1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\"\n",
    "\n",
    "for i in range(min(len(true_stories), len(false_stories))):\n",
    "    question = true_stories[i]['question']\n",
    "    visible_prompt = f\"Instructions: {instruction}\\n\\nStory: {true_stories[i]['story']}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "    question = false_stories[i]['question']\n",
    "    invisible_prompt = f\"Instructions: {instruction}\\n\\nStory: {false_stories[i]['story']}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "    visible_ans = true_stories[i]['answer'].split()\n",
    "    invisible_ans = false_stories[i]['answer'].split()\n",
    "\n",
    "    # Find the index of first word which is different in both answers\n",
    "    diff_idx = 0\n",
    "    for idx, (v, j) in enumerate(zip(visible_ans, invisible_ans)):\n",
    "        if v != j:\n",
    "            diff_idx = idx\n",
    "            break\n",
    "    \n",
    "    visible_ans = \" \".join(visible_ans[diff_idx:])[:-1]\n",
    "    invisible_ans = \" \".join(invisible_ans[diff_idx:])[:-1]\n",
    "\n",
    "    visible_ans_one_word = \" \".join(visible_ans[diff_idx:diff_idx+1])\n",
    "    invisible_ans_one_word = \" \".join(invisible_ans[diff_idx:diff_idx+1])\n",
    "\n",
    "    visible_ans_one_word = visible_ans[:-1] if visible_ans[-1] == \".\" else visible_ans\n",
    "    invisible_ans_one_word = invisible_ans[:-1] if invisible_ans[-1] == \".\" else invisible_ans\n",
    "\n",
    "    dataset.append({\n",
    "        \"alt_story\": true_stories[i]['story'],\n",
    "        \"alt_question\": true_stories[i]['question'],\n",
    "        \"alt_prompt\": visible_prompt,\n",
    "        \"alt_ans\": visible_ans,\n",
    "        \"org_story\": false_stories[i]['story'],\n",
    "        \"org_question\": false_stories[i]['question'],\n",
    "        \"org_prompt\": invisible_prompt,\n",
    "        \"org_ans\": invisible_ans,\n",
    "        \"target\": visible_ans,\n",
    "    })\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\n",
      "\n",
      "Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor sees her coworker swapping the milk.\n",
      "Question: Does Noor believe the milk pitcher contains oat milk or almond milk?\n",
      "Answer: almond milk\n",
      "Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\n",
      "\n",
      "Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor does not see her coworker swapping the milk.\n",
      "Question: Does Noor believe the milk pitcher contains oat milk or almond milk?\n",
      "Answer: oat milk\n",
      "Target: almond milk\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(dataset[idx]['alt_prompt'], dataset[idx]['alt_ans'])\n",
    "print(dataset[idx]['org_prompt'], dataset[idx]['org_ans'])\n",
    "print(f\"Target: {dataset[idx]['target']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 40\n",
    "valid_size = 20\n",
    "batch_size = 1\n",
    "\n",
    "train_dataset = dataset[:train_size]\n",
    "valid_dataset = dataset[train_size:train_size+valid_size]\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Custom Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 40\n",
    "valid_size = 20\n",
    "batch_size = 4\n",
    "\n",
    "train_dataset = get_visibility_align_exps(STORY_TEMPLATES,\n",
    "                             all_characters,\n",
    "                             all_containers,\n",
    "                             all_states,\n",
    "                             train_size,\n",
    "                             question_type=\"belief_question\",\n",
    "                             diff_visibility=True)\n",
    "valid_dataset = get_visibility_align_exps(STORY_TEMPLATES,\n",
    "                             all_characters,\n",
    "                             all_containers,\n",
    "                             all_states,\n",
    "                             valid_size,\n",
    "                             question_type=\"belief_question\",\n",
    "                             diff_visibility=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=valid_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any beliefs about the container and its contents which they cannot observe. 4. To answer the question, predict only what is inside the queried container, strictly based on the belief of the character, mentioned in the question. 5. If the queried character has no belief about the container in question, then predict 'unknown'. 6. Do not predict container or character as the final output.\n",
      "\n",
      "Story: Max and Karen are working in a busy restaurant. To complete an order, Max grabs an opaque tun and fills it with coffee. Then Karen grabs another opaque dispenser and fills it with cocoa. They are working side by side and can clearly observe each other's actions.\n",
      "Question: What does Karen believe the tun contains?\n",
      "Answer: coffee\n",
      "Instruction: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any beliefs about the container and its contents which they cannot observe. 4. To answer the question, predict only what is inside the queried container, strictly based on the belief of the character, mentioned in the question. 5. If the queried character has no belief about the container in question, then predict 'unknown'. 6. Do not predict container or character as the final output.\n",
      "\n",
      "Story: Max and Karen are working in a busy restaurant. To complete an order, Max grabs an opaque tun and fills it with port. Then Karen grabs another opaque dispenser and fills it with water. They are working in the entirely separate sections, with no visibility between them.\n",
      "Question: What does Karen believe the tun contains?\n",
      "Answer: unknown\n",
      " port\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(train_dataset[idx]['corrupt_prompt'], train_dataset[idx]['corrupt_ans'])\n",
    "print(train_dataset[idx]['clean_prompt'], train_dataset[idx]['clean_ans'])\n",
    "print(train_dataset[idx]['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Singular Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sing_vecs = defaultdict(dict)\n",
    "for l in range(41):\n",
    "    sing_vecs[l] = torch.load(f\"../svd_results/bigtom/singular_vecs/{l}.pt\").cpu()\n",
    "    # sing_vecs[l] = torch.load(f\"../svd_results/toy/singular_vecs/{l}.pt\").cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Task Loss: -9.1875, L1 Loss: 95.0000, Total Loss: 85.8125\n",
      "#Causal SVs: 3814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 8/40 [00:57<03:47,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 8, Task Loss: -29.5312, L1 Loss: 45.2500, Total Loss: 42.0816\n",
      "#Causal SVs: 1417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 16/40 [02:00<03:20,  8.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 16, Task Loss: -14.0391, L1 Loss: 15.6875, Total Loss: 19.9297\n",
      "#Causal SVs: 566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 24/40 [03:04<02:03,  7.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 24, Task Loss: -32.3750, L1 Loss: 9.3750, Total Loss: 6.2934\n",
      "#Causal SVs: 359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 32/40 [04:03<00:59,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 32, Task Loss: -67.9375, L1 Loss: 6.8438, Total Loss: -4.2536\n",
      "#Causal SVs: 252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [05:02<00:00,  7.55s/it]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 0, Task Loss: -29.8594, L1 Loss: 5.3125, Total Loss: -24.5469\n",
      "#Causal SVs: 204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 8/40 [01:34<06:13, 11.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 8, Task Loss: -29.0938, L1 Loss: 4.4375, Total Loss: -28.5608\n",
      "#Causal SVs: 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 16/40 [03:09<04:36, 11.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 16, Task Loss: -15.8828, L1 Loss: 3.8281, Total Loss: -28.5795\n",
      "#Causal SVs: 152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 24/40 [04:36<02:39,  9.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 24, Task Loss: -35.0625, L1 Loss: 3.4688, Total Loss: -29.3403\n",
      "#Causal SVs: 135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 32/40 [05:37<01:00,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 32, Task Loss: -72.3750, L1 Loss: 3.2969, Total Loss: -32.9841\n",
      "#Causal SVs: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [06:33<00:00,  9.85s/it]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Batch: 0, Task Loss: -29.1094, L1 Loss: 3.1406, Total Loss: -25.9688\n",
      "#Causal SVs: 125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 8/40 [00:57<03:48,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Batch: 8, Task Loss: -28.7188, L1 Loss: 3.0156, Total Loss: -29.9861\n",
      "#Causal SVs: 117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 16/40 [01:56<02:53,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Batch: 16, Task Loss: -15.0000, L1 Loss: 2.9062, Total Loss: -29.6645\n",
      "#Causal SVs: 117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 24/40 [02:55<02:00,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Batch: 24, Task Loss: -34.1875, L1 Loss: 2.8438, Total Loss: -30.9747\n",
      "#Causal SVs: 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 32/40 [03:54<00:58,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Batch: 32, Task Loss: -74.5625, L1 Loss: 2.7812, Total Loss: -34.1181\n",
      "#Causal SVs: 115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [04:51<00:00,  7.29s/it]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Batch: 0, Task Loss: -29.1250, L1 Loss: 2.6719, Total Loss: -26.4531\n",
      "#Causal SVs: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 8/40 [00:58<03:49,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Batch: 8, Task Loss: -27.6562, L1 Loss: 2.6250, Total Loss: -29.9523\n",
      "#Causal SVs: 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 16/40 [01:58<02:55,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Batch: 16, Task Loss: -16.6719, L1 Loss: 2.5938, Total Loss: -29.5216\n",
      "#Causal SVs: 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 24/40 [03:21<02:55, 10.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Batch: 24, Task Loss: -36.0312, L1 Loss: 2.5781, Total Loss: -30.6397\n",
      "#Causal SVs: 105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 32/40 [05:00<01:38, 12.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Batch: 32, Task Loss: -71.6875, L1 Loss: 2.5938, Total Loss: -34.1333\n",
      "#Causal SVs: 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [06:30<00:00,  9.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete for 32!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_accs, rank = {}, {}\n",
    "\n",
    "for layer_idx in range(32 , 34, 2):\n",
    "    modules = [i for i in range(sing_vecs[layer_idx].shape[0])]\n",
    "    mask = torch.ones(len(modules), requires_grad=True, device=\"cuda\", dtype=torch.bfloat16)\n",
    "    optimizer = torch.optim.Adam([mask], lr=1e-1)\n",
    "    n_epochs = 4\n",
    "    lamb = 0.025\n",
    "\n",
    "    print(f\"Training layer: {layer_idx}\")\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for bi, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "            alt_prompt = batch[\"alt_prompt\"][0]\n",
    "            org_prompt = batch[\"org_prompt\"][0]\n",
    "            target = batch[\"target\"][0]\n",
    "            target_token = model.tokenizer(target, return_tensors=\"pt\", padding=True, padding_side=\"right\").input_ids[0, 1:]\n",
    "            batch_size = target_token.shape[0]\n",
    "\n",
    "            alt_ques_idx = get_ques_start_token_idx(model.tokenizer, alt_prompt)\n",
    "            alt_prompt_len = get_prompt_token_len(model.tokenizer, alt_prompt)\n",
    "            org_ques_idx = get_ques_start_token_idx(model.tokenizer, org_prompt)\n",
    "            org_prompt_len = get_prompt_token_len(model.tokenizer, org_prompt)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with model.trace() as tracer:\n",
    "\n",
    "                alt_acts = defaultdict(dict)\n",
    "                with tracer.invoke(alt_prompt):\n",
    "                    for t_idx, t in enumerate(range(alt_ques_idx, alt_prompt_len)):\n",
    "                        alt_acts[t_idx] = model.model.layers[layer_idx].output[0][0, t].clone()\n",
    "\n",
    "                with tracer.invoke(org_prompt):\n",
    "                    sing_vec = sing_vecs[layer_idx].cuda()\n",
    "                    masked_vec = sing_vec * mask.unsqueeze(-1)\n",
    "                    proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "\n",
    "                    for t_idx, t in enumerate(range(org_ques_idx, org_prompt_len)):\n",
    "                        curr_output = model.model.layers[layer_idx].output[0][0, t].clone()\n",
    "                        \n",
    "                        alt_proj = torch.matmul(alt_acts[t_idx], proj_matrix)\n",
    "                        org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "\n",
    "                        modified_out = curr_output - org_proj + alt_proj\n",
    "                        model.model.layers[layer_idx].output[0][0, t] = modified_out\n",
    "                    \n",
    "                    del sing_vec, proj_matrix, masked_vec\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "                    logits = model.lm_head.output[0, -1].save()\n",
    "            \n",
    "            target_logit = logits[target_token]\n",
    "\n",
    "            task_loss = -torch.sum(target_logit)\n",
    "            l1_loss = lamb * torch.norm(mask, p=1)\n",
    "            loss = task_loss + l1_loss.to(task_loss.device)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if bi % (len(train_dataloader)//5) == 0:\n",
    "                mean_loss = epoch_loss / (bi + 1)\n",
    "                print(f\"Epoch: {epoch}, Batch: {bi}, Task Loss: {task_loss.item():.4f}, \"\n",
    "                    f\"L1 Loss: {l1_loss.item():.4f}, Total Loss: {mean_loss:.4f}\")\n",
    "                with torch.no_grad():\n",
    "                    mask.data.clamp_(0, 1)\n",
    "                    rounded = torch.round(mask)\n",
    "                    print(f\"#Causal SVs: {(rounded == 1).sum().item()}\")\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Clamp after optimizer step\n",
    "            with torch.no_grad():\n",
    "                mask.data.clamp_(0, 1)\n",
    "\n",
    "    print(f\"Training complete for {layer_idx}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4375, 1.0000, 0.8906, 0.3535, 0.3984, 0.5234, 0.0000, 0.8047, 1.0000,\n",
       "        1.0000, 0.9961, 0.9180, 0.6992, 0.0000, 0.0000, 0.0000, 1.0000, 0.6328,\n",
       "        0.9922, 0.0000, 1.0000, 0.0000, 1.0000, 0.0229, 0.8594, 1.0000, 0.3906,\n",
       "        0.7344, 1.0000, 1.0000, 0.0000, 0.2598, 0.0000, 0.0000, 0.0000, 1.0000,\n",
       "        0.6602, 1.0000, 0.0000, 0.8945, 0.0000, 1.0000, 0.0000, 0.2158, 0.8906,\n",
       "        0.4590, 0.0000, 0.9883, 0.0000, 0.9375, 0.0000, 0.8477, 0.7578, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.9844, 0.0000, 1.0000, 0.0000, 0.4824, 0.0000,\n",
       "        0.9023, 0.0000, 0.0000, 0.3418, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000,\n",
       "        0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.9883, 0.0000, 1.0000, 0.0000,\n",
       "        1.0000, 1.0000, 0.0000, 0.0000, 0.9844, 1.0000, 1.0000, 0.0000, 0.0000,\n",
       "        0.9961, 1.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000], device='cuda:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[:100]\n",
    "# 0.025: 0.75 (4 epochs) | 104 SVs\n",
    "# 0.03: 0.75 (4 epochs) | 81 Svs\n",
    "# 0.035: 0.65 (4 epochs) | 71 svs\n",
    "# 0.04: 0.35 (4 epochs) | 61 svs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Causal SVs: 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: the mole sauce is extremely spicy\n",
      "Prediction:  extremely spicy\n",
      "Answer:\n",
      "Check: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:10<03:19, 10.51s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: ruined with black paint\n",
      "Prediction:  ruined with black paint\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:19<02:53,  9.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: the paintbrushes are ruined\n",
      "Prediction:  ruined paintbrushes\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:27<02:30,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: damaged by the sandstorm\n",
      "Prediction:  Damaged by the sandstorm (\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:35<02:18,  8.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: filled with sand\n",
      "Prediction:  Filled with sand.\n",
      "Answer:\n",
      "Check: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:46<02:22,  9.52s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: washed away and diluted\n",
      "Prediction:  Not full and ready for harvest.\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:56<02:13,  9.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: punctured and non-functional\n",
      "Prediction:  punctured and non-functional\n",
      "Answer:\n",
      "Check: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [01:07<02:11, 10.13s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: disrupted by fallen leaves\n",
      "Prediction:  Disrupted by fallen leaves.\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [01:17<01:58,  9.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: disrupted by wind and leaves\n",
      "Prediction:  Disrupted\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [01:24<01:40,  9.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: disarrayed by the wind\n",
      "Prediction:  Disarrayed by the wind.\n",
      "Answer:\n",
      "Check: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [01:35<01:37,  9.78s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: branches have been snapped off by the wind\n",
      "Prediction:  Overgrown branches\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [01:44<01:24,  9.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: torn\n",
      "Prediction:  torn\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [01:51<01:10,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: there is a storm approaching\n",
      "Prediction:  There is no storm approaching.\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [01:59<01:00,  8.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: cool and unsuitable for baking\n",
      "Prediction:  cool and unsuitable for baking\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [02:08<00:50,  8.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: has loosened straps\n",
      "Prediction:  Loosened straps\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [02:21<00:49,  9.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: 3 AM\n",
      "Prediction:  5 AM\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [02:35<00:44, 11.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: has hardened\n",
      "Prediction:  soft and loose\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [02:50<00:37, 12.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: weakened and infested with termites\n",
      "Prediction:  Weakened and infested with\n",
      "Answer:\n",
      "Check: No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [03:08<00:28, 14.06s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: damaged with teeth marks\n",
      "Prediction:  Damaged with teeth marks (No\n",
      "Answer:\n",
      "Check: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [03:23<00:14, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: blunt and damaged\n",
      "Prediction:  Blunt and damaged\n",
      "Answer:\n",
      "Check: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:37<00:00, 10.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.65\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct, total = 0, 0\n",
    "layer_idx = 32\n",
    "\n",
    "with torch.inference_mode():\n",
    "    mask = torch.load(\"../masks/bigtom/32.pt\")\n",
    "    mask_data = mask.data.clone()\n",
    "    mask_data.clamp_(0, 1)\n",
    "    rounded = torch.round(mask)\n",
    "    print(f\"#Causal SVs: {(rounded == 1).sum().item()}\")\n",
    "    # rank[layer_idx] = (rounded == 1).sum().item()\n",
    "    # # Save rounded on disk\n",
    "    # torch.save(rounded, f\"../masks/bigtom/{layer_idx}.pt\")\n",
    "\n",
    "    for bi, batch in tqdm(enumerate(valid_dataloader), total=len(valid_dataloader)):\n",
    "        alt_prompt = batch[\"alt_prompt\"][0]\n",
    "        org_prompt = batch[\"org_prompt\"][0]\n",
    "        alt_ans = batch[\"alt_ans\"][0]\n",
    "        batch_size = 1\n",
    "\n",
    "        alt_ques_idx = get_ques_start_token_idx(model.tokenizer, alt_prompt)\n",
    "        alt_prompt_len = get_prompt_token_len(model.tokenizer, alt_prompt)\n",
    "        org_ques_idx = get_ques_start_token_idx(model.tokenizer, org_prompt)\n",
    "        org_prompt_len = get_prompt_token_len(model.tokenizer, org_prompt)\n",
    "\n",
    "        with model.session() as session:\n",
    "            # Cache alternative activations\n",
    "            alt_acts = defaultdict(dict)\n",
    "            with model.trace(alt_prompt):\n",
    "                for t_idx, t in enumerate(range(alt_ques_idx, alt_prompt_len)):\n",
    "                    alt_acts[t_idx] = model.model.layers[layer_idx].output[0][:, t].save()\n",
    "\n",
    "            # Process original prompt with modifications\n",
    "            with model.generate(org_prompt, max_new_tokens=8, do_sample=False, num_return_sequences=1, eos_token_id=model.tokenizer.eos_token_id):\n",
    "                sing_vec = sing_vecs[layer_idx].cuda()\n",
    "                masked_vec = sing_vec.to(rounded.device) * rounded.unsqueeze(-1)\n",
    "                proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "\n",
    "                # sing_vec = sing_vecs[layer_idx][:100, :].t().half()\n",
    "                # proj_matrix = torch.matmul(sing_vec, sing_vec.t())\n",
    "\n",
    "                for t_idx, t in enumerate(range(org_ques_idx, org_prompt_len)):\n",
    "                    curr_output = model.model.layers[layer_idx].output[0][:, t].clone()\n",
    "\n",
    "                    # Compute projections while maintaining gradients\n",
    "                    alt_proj = torch.matmul(alt_acts[t_idx], proj_matrix)\n",
    "                    org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "\n",
    "                    modified_out = curr_output - org_proj + alt_proj\n",
    "                    model.model.layers[layer_idx].output[0][:, t] = modified_out\n",
    "\n",
    "                out = model.generator.output.save()\n",
    "\n",
    "                del sing_vec, proj_matrix\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            check = check_pred(model.tokenizer.decode(out[0][org_prompt_len:-1]), alt_ans, verbose=True)\n",
    "            print(f\"Check: {check}\")\n",
    "            if check == \"Yes\":\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        \n",
    "        del alt_acts, alt_prompt, org_prompt, alt_ans, out\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Validation accuracy: {correct / total:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
