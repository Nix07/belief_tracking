{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "env.yml not found in /disk/u/nikhil/mind!\n",
      "Setting MODEL_ROOT=\"\". Models will now be downloaded to conda env cache, if not already there\n",
      "Other defaults are set to:\n",
      "    DATA_DIR = \"data\"\n",
      "    RESULTS_DIR = \"results\"\n",
      "    HPARAMS_DIR = \"hparams\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import Any, List, Optional\n",
    "import nnsight\n",
    "from nnsight import CONFIG, LanguageModel\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from einops import einsum\n",
    "import time\n",
    "from einops import rearrange, reduce\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from src.dataset import SampleV3, DatasetV3, STORY_TEMPLATES\n",
    "from src.utils import env_utils\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "random.seed(10)\n",
    "\n",
    "CONFIG.set_default_api_key(\"d9e00ab7d4f74643b3176de0913f24a7\")\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "CONFIG.APP.REMOTE_LOGGING = False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = {}\n",
    "all_containers= {}\n",
    "all_characters = json.load(open(os.path.join(env_utils.DEFAULT_DATA_DIR, \"synthetic_entities\", \"characters.json\"), \"r\"))\n",
    "\n",
    "for TYPE, DCT in {\"states\": all_states, \"containers\": all_containers}.items():\n",
    "    ROOT = os.path.join(\n",
    "        env_utils.DEFAULT_DATA_DIR, \"synthetic_entities\", TYPE\n",
    "    )\n",
    "    for file in os.listdir(ROOT):\n",
    "        file_path = os.path.join(ROOT, file)\n",
    "        with open(file_path, \"r\") as f:\n",
    "            names = json.load(f)\n",
    "        DCT[file.split(\".\")[0]] = names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [00:49<00:00,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# model = LanguageModel(\"meta-llama/Meta-Llama-3.1-405B\")\n",
    "model = LanguageModel(\"meta-llama/Meta-Llama-3-70B-Instruct\", cache_dir=\"/disk/u/nikhil/.cache/huggingface/hub/\", device_map=\"auto\", load_in_4bit=True, torch_dtype=torch.float16, dispatch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ques_start_token_idx(tokenizer, prompt):\n",
    "    if isinstance(prompt, list):\n",
    "        \n",
    "    input_tokens = tokenizer.encode(prompt, return_tensors=\"pt\").squeeze()\n",
    "    corrolary_token = tokenizer.encode(\":\", return_tensors=\"pt\").squeeze()[-1].item()\n",
    "    ques_start_idx = (input_tokens == corrolary_token).nonzero()[2].item()\n",
    "\n",
    "    return ques_start_idx-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_token_len(tokenizer, prompt):\n",
    "    input_tokens = tokenizer.encode(prompt, return_tensors=\"pt\").squeeze()\n",
    "    return len(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pred(pred, target, verbose=False):\n",
    "    prompt = f\"Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \\n\\nGround truth: {target}\\nPrediction: {pred}\\nAnswer:\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(prompt)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with model.generate(prompt, max_new_tokens=5, do_sample=False, num_return_sequences=1, pad_token_id=model.tokenizer.pad_token_id):\n",
    "            out = model.generator.output.save()\n",
    "\n",
    "    prompt_len = get_prompt_token_len(model.tokenizer, prompt)\n",
    "\n",
    "    return model.tokenizer.decode(out[0][prompt_len:-1]).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading BigToM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a csv file\n",
    "df_false = pd.read_csv(\"../data/bigtom/0_forward_belief_false_belief/stories.csv\", delimiter=\";\")\n",
    "df_true = pd.read_csv(\"../data/bigtom/0_forward_belief_true_belief/stories.csv\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each row in the dataframe extract story, answer, and distractor\n",
    "true_stories, false_stories = [], []\n",
    "for i in range(len(df_true)):\n",
    "    story = df_true.iloc[i]['story']\n",
    "    question = df_true.iloc[i]['question']\n",
    "    answer = df_true.iloc[i]['answer']\n",
    "    distractor = df_true.iloc[i]['distractor']\n",
    "    true_stories.append({\"story\": story, \"question\": question, \"answer\": answer, \"distractor\": distractor})\n",
    "\n",
    "for i in range(len(df_false)):\n",
    "    story = df_false.iloc[i]['story']\n",
    "    question = df_true.iloc[i]['question']\n",
    "    answer = df_false.iloc[i]['answer']\n",
    "    distractor = df_false.iloc[i]['distractor']\n",
    "    false_stories.append({\"story\": story, \"question\": question, \"answer\": answer, \"distractor\": distractor})\n",
    "\n",
    "dataset = []\n",
    "instruction = \"1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\"\n",
    "\n",
    "for i in range(min(len(true_stories), len(false_stories))):\n",
    "    question = true_stories[i]['question']\n",
    "    visible_prompt = f\"Instructions: {instruction}\\n\\nStory: {true_stories[i]['story']}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "    question = false_stories[i]['question']\n",
    "    invisible_prompt = f\"Instructions: {instruction}\\n\\nStory: {false_stories[i]['story']}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "    visible_ans = true_stories[i]['answer'].split()\n",
    "    invisible_ans = false_stories[i]['answer'].split()\n",
    "\n",
    "    # Find the index of first word which is different in both answers\n",
    "    diff_idx = 0\n",
    "    for idx, (v, j) in enumerate(zip(visible_ans, invisible_ans)):\n",
    "        if v != j:\n",
    "            diff_idx = idx\n",
    "            break\n",
    "    \n",
    "    visible_ans = \" \".join(visible_ans[diff_idx:])[:-1]\n",
    "    invisible_ans = \" \".join(invisible_ans[diff_idx:])[:-1]\n",
    "\n",
    "    visible_ans_one_word = \" \".join(visible_ans[diff_idx:diff_idx+1])\n",
    "    invisible_ans_one_word = \" \".join(invisible_ans[diff_idx:diff_idx+1])\n",
    "\n",
    "    visible_ans_one_word = visible_ans[:-1] if visible_ans[-1] == \".\" else visible_ans\n",
    "    invisible_ans_one_word = invisible_ans[:-1] if invisible_ans[-1] == \".\" else invisible_ans\n",
    "\n",
    "    dataset.append({\n",
    "        \"alt_story\": true_stories[i]['story'],\n",
    "        \"alt_question\": true_stories[i]['question'],\n",
    "        \"alt_prompt\": visible_prompt,\n",
    "        \"alt_ans\": visible_ans,\n",
    "        \"org_story\": false_stories[i]['story'],\n",
    "        \"org_question\": false_stories[i]['question'],\n",
    "        \"org_prompt\": invisible_prompt,\n",
    "        \"org_ans\": invisible_ans,\n",
    "        \"target\": visible_ans,\n",
    "    })\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\n",
      "\n",
      "Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor sees her coworker swapping the milk.\n",
      "Question: Does Noor believe the milk pitcher contains oat milk or almond milk?\n",
      "Answer: almond milk\n",
      "Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\n",
      "\n",
      "Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor does not see her coworker swapping the milk.\n",
      "Question: Does Noor believe the milk pitcher contains oat milk or almond milk?\n",
      "Answer: oat milk\n",
      "Target: almond milk\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(dataset[idx]['alt_prompt'], dataset[idx]['alt_ans'])\n",
    "print(dataset[idx]['org_prompt'], dataset[idx]['org_ans'])\n",
    "print(f\"Target: {dataset[idx]['target']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 80\n",
    "valid_size = 20\n",
    "batch_size = 1\n",
    "\n",
    "train_dataset = dataset[:train_size]\n",
    "valid_dataset = dataset[train_size:train_size+valid_size]\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Custom Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 40\n",
    "valid_size = 20\n",
    "batch_size = 4\n",
    "\n",
    "train_dataset = get_visibility_align_exps(STORY_TEMPLATES,\n",
    "                             all_characters,\n",
    "                             all_containers,\n",
    "                             all_states,\n",
    "                             train_size,\n",
    "                             question_type=\"belief_question\",\n",
    "                             diff_visibility=True)\n",
    "valid_dataset = get_visibility_align_exps(STORY_TEMPLATES,\n",
    "                             all_characters,\n",
    "                             all_containers,\n",
    "                             all_states,\n",
    "                             valid_size,\n",
    "                             question_type=\"belief_question\",\n",
    "                             diff_visibility=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=valid_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any beliefs about the container and its contents which they cannot observe. 4. To answer the question, predict only what is inside the queried container, strictly based on the belief of the character, mentioned in the question. 5. If the queried character has no belief about the container in question, then predict 'unknown'. 6. Do not predict container or character as the final output.\n",
      "\n",
      "Story: Max and Karen are working in a busy restaurant. To complete an order, Max grabs an opaque tun and fills it with coffee. Then Karen grabs another opaque dispenser and fills it with cocoa. They are working side by side and can clearly observe each other's actions.\n",
      "Question: What does Karen believe the tun contains?\n",
      "Answer: coffee\n",
      "Instruction: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any beliefs about the container and its contents which they cannot observe. 4. To answer the question, predict only what is inside the queried container, strictly based on the belief of the character, mentioned in the question. 5. If the queried character has no belief about the container in question, then predict 'unknown'. 6. Do not predict container or character as the final output.\n",
      "\n",
      "Story: Max and Karen are working in a busy restaurant. To complete an order, Max grabs an opaque tun and fills it with port. Then Karen grabs another opaque dispenser and fills it with water. They are working in the entirely separate sections, with no visibility between them.\n",
      "Question: What does Karen believe the tun contains?\n",
      "Answer: unknown\n",
      " port\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(train_dataset[idx]['corrupt_prompt'], train_dataset[idx]['corrupt_ans'])\n",
    "print(train_dataset[idx]['clean_prompt'], train_dataset[idx]['clean_ans'])\n",
    "print(train_dataset[idx]['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Singular Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sing_vecs = defaultdict(dict)\n",
    "for l in range(41):\n",
    "    sing_vecs[l] = torch.load(f\"../svd_results/bigtom/singular_vecs/{l}.pt\").cpu()\n",
    "    # sing_vecs[l] = torch.load(f\"../svd_results/toy/singular_vecs/{l}.pt\").cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_layers = [i for i in range(0, 30, 10)] + [i for i in range(22, 30, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\\n\\nStory: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor sees her coworker swapping the milk.\\nQuestion: Does Noor believe the milk pitcher contains oat milk or almond milk?\\nAnswer:\",\n",
       " \"Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\\n\\nStory: Zara is attending an art exhibition at a local gallery in her town. Zara wants to learn more about a particular abstract painting she admires. The label next to the artwork states that the painting is the work of a famous artist, and Zara reads the label carefully. A gallery staff member, realizing there was a mix-up, changes the label with the correct information attributing the painting to an emerging artist instead. Zara reads the updated label carefully.\\nQuestion: Does Zara believe the painting is the work of a famous artist or an emerging artist?\\nAnswer:\",\n",
       " \"Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\\n\\nStory: Carlos is a farmer in a small village in Mexico, tending to his cornfield. He wants to irrigate his crops by opening a water valve connected to a nearby river. Carlos closed the valve the previous evening. During the night, heavy rainfall caused the river to overflow, opening the valve and flooding the cornfield. Carlos sees the flooded cornfield.\\nQuestion: Does Carlos believe the water valve is closed or open?\\nAnswer:\",\n",
       " \"Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\\n\\nStory: Kofi is a fisherman from a small village in Ghana. He wants to catch enough fish today to provide for his family and sell the surplus at the market. Kofi repaired his fishing net last night. While Kofi is away from his boat, a group of monkeys comes and plays with the fishing net, tearing it apart. Kofi observes the monkeys damaging his fishing net.\\nQuestion: Does Kofi believe his fishing net is in good condition or torn apart?\\nAnswer:\"]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:777\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 777\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:739\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 739\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 4 at dim 1 (got 2)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[141], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m org_prompt \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     17\u001b[0m target \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m target_token \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minput_ids[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(target_token[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m target_token\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2868\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2866\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2867\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2868\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2870\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2956\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2951\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2952\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2953\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2954\u001b[0m         )\n\u001b[1;32m   2955\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2956\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2958\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2974\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2979\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2980\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2998\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2999\u001b[0m     )\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3158\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3148\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3149\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3150\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3151\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3156\u001b[0m )\n\u001b[0;32m-> 3158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3160\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3176\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3178\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:587\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_ids \u001b[38;5;129;01min\u001b[39;00m sanitized_tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n\u001b[0;32m--> 587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msanitized_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitized_encodings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:241\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    237\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:793\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    789\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    790\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    791\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    792\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    794\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    797\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    798\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "valid_accs, rank = {}, {}\n",
    "\n",
    "for layer_idx in range(32, 34, 2):\n",
    "    modules = [i for i in range(sing_vecs[0].shape[0])]\n",
    "    mask = torch.ones(len(modules), requires_grad=True, device=\"cuda\", dtype=torch.bfloat16)\n",
    "    optimizer = torch.optim.Adam([mask], lr=1e-1)\n",
    "    n_epochs = 3\n",
    "    lamb = 0.05\n",
    "\n",
    "    print(f\"Training layer: {layer_idx}\")\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for bi, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "            alt_prompt = batch[\"alt_prompt\"]\n",
    "            org_prompt = batch[\"org_prompt\"]\n",
    "            target = batch[\"target\"]\n",
    "            target_token = model.tokenizer(target, return_tensors=\"pt\", padding=True, padding_side=\"right\").input_ids[:, 1:]\n",
    "            # print(f\"Target: {model.tokenizer.decode(target_token[0])}\")\n",
    "            batch_size = target_token.shape[0]\n",
    "            # print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "            alt_ques_idx = get_ques_start_token_idx(model.tokenizer, alt_prompt)\n",
    "            alt_prompt_len = get_prompt_token_len(model.tokenizer, alt_prompt)\n",
    "            org_ques_idx = get_ques_start_token_idx(model.tokenizer, org_prompt)\n",
    "            org_prompt_len = get_prompt_token_len(model.tokenizer, org_prompt)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with model.trace() as tracer:\n",
    "                # Cache alternative activations\n",
    "                alt_acts = defaultdict(dict)\n",
    "                with tracer.invoke(alt_prompt):\n",
    "                    for t_idx, t in enumerate(range(alt_ques_idx, alt_prompt_len)):\n",
    "                        alt_acts[t_idx] = model.model.layers[layer_idx].output[0][:, t].clone()\n",
    "                \n",
    "                # Process original prompt with modifications\n",
    "                with tracer.invoke(org_prompt):\n",
    "                    sing_vec = sing_vecs[layer_idx].cuda()\n",
    "                    # Apply mask and ensure gradients flow\n",
    "                    masked_vec = sing_vec * mask.unsqueeze(-1)\n",
    "                    proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "                    \n",
    "                    for t in enumerate(range(org_ques_idx, org_prompt_len)):\n",
    "                        curr_output = model.model.layers[layer_idx].output[0][:, t].clone()\n",
    "                        \n",
    "                        # Compute projections while maintaining gradients\n",
    "                        alt_proj = torch.matmul(alt_acts[t_idx], proj_matrix)\n",
    "                        org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "\n",
    "                        modified_out = curr_output - org_proj + alt_proj\n",
    "                        model.model.layers[layer_idx].output[0][:, t] = modified_out\n",
    "                    \n",
    "                    del sing_vec, proj_matrix, masked_vec\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "                    logits = model.lm_head.output[:, -1].save()\n",
    "            \n",
    "            # Compute loss with L1 regularization\n",
    "            target_logit = logits[torch.arange(batch_size), target_token]\n",
    "            print(f\"Target logit: {target_logit}\")\n",
    "\n",
    "            task_loss = -torch.mean(target_logit)\n",
    "            l1_loss = lamb * torch.norm(mask, p=1)\n",
    "            loss = task_loss + l1_loss.to(task_loss.device)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if bi % (len(train_dataloader)//10) == 0:\n",
    "                mean_loss = epoch_loss / (bi + 1)\n",
    "                print(f\"Epoch: {epoch}, Batch: {bi}, Task Loss: {task_loss.item():.4f}, \"\n",
    "                    f\"L1 Loss: {l1_loss.item():.4f}, Total Loss: {mean_loss:.4f}\")\n",
    "                with torch.no_grad():\n",
    "                    mask.data.clamp_(0, 1)\n",
    "                    rounded = torch.round(mask)\n",
    "                    print(f\"#Causal SVs: {(rounded == 1).sum().item()}\")\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Clamp after optimizer step\n",
    "            with torch.no_grad():\n",
    "                mask.data.clamp_(0, 1)\n",
    "\n",
    "    print(f\"Training complete for {layer_idx}!\")\n",
    "\n",
    "    print(f\"Validation started for {layer_idx}\")\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        mask_data = mask.data.clone()\n",
    "        mask_data.clamp_(0, 1)\n",
    "        rounded = torch.round(mask_data)\n",
    "        print(f\"#Causal SVs: {(rounded == 1).sum().item()}\")\n",
    "        rank[layer_idx] = (rounded == 1).sum().item()\n",
    "        # Save rounded on disk\n",
    "        torch.save(rounded, f\"../masks/bigtom/{layer_idx}.pt\")\n",
    "\n",
    "        for bi, batch in tqdm(enumerate(valid_dataloader), total=len(valid_dataloader)):\n",
    "            alt_prompt = batch[\"alt_prompt\"][0]\n",
    "            org_prompt = batch[\"org_prompt\"][0]\n",
    "            alt_ans = batch[\"alt_ans\"][0]\n",
    "            batch_size = 1\n",
    "\n",
    "            alt_ques_idx = get_ques_start_token_idx(model.tokenizer, alt_prompt)\n",
    "            alt_prompt_len = get_prompt_token_len(model.tokenizer, alt_prompt)\n",
    "            org_ques_idx = get_ques_start_token_idx(model.tokenizer, org_prompt)\n",
    "            org_prompt_len = get_prompt_token_len(model.tokenizer, org_prompt)\n",
    "\n",
    "            with model.session() as session:\n",
    "                # Cache alternative activations\n",
    "                alt_acts = defaultdict(dict)\n",
    "                with model.trace(alt_prompt):\n",
    "                    for t_idx, t in enumerate(range(alt_ques_idx, alt_prompt_len)):\n",
    "                        alt_acts[t_idx] = model.model.layers[layer_idx].output[0][:, t].save()\n",
    "\n",
    "                # Process original prompt with modifications\n",
    "                with model.generate(org_prompt, max_new_tokens=8, do_sample=False, num_return_sequences=1, pad_token_id=model.tokenizer.pad_token_id, eos_token_id=model.tokenizer.eos_token_id):\n",
    "                    sing_vec = sing_vecs[layer_idx].cuda()\n",
    "                    masked_vec = sing_vec.to(rounded.device) * rounded.unsqueeze(-1)\n",
    "                    proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "\n",
    "                    # sing_vec = sing_vecs[layer_idx][:100, :].t().half()\n",
    "                    # proj_matrix = torch.matmul(sing_vec, sing_vec.t())\n",
    "\n",
    "                    for t_idx, t in enumerate(range(org_ques_idx, org_prompt_len)):\n",
    "                        curr_output = model.model.layers[layer_idx].output[0][:, t].clone()\n",
    "\n",
    "                        # Compute projections while maintaining gradients\n",
    "                        alt_proj = torch.matmul(alt_acts[t_idx], proj_matrix)\n",
    "                        org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "\n",
    "                        modified_out = curr_output - org_proj + alt_proj\n",
    "                        model.model.layers[layer_idx].output[0][:, t] = modified_out\n",
    "\n",
    "                    out = model.generator.output.save()\n",
    "\n",
    "                    del sing_vec, proj_matrix\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                check = check_pred(model.tokenizer.decode(out[0][org_prompt_len:-1]), alt_ans, verbose=True)\n",
    "                print(f\"Check: {check}\")\n",
    "                if check == \"Yes\":\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "            \n",
    "            del alt_acts, alt_prompt, org_prompt, alt_ans, out\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Validation accuracy: {correct / total:.2f}\\n\")\n",
    "    valid_accs[layer_idx] = round(correct / total, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15.3672,  3.3418, 16.8438, 17.6406]], device='cuda:7',\n",
       "       dtype=torch.float16, grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[torch.arange(batch_size), target_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Causal SVs: 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: the mole sauce is extremely spicy\n",
      "Prediction:  extremely spicy\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:07<02:19,  7.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: ruined with black paint\n",
      "Prediction:  smooth and flawless\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:14<02:13,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: the paintbrushes are ruined\n",
      "Prediction:  ruined paintbrushes\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:22<02:09,  7.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: damaged by the sandstorm\n",
      "Prediction:  Damaged by the sandstorm (\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:31<02:06,  7.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: filled with sand\n",
      "Prediction:  Filled with honey.\n",
      "Answer:\n",
      "Check: No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:42<02:15,  9.04s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: washed away and diluted\n",
      "Prediction:  Full and ready for harvest\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:51<02:07,  9.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: punctured and non-functional\n",
      "Prediction:  functional\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:58<01:50,  8.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: disrupted by fallen leaves\n",
      "Prediction:  Disrupted by fallen leaves.\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [01:06<01:40,  8.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: disrupted by wind and leaves\n",
      "Prediction:  perfectly raked\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [01:16<01:35,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: disarrayed by the wind\n",
      "Prediction:  Disarrayed by the wind.\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [01:26<01:33,  9.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: branches have been snapped off by the wind\n",
      "Prediction:  Overgrown branches\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [01:34<01:19,  8.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: torn\n",
      "Prediction:  torn\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [01:41<01:07,  8.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: there is a storm approaching\n",
      "Prediction:  Perfect for sailing\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [01:49<00:56,  8.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: cool and unsuitable for baking\n",
      "Prediction:  cool and unsuitable for baking\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [01:57<00:49,  8.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: has loosened straps\n",
      "Prediction:  Working condition\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [02:04<00:39,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: 3 AM\n",
      "Prediction:  5 AM\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [02:13<00:32,  8.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: has hardened\n",
      "Prediction:  soft and loose\n",
      "Answer:\n",
      "Check: No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [02:23<00:25,  8.61s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: weakened and infested with termites\n",
      "Prediction:  strong and free of imperfections\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [02:32<00:17,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: damaged with teeth marks\n",
      "Prediction:  Damaged with teeth marks (No\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [02:41<00:08,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \n",
      "\n",
      "Ground truth: blunt and damaged\n",
      "Prediction:  Blunt and damaged\n",
      "Answer:\n",
      "Check: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:52<00:00,  8.60s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'rint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m alt_acts, alt_prompt, org_prompt, alt_ans, out\n\u001b[1;32m     62\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 64\u001b[0m \u001b[43mrint\u001b[49m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtotal\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rint' is not defined"
     ]
    }
   ],
   "source": [
    "correct, total = 0, 0\n",
    "\n",
    "with torch.inference_mode():\n",
    "    mask_data = mask.data.clone()\n",
    "    mask_data.clamp_(0, 1)\n",
    "    rounded = torch.round(mask_data)\n",
    "    print(f\"#Causal SVs: {(rounded == 1).sum().item()}\")\n",
    "    rank[layer_idx] = (rounded == 1).sum().item()\n",
    "    # Save rounded on disk\n",
    "    torch.save(rounded, f\"../masks/bigtom/{layer_idx}.pt\")\n",
    "\n",
    "    for bi, batch in tqdm(enumerate(valid_dataloader), total=len(valid_dataloader)):\n",
    "        alt_prompt = batch[\"alt_prompt\"][0]\n",
    "        org_prompt = batch[\"org_prompt\"][0]\n",
    "        alt_ans = batch[\"alt_ans\"][0]\n",
    "        batch_size = 1\n",
    "\n",
    "        alt_ques_idx = get_ques_start_token_idx(model.tokenizer, alt_prompt)\n",
    "        alt_prompt_len = get_prompt_token_len(model.tokenizer, alt_prompt)\n",
    "        org_ques_idx = get_ques_start_token_idx(model.tokenizer, org_prompt)\n",
    "        org_prompt_len = get_prompt_token_len(model.tokenizer, org_prompt)\n",
    "\n",
    "        with model.session() as session:\n",
    "            # Cache alternative activations\n",
    "            alt_acts = defaultdict(dict)\n",
    "            with model.trace(alt_prompt):\n",
    "                for t_idx, t in enumerate(range(alt_ques_idx, alt_prompt_len)):\n",
    "                    alt_acts[t_idx] = model.model.layers[layer_idx].output[0][:, t].save()\n",
    "\n",
    "            # Process original prompt with modifications\n",
    "            with model.generate(org_prompt, max_new_tokens=8, do_sample=False, num_return_sequences=1, eos_token_id=model.tokenizer.eos_token_id):\n",
    "                sing_vec = sing_vecs[layer_idx].cuda()\n",
    "                masked_vec = sing_vec.to(rounded.device) * rounded.unsqueeze(-1)\n",
    "                proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "\n",
    "                # sing_vec = sing_vecs[layer_idx][:100, :].t().half()\n",
    "                # proj_matrix = torch.matmul(sing_vec, sing_vec.t())\n",
    "\n",
    "                for t_idx, t in enumerate(range(org_ques_idx, org_prompt_len)):\n",
    "                    curr_output = model.model.layers[layer_idx].output[0][:, t].clone()\n",
    "\n",
    "                    # Compute projections while maintaining gradients\n",
    "                    alt_proj = torch.matmul(alt_acts[t_idx], proj_matrix)\n",
    "                    org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "\n",
    "                    modified_out = curr_output - org_proj + alt_proj\n",
    "                    model.model.layers[layer_idx].output[0][:, t] = modified_out\n",
    "\n",
    "                out = model.generator.output.save()\n",
    "\n",
    "                del sing_vec, proj_matrix\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            check = check_pred(model.tokenizer.decode(out[0][org_prompt_len:-1]), alt_ans, verbose=True)\n",
    "            print(f\"Check: {check}\")\n",
    "            if check == \"Yes\":\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        \n",
    "        del alt_acts, alt_prompt, org_prompt, alt_ans, out\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Validation accuracy: {correct / total:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
