{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "from src.functional import free_gpu_cache, predict_next_token\n",
    "from src.utils import env_utils\n",
    "\n",
    "from src.models import load_LM\n",
    "import torch\n",
    "from nnsight import LanguageModel\n",
    "from src.utils import env_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_KEY = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "svd_path = os.path.join(\n",
    "    env_utils.DEFAULT_RESULTS_DIR, \"SVDs\",\n",
    "    MODEL_KEY.split(\"/\")[-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_binding_ID(\n",
    "    h_orig: torch.Tensor,\n",
    "    h_other: torch.Tensor,\n",
    "    Proj: torch.Tensor,\n",
    "    rank: int = 10\n",
    "):\n",
    "    # The prediction will also change if the full h_other is patched in stead of h_orig\n",
    "    h_orig = h_orig.to(torch.float32)\n",
    "    h_other = h_other.to(torch.float32)\n",
    "    Proj = Proj.to(torch.float32)\n",
    "\n",
    "    Proj_r = Proj[:rank].to(lm.device)\n",
    "    Proj_r = Proj_r.T @ Proj_r\n",
    "    return (\n",
    "        (torch.eye(Proj_r.shape[0]).to(lm.device) - Proj_r) @ h_orig # remove patching ID from s_orig\n",
    "        + Proj_r @ h_other # add patching ID from s_other\n",
    "    )\n",
    "\n",
    "def remove_binding_ID(\n",
    "    h: torch.Tensor,\n",
    "    Proj: torch.Tensor,\n",
    "    rank: int = 10\n",
    "):\n",
    "    h = h.to(torch.float32)\n",
    "    Proj = Proj.to(torch.float32)\n",
    "\n",
    "    Proj_r = Proj[:rank].to(lm.device)\n",
    "    Proj_r = Proj_r.T @ Proj_r\n",
    "    return (torch.eye(Proj_r.shape[0]).to(lm.device) - Proj_r) @ h\n",
    "\n",
    "\n",
    "def load_Vh(type: str, layer_name: str):\n",
    "    svd = np.load(os.path.join(svd_path, type, \"projections\", f\"{layer_name}.npz\"))\n",
    "    Vh = torch.Tensor(svd['Vh'])\n",
    "    return Vh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [01:03<00:00,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded /home/local_arnab/Codes/00_MODEL/meta-llama/Meta-Llama-3-70B-Instruct | size: 36650.535 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lm = load_LM(\n",
    "    model_key=MODEL_KEY,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample.true_state={'hat': 'pen', 'box': 'pen'}\n",
      "sample.protagonist_belief={'hat': 'knife', 'box': 'pen'}\n"
     ]
    }
   ],
   "source": [
    "from src.dataset import SampleV3, DatasetV3\n",
    "\n",
    "sample = SampleV3(\n",
    "    protagonist=\"Nora\",\n",
    "    perpetrator=\"Jon\",\n",
    "    objects=[\"knife\", \"pen\"],\n",
    "    containers=[\"hat\", \"box\"],\n",
    "    event_idx=0,\n",
    "    event_noticed=False\n",
    ")\n",
    "\n",
    "print(f\"{sample.true_state=}\")\n",
    "print(f\"{sample.protagonist_belief=}\")\n",
    "\n",
    "dataset = DatasetV3(samples = [sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer:\" tag.\n",
      "\n",
      "Story: Nora is a magician performing at a grand theater. Nora wants to amaze the audience with a trick involving a knife. Nora places the knife in a hat and sets it on the stage. Then Nora prepares a backup box and places a pen inside. An assistant named Jon, who thinks the trick should be different, swaps the knife in the hat with the pen while Nora is backstage. Nora didn't see Jon swapping the the contents of hat.\n",
      "Question: Does Jon believe the hat contains knife?\n",
      "Answer:\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "from scripts.collect_binding_id_states import collect_token_latent_in_question\n",
    "\n",
    "prompt, answer = dataset.__getitem__(\n",
    "    0, \n",
    "    set_actor=\"perpetrator\",\n",
    "    # set_ans=\"no\",\n",
    "    set_container=0,\n",
    "    set_obj=0\n",
    ")\n",
    "print(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_perpetrator = collect_token_latent_in_question(\n",
    "    lm = lm,\n",
    "    prompt = prompt_perp,\n",
    "    answer = answer_perp,\n",
    "    token_of_interest= sample.perpetrator,\n",
    "    detensorize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_gpu_cache()\n",
    "\n",
    "patches = []\n",
    "for layer_idx in range(13, 20):\n",
    "    layer = LAYER_NAME_FORMAT.format(layer_idx)\n",
    "    Vh = load_Vh(\"characters\", layer)\n",
    "    location = (layer, QUESTION_ACTOR_IDX)\n",
    "    patches.append(PatchSpec(\n",
    "        location=location,\n",
    "        patch=patch_binding_ID(\n",
    "            h_orig=h_perpetrator.states[location],\n",
    "            h_other=h_protagonist.states[location],\n",
    "            Proj=Vh,\n",
    "            rank=3\n",
    "        )\n",
    "    ))\n",
    "\n",
    "predict_next_token(\n",
    "    lm=lm, \n",
    "    input=prompt_perp,\n",
    "    patches=patches,\n",
    "    interested_tokens=[14410]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
