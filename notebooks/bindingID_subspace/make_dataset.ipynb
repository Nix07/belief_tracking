{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "from src.functional import free_gpu_cache\n",
    "from src.utils import env_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [00:59<00:00,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded /home/local_arnab/Codes/00_MODEL/meta-llama/Meta-Llama-3-70B-Instruct | size: 36650.535 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.models import load_LM\n",
    "import torch\n",
    "\n",
    "# model_key = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_key = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "lm = load_LM(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer:\" tag.\n",
      "\n",
      "Story: Adam is working in a busy restaurant. A customer asks Adam for oat milk. Adam grabs an opaque pitcher and fills it with oat milk. Then Adam grabs another opaque cup and fills it with almond milk. A coworker named Bob observes Adam pouring the contents in the pitcher and the cup. But Bob didn't hear the customer's request and swaps the oat milk in the pitcher with almond milk while Adam was attending to another task. Adam can't see what is in the pitcher and the cup without opening their lid. Adam saw Bob swapping the the contents of pitcher.\n",
      "Question: Does Adam believe the cup contains oat milk?\n",
      "Answer:\n",
      "no\n",
      "sample.true_state={'pitcher': 'almond milk', 'cup': 'almond milk'}\n",
      "sample.protagonist_belief={'pitcher': 'almond milk', 'cup': 'almond milk'}\n"
     ]
    }
   ],
   "source": [
    "from src.dataset import SampleV3, DatasetV3\n",
    "\n",
    "sample = SampleV3(\n",
    "    protagonist=\"Adam\",\n",
    "    perpetrator=\"Bob\",\n",
    "    objects=[\"oat milk\", \"almond milk\"],\n",
    "    containers=[\"pitcher\", \"cup\"],\n",
    "    event_idx=0,\n",
    "    event_noticed=True\n",
    ")\n",
    "\n",
    "dataset = DatasetV3(samples = [sample])\n",
    "prompt, answer = dataset.__getitem__(\n",
    "    0, set_ans=\"no\",\n",
    "    # set_actor=\"perpetrator\"\n",
    "    set_container=1\n",
    ")\n",
    "print(prompt)\n",
    "print(answer)\n",
    "\n",
    "print(f\"{sample.true_state=}\")\n",
    "print(f\"{sample.protagonist_belief=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' No', prob=0.41574540734291077, logit=19.625, token_id=2360),\n",
       " PredictedToken(token=' NO', prob=0.3612058758735657, logit=19.484375, token_id=5782),\n",
       " PredictedToken(token=' no', prob=0.15294420719146729, logit=18.625, token_id=912),\n",
       " PredictedToken(token=' **', prob=0.029649581760168076, logit=16.984375, token_id=3146),\n",
       " PredictedToken(token=' YES', prob=0.008363048546016216, logit=15.71875, token_id=14410)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import predict_next_token\n",
    "\n",
    "predict_next_token(\n",
    "    lm=lm,\n",
    "    input=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.path.join(\n",
    "    env_utils.DEFAULT_DATA_DIR, \"synthetic_entities\",\n",
    "    \"object.json\"\n",
    ")\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    names = json.load(f)\n",
    "\n",
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'filtered 109 names to 30 single token names'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import prepare_input\n",
    "single_token_names = []\n",
    "\n",
    "for name in names:\n",
    "    tok = prepare_input(prompts=f\" {name}\", tokenizer=lm)\n",
    "    if tok.input_ids.shape[1] == 2 and tok.input_ids[0][0] == lm.tokenizer.bos_token_id:\n",
    "        single_token_names.append(name)\n",
    "\n",
    "f\"filtered {len(names)} names to {len(single_token_names)} single token names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['water',\n",
       " 'milk',\n",
       " 'tea',\n",
       " 'beer',\n",
       " 'soda',\n",
       " 'juice',\n",
       " 'coffee',\n",
       " 'wine',\n",
       " 'whiskey',\n",
       " 'vodka',\n",
       " 'gin',\n",
       " 'rum',\n",
       " 'champagne',\n",
       " 'cider',\n",
       " 'cocktail',\n",
       " 'punch',\n",
       " 'espresso',\n",
       " 'cocoa',\n",
       " 'cola',\n",
       " 'sprite',\n",
       " 'monster',\n",
       " 'bourbon',\n",
       " 'sake',\n",
       " 'port',\n",
       " 'float',\n",
       " 'fizz',\n",
       " 'sling',\n",
       " 'stout',\n",
       " 'ale',\n",
       " 'porter']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file_path, \"w\") as f:\n",
    "    json.dump(list(single_token_names), f)\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    names = json.load(f)\n",
    "\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 30, 23)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "root = os.path.join(\n",
    "    env_utils.DEFAULT_DATA_DIR, \"synthetic_entities\"\n",
    ")\n",
    "actors = json.load(open(os.path.join(root, \"actor.json\")))\n",
    "objects = json.load(open(os.path.join(root, \"object.json\")))\n",
    "containers = json.load(open(os.path.join(root, \"container.json\")))\n",
    "\n",
    "actors = list(set(actors))\n",
    "objects = list(set(objects))\n",
    "containers = list(set(containers))\n",
    "\n",
    "len(actors), len(objects), len(containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "actorsC2 = list(itertools.combinations(actors, 2))\n",
    "objectsC2 = list(itertools.combinations(objects, 2))\n",
    "containersC2 = list(itertools.combinations(containers, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered to 1000 samples from 1000\n"
     ]
    }
   ],
   "source": [
    "limit = 1000\n",
    "\n",
    "configs = []\n",
    "while len(configs) < limit:\n",
    "    protagonist, perpetrator = random.choice(actorsC2)\n",
    "    object1, object2 = random.choice(objectsC2)\n",
    "    container1, container2 = random.choice(containersC2)\n",
    "\n",
    "    # protagonists belief != true state only if event_idx == 0 and event_noticed == False\n",
    "    # roughly 64% of the time protagonist believes will be false\n",
    "    event_idx = random.choices([0,1], weights=[0.8, 0.2], k = 1)[0]\n",
    "    event_noticed = random.choices([False, True], weights=[0.8, 0.2], k = 1)[0]\n",
    "\n",
    "    configs.append((\n",
    "        protagonist, perpetrator,\n",
    "        object1, object2,\n",
    "        container1, container2,\n",
    "        event_idx, event_noticed\n",
    "    ))\n",
    "\n",
    "    if len(configs) % 1000 == 0:\n",
    "        pre_len = len(configs)\n",
    "        configs = list(set(configs))\n",
    "        print(f\"filtered to {len(configs)} samples from {pre_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.dataset import SampleV3, DatasetV3\n",
    "samples: list[SampleV3] = []\n",
    "for config in configs:\n",
    "    sample = SampleV3(\n",
    "        protagonist=config[0],\n",
    "        perpetrator=config[1],\n",
    "        objects=[config[2], config[3]],\n",
    "        containers=[config[4], config[5]],\n",
    "        event_idx=config[6],\n",
    "        event_noticed=config[7]\n",
    "    )\n",
    "    samples.append(sample)\n",
    "\n",
    "dataset = DatasetV3(samples=samples)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer (yes/no):\" tag.\n",
      "\n",
      "Story: Neil is working in a busy restaurant. A customer asks Neil for monster. Neil grabs an opaque flask and fills it with monster. Then Neil grabs another opaque drum and fills it with milk. A coworker named Steve observes Neil pouring the contents in the flask and the drum. But Steve didn't hear the customer's request and swaps the milk in the drum with milk while Neil was attending to another task. Neil can't see what is in the flask and the drum without opening their lid. Neil didn't see Steve swapping the the contents of drum.\n",
      "\n",
      "Question: Does Neil believe that there is monster in the flask?\n",
      "Answer:\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "# question, answer = dataset[2]\n",
    "question, answer = dataset.__getitem__(2, set_ans=\"yes\")\n",
    "print(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import env_utils\n",
    "\n",
    "with open(\n",
    "    os.path.join(env_utils.DEFAULT_DATA_DIR, \"dataset_v3.json\"), \"w\") as f:\n",
    "    json.dump(dataset.to_dict(), f\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open (os.path.join(env_utils.DEFAULT_DATA_DIR, \"dataset_v3.json\"), \"r\") as f:\n",
    "    dataset_dict = json.load(f)\n",
    "\n",
    "loaded_dataset = DatasetV3.from_dict(dataset_dict)\n",
    "len(loaded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scripts.collect_binding_id_states import ExperimentResults\n",
    "\n",
    "# cache_path = os.path.join(\n",
    "#     env_utils.DEFAULT_RESULTS_DIR, \n",
    "#     \"binding_id_states\",\n",
    "#     \"Meta-Llama-3-70B-Instruct\", \n",
    "#     \"results.json\"\n",
    "# )\n",
    "# with open(cache_path, \"r\") as f:\n",
    "#     results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = ExperimentResults.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res.cached_states[5].context_informed_actor['model.layers.79_<>_155'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 ^ 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
