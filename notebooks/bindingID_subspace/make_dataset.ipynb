{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdms\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "from src.functional import free_gpu_cache\n",
    "from src.utils import env_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [01:05<00:00,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded /home/local_arnab/Codes/00_MODEL/meta-llama/Meta-Llama-3-70B-Instruct | size: 36650.535 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.models import load_LM\n",
    "import torch\n",
    "\n",
    "# model_key = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_key = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "lm = load_LM(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample.true_state={'pitcher': 'almond milk', 'cup': 'almond milk'}\n",
      "sample.protagonist_belief={'pitcher': 'oat milk', 'cup': 'almond milk'}\n",
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer:\" tag.\n",
      "\n",
      "Story: Adam is working in a busy restaurant. A customer asks Adam for oat milk. Adam grabs an opaque pitcher and fills it with oat milk. Then Adam grabs another opaque cup and fills it with almond milk. A coworker named Bob observes Adam pouring the contents in the pitcher and the cup. But Bob didn't hear the customer's request and swaps the oat milk in the pitcher with almond milk while Adam was attending to another task. Adam can't see what is in the pitcher and the cup without opening their lid. Adam didn't see Bob swapping the the contents of pitcher.\n",
      "Question: Does Adam believe the cup contains almond milk?\n",
      "Answer:\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "from src.dataset import SampleV3, DatasetV3\n",
    "\n",
    "sample = SampleV3(\n",
    "    protagonist=\"Adam\",\n",
    "    perpetrator=\"Bob\",\n",
    "    objects=[\"oat milk\", \"almond milk\"],\n",
    "    containers=[\"pitcher\", \"cup\"],\n",
    "    event_idx=0,\n",
    "    event_noticed=False\n",
    ")\n",
    "\n",
    "print(f\"{sample.true_state=}\")\n",
    "print(f\"{sample.protagonist_belief=}\")\n",
    "\n",
    "dataset = DatasetV3(samples = [sample])\n",
    "prompt, answer = dataset.__getitem__(\n",
    "    0, \n",
    "    set_actor=\"protagonist\",\n",
    "    # set_ans=\"no\",\n",
    "    # set_container=0,\n",
    "    set_obj=1\n",
    ")\n",
    "print(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' No', prob=0.5262770652770996, logit=20.78125, token_id=2360),\n",
       " PredictedToken(token=' NO', prob=0.22283974289894104, logit=19.921875, token_id=5782),\n",
       " PredictedToken(token=' no', prob=0.20289787650108337, logit=19.828125, token_id=912),\n",
       " PredictedToken(token=' **', prob=0.03160539269447327, logit=17.96875, token_id=3146),\n",
       " PredictedToken(token=' __', prob=0.005000699311494827, logit=16.125, token_id=1328)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import predict_next_token\n",
    "\n",
    "predict_next_token(\n",
    "    lm=lm,\n",
    "    input=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.path.join(\n",
    "    env_utils.DEFAULT_DATA_DIR, \"synthetic_entities\",\n",
    "    \"object.json\"\n",
    ")\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    names = json.load(f)\n",
    "\n",
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'filtered 30 names to 30 single token names'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import prepare_input\n",
    "single_token_names = []\n",
    "\n",
    "for name in names:\n",
    "    tok = prepare_input(prompts=f\" {name}\", tokenizer=lm)\n",
    "    if tok.input_ids.shape[1] == 2 and tok.input_ids[0][0] == lm.tokenizer.bos_token_id:\n",
    "        single_token_names.append(name)\n",
    "\n",
    "f\"filtered {len(names)} names to {len(single_token_names)} single token names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['water',\n",
       " 'milk',\n",
       " 'tea',\n",
       " 'beer',\n",
       " 'soda',\n",
       " 'juice',\n",
       " 'coffee',\n",
       " 'wine',\n",
       " 'whiskey',\n",
       " 'vodka',\n",
       " 'gin',\n",
       " 'rum',\n",
       " 'champagne',\n",
       " 'cider',\n",
       " 'cocktail',\n",
       " 'punch',\n",
       " 'espresso',\n",
       " 'cocoa',\n",
       " 'cola',\n",
       " 'sprite',\n",
       " 'monster',\n",
       " 'bourbon',\n",
       " 'sake',\n",
       " 'port',\n",
       " 'float',\n",
       " 'fizz',\n",
       " 'sling',\n",
       " 'stout',\n",
       " 'ale',\n",
       " 'porter']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file_path, \"w\") as f:\n",
    "    json.dump(list(single_token_names), f)\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    names = json.load(f)\n",
    "\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 30, 21)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "root = os.path.join(\n",
    "    env_utils.DEFAULT_DATA_DIR, \"synthetic_entities\"\n",
    ")\n",
    "actors = json.load(open(os.path.join(root, \"actor.json\")))\n",
    "objects = json.load(open(os.path.join(root, \"object.json\")))\n",
    "containers = json.load(open(os.path.join(root, \"container.json\")))\n",
    "\n",
    "actors = list(set(actors))\n",
    "objects = list(set(objects))\n",
    "containers = list(set(containers))\n",
    "\n",
    "len(actors), len(objects), len(containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "actorsC2 = list(itertools.combinations(actors, 2))\n",
    "objectsC2 = list(itertools.combinations(objects, 2))\n",
    "containersC2 = list(itertools.combinations(containers, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered to 1000 samples from 1000\n",
      "filtered to 2000 samples from 2000\n",
      "filtered to 3000 samples from 3000\n",
      "filtered to 4000 samples from 4000\n",
      "filtered to 5000 samples from 5000\n",
      "filtered to 6000 samples from 6000\n",
      "filtered to 7000 samples from 7000\n",
      "filtered to 8000 samples from 8000\n",
      "filtered to 9000 samples from 9000\n",
      "filtered to 10000 samples from 10000\n"
     ]
    }
   ],
   "source": [
    "limit = 10000\n",
    "\n",
    "configs = []\n",
    "while len(configs) < limit:\n",
    "    protagonist, perpetrator = random.choice(actorsC2)\n",
    "    object1, object2 = random.choice(objectsC2)\n",
    "    container1, container2 = random.choice(containersC2)\n",
    "\n",
    "    # changing th econtents of the second container has not related to the customer's order.\n",
    "    event_idx = random.choices([0,1], weights=[0.6, 0.4], k = 1)[0]\n",
    "\n",
    "    # protagonists belief != true state only if event_noticed == False\n",
    "    event_noticed = random.choices([False, True], weights=[0.7, 0.3], k = 1)[0]\n",
    "\n",
    "    configs.append((\n",
    "        protagonist, perpetrator,\n",
    "        object1, object2,\n",
    "        container1, container2,\n",
    "        event_idx, event_noticed\n",
    "    ))\n",
    "\n",
    "    if len(configs) % 1000 == 0:\n",
    "        pre_len = len(configs)\n",
    "        configs = list(set(configs))\n",
    "        print(f\"filtered to {len(configs)} samples from {pre_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.dataset import SampleV3, DatasetV3\n",
    "samples: list[SampleV3] = []\n",
    "for config in configs:\n",
    "    sample = SampleV3(\n",
    "        protagonist=config[0],\n",
    "        perpetrator=config[1],\n",
    "        objects=[config[2], config[3]],\n",
    "        containers=[config[4], config[5]],\n",
    "        event_idx=config[6],\n",
    "        event_noticed=config[7]\n",
    "    )\n",
    "    samples.append(sample)\n",
    "\n",
    "dataset = DatasetV3(samples=samples)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer:\" tag.\n",
      "\n",
      "Story: Josh is working in a busy restaurant. A customer asks Josh for espresso. Josh grabs an opaque mug and fills it with espresso. Then Josh grabs another opaque can and fills it with beer. A coworker named Greg observes Josh pouring the contents in the mug and the can. But Greg didn't hear the customer's request and swaps the espresso in the mug with beer while Josh was attending to another task. Josh can't see what is in the mug and the can without opening their lid. Josh didn't see Greg swapping the the contents of mug.\n",
      "Question: Does Josh believe the mug contains espresso?\n",
      "Answer:\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "# question, answer = dataset[2]\n",
    "question, answer = dataset.__getitem__(2, set_ans=\"yes\")\n",
    "print(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import env_utils\n",
    "\n",
    "with open(\n",
    "    os.path.join(env_utils.DEFAULT_DATA_DIR, \"dataset_v3.json\"), \"w\") as f:\n",
    "    json.dump(dataset.to_dict(), f\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open (os.path.join(env_utils.DEFAULT_DATA_DIR, \"dataset_v3.json\"), \"r\") as f:\n",
    "    dataset_dict = json.load(f)\n",
    "\n",
    "loaded_dataset = DatasetV3.from_dict(dataset_dict)\n",
    "len(loaded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scripts.collect_binding_id_states import ExperimentResults\n",
    "\n",
    "# cache_path = os.path.join(\n",
    "#     env_utils.DEFAULT_RESULTS_DIR, \n",
    "#     \"binding_id_states\",\n",
    "#     \"Meta-Llama-3-70B-Instruct\", \n",
    "#     \"results.json\"\n",
    "# )\n",
    "# with open(cache_path, \"r\") as f:\n",
    "#     results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = ExperimentResults.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res.cached_states[5].context_informed_actor['model.layers.79_<>_155'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['something']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"something\"\n",
    "s.split(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
