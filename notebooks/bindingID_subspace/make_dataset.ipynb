{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "from src.functional import free_gpu_cache\n",
    "from src.utils import env_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_template = \"<actor> works in a busy restaurant. A customer asks <actor> for <obj_belief>. <actor> grabs an opaque <container> and fills it with <obj_belief>. A coworker, who didn't hear the customer's request, swaps the <obj_belief> in the <container> with <obj_true> while <actor> is attending to another task. <actor> does not see the coworker swapping the contents in the <container> and also can't see what is in it without opening it.\"\n",
    "\n",
    "container_options = [\n",
    "    \"pitcher\", \"bottle\", \"flask\", # \"vial\", \"beaker\", \"jug\", \"cup\", \"mug\", \"glass\", \"tumbler\", \"thermos\", \"carafe\", \"decanter\", \"growler\", \"canteen\", \"gourd\", \"goblet\", \"stein\", \"chalice\", \"tankard\", \"shot glass\", \"snifter\",\n",
    "]\n",
    "milk_options = [\n",
    "    \"oat milk\", \"almond milk\", \"soy milk\", \"rice milk\", \"whole milk\", \n",
    "]\n",
    "juice_options = [\n",
    "    \"orange juice\", \"mango juice\", \"apple juice\", \"grape juice\", \"cranberry juice\", \"pomegranate juice\", \"guava juice\", \"passion fruit juice\", \"lychee juice\", \"kiwi juice\", \"watermelon juice\", \"cantaloupe juice\", \"honeydew juice\", \"pineapple juice\", \"coconut juice\", \"papaya juice\", \"pear juice\", \"peach juice\", \"plum juice\", \"apricot juice\", \"cherry juice\", \"blueberry juice\", \"blackberry juice\", \"raspberry juice\", \"strawberry juice\"\n",
    "]\n",
    "alcohol_options = [\n",
    "    \"beer\", \"wine\", \"whiskey\", \"vodka\", \"rum\", \"tequila\", \"gin\", \"brandy\", \"cognac\", \"liqueur\", \"absinthe\", \"sake\", \"soju\", \"baijiu\", \"shochu\", \"akvavit\", \"aquavit\", \"arak\", \"arrack\", \"bitters\", \"bourbon\", \"calvados\", \"cachaça\", \"grappa\", \"mezcal\", \"pisco\", \"rakia\", \"slivovitz\", \"ouzo\", \"raki\", \"schnapps\", \"scotch\", \"vermouth\", \"amaretto\", \"baileys\", \"chartreuse\", \"cointreau\", \"curacao\", \"frangelico\", \"grand marnier\", \"kahlua\", \"sambuca\", \"triple sec\", \"campari\", \"jagermeister\", \"malibu\", \"midori\", \"ouzo\", \"pimms\", \"southern comfort\", \"tuaca\", \"umbrella drink\", \"zima\", \"zombie\", \"zinfandel\", \"zinger\"\n",
    "]\n",
    "obj_options = [\n",
    "    set(milk_options), set(juice_options), set(alcohol_options)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded /home/local_arnab/Codes/00_MODEL/meta-llama/Meta-Llama-3-8B-Instruct | size: 5332.516 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.models import load_LM\n",
    "import torch\n",
    "\n",
    "model_key = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_key = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "lm = load_LM(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(env_utils.DEFAULT_DATA_DIR, \"english_names.json\"), \"r\") as f:\n",
    "    names = json.load(f)\n",
    "\n",
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'filtered 103 names to 103 single token names'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import prepare_input\n",
    "single_token_names = []\n",
    "\n",
    "for name in names:\n",
    "    tok = prepare_input(prompts=f\" {name}\", tokenizer=lm)\n",
    "    if tok.input_ids.shape[1] == 2 and tok.input_ids[0][0] == lm.tokenizer.bos_token_id:\n",
    "        single_token_names.append(name)\n",
    "\n",
    "f\"filtered {len(names)} names to {len(single_token_names)} single token names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(env_utils.DEFAULT_DATA_DIR, \"english_names.json\"), \"w\") as f:\n",
    "    json.dump(list(single_token_names), f)\n",
    "\n",
    "with open(os.path.join(env_utils.DEFAULT_DATA_DIR, \"english_names.json\"), \"r\") as f:\n",
    "    names = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5561"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.dataset import SampleV2\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "obj_choose_2 = {\n",
    "    \"milk\": list(itertools.combinations(milk_options, 2)),\n",
    "    \"juice\": list(itertools.combinations(juice_options, 2)),\n",
    "    \"alcohol\": list(itertools.combinations(alcohol_options, 2)),\n",
    "}\n",
    "\n",
    "unique_samples = []\n",
    "\n",
    "for actor in names:\n",
    "    for container in container_options:\n",
    "        for obj_type in obj_choose_2:\n",
    "            obj_pairs = random.sample(obj_choose_2[obj_type], 6)\n",
    "            for obj_belief, obj_true in obj_pairs:\n",
    "                story = story_template.replace(\"<actor>\", actor)\n",
    "                story = story.replace(\"<container>\", container)\n",
    "                story = story.replace(\"<obj_belief>\", obj_belief)\n",
    "                story = story.replace(\"<obj_true>\", obj_true)\n",
    "        \n",
    "                sample = SampleV2(\n",
    "                    story=story,\n",
    "                    actor=actor,\n",
    "                    container=container,\n",
    "                    obj_belief=obj_belief,\n",
    "                    obj_true=obj_true,\n",
    "                )\n",
    "\n",
    "                unique_samples.append(sample)\n",
    "    #         break\n",
    "    #     break\n",
    "    # break\n",
    "\n",
    "unique_samples = list(set(unique_samples))\n",
    "\n",
    "len(unique_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5561"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import DatasetV2\n",
    "\n",
    "dataset = DatasetV2(samples = unique_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer:\" tag.\n",
      "\n",
      "Story: Frank works in a busy restaurant. A customer asks Frank for rice milk. Frank grabs an opaque flask and fills it with rice milk. A coworker, who didn't hear the customer's request, swaps the rice milk in the flask with whole milk while Frank is attending to another task. Frank does not see the coworker swapping the contents in the flask and also can't see what is in it without opening it.\n",
      "\n",
      "Question: Does Frank believe that there is rice milk in the flask?\n",
      "Answer:\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "# question, answer = dataset[2]\n",
    "question, answer = dataset.__getitem__(2, set_ans=\"yes\")\n",
    "print(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import env_utils\n",
    "\n",
    "with open(os.path.join(env_utils.DEFAULT_DATA_DIR, \"dataset_v2.json\"), \"w\") as f:\n",
    "    json.dump(dataset.to_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5561"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open (os.path.join(env_utils.DEFAULT_DATA_DIR, \"dataset_v2.json\"), \"r\") as f:\n",
    "    dataset_dict = json.load(f)\n",
    "\n",
    "loaded_dataset = DatasetV2.from_dict(dataset_dict)\n",
    "len(loaded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.collect_binding_id_states import ExperimentResults\n",
    "\n",
    "cache_path = os.path.join(\n",
    "    env_utils.DEFAULT_RESULTS_DIR, \n",
    "    \"binding_id_states\",\n",
    "    \"Meta-Llama-3-70B-Instruct\", \n",
    "    \"results.json\"\n",
    ")\n",
    "with open(cache_path, \"r\") as f:\n",
    "    results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ExperimentResults.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8192])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.cached_states[5].context_informed_actor['model.layers.79_<>_155'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
