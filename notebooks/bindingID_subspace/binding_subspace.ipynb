{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "from src.functional import free_gpu_cache\n",
    "from src.utils import env_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [01:11<00:00,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded /home/local_arnab/Codes/00_MODEL/meta-llama/Meta-Llama-3-70B-Instruct | size: 36650.535 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.models import load_LM\n",
    "import torch\n",
    "from nnsight import LanguageModel\n",
    "from src.utils import env_utils\n",
    "\n",
    "# model_key = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_key = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "lm = load_LM(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5561\n"
     ]
    }
   ],
   "source": [
    "from src.dataset import DatasetV2, SampleV2\n",
    "\n",
    "with open (os.path.join(env_utils.DEFAULT_DATA_DIR, \"dataset_v2.json\"), \"r\") as f:\n",
    "    dataset_dict = json.load(f)\n",
    "\n",
    "dataset = DatasetV2.from_dict(dataset_dict)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import get_hs, find_token_range, prepare_input, logit_lens\n",
    "def collect_actor_latent_in_question(\n",
    "    lm: LanguageModel,\n",
    "    question: str,\n",
    "    actor: str,\n",
    "    layers: list = list(range(7, 28)),\n",
    "    layer_name_format: str = \"model.layers.{}\"\n",
    ") -> dict[int: torch.Tensor]:\n",
    "    inputs = prepare_input(prompts=question, tokenizer=lm, return_offsets_mapping=True)\n",
    "    action_last_range = find_token_range(\n",
    "        string=question,\n",
    "        substring=actor,\n",
    "        occurrence=-1,\n",
    "        offset_mapping=inputs[\"offset_mapping\"][0],\n",
    "    )\n",
    "    actor_last_idx = action_last_range[1] - 1\n",
    "    inputs.pop(\"offset_mapping\")\n",
    "    print(actor_last_idx, \" >> \", lm.tokenizer.decode(inputs[\"input_ids\"][0][actor_last_idx]))\n",
    "\n",
    "    \n",
    "    last_loc = (layer_name_format.format(lm.config.num_hidden_layers-1), -1)\n",
    "    locations = [(layer_name_format.format(i), actor_last_idx) for i in layers] + [last_loc]\n",
    "\n",
    "    hs = get_hs(\n",
    "        lm=lm,\n",
    "        input=inputs,\n",
    "        locations=locations,\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "    predicted_ans = logit_lens(lm = lm, h = hs[last_loc], k = 2)[0]\n",
    "    print(f\"{predicted_ans=}\")\n",
    "\n",
    "    return hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer (yes/no):\" tag.\n",
      "\n",
      "Story: Rob works in a busy restaurant. A customer asks Rob for aquavit. Rob grabs an opaque pitcher and fills it with aquavit. A coworker, who didn't hear the customer's request, swaps the aquavit in the pitcher with ouzo while Rob is attending to another task. Rob does not see the coworker swapping the contents in the pitcher and also can't see what is in it without opening it.\n",
      "\n",
      "Question: Does Rob believe that there is aquavit in the pitcher?\n",
      "Answer:\n",
      "yes\n",
      "147  >>   Rob\n",
      "predicted_ans=PredictedToken(token=' yes', prob=0.5722293257713318, logit=21.953125, token_id=10035)\n"
     ]
    }
   ],
   "source": [
    "idx = 25\n",
    "question, answer = dataset.__getitem__(idx, set_ans=\"yes\")\n",
    "# question = question.replace(\"Answer:\", \"Answer (yes/no):\")\n",
    "actor = dataset.samples[idx].actor\n",
    "print(question)\n",
    "print(answer)\n",
    "\n",
    "hs = collect_actor_latent_in_question(\n",
    "    lm=lm,\n",
    "    question=question,\n",
    "    actor=actor,\n",
    "    layers=list(range(7, 28)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(6000 * 5)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
