{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "from src.functional import free_gpu_cache\n",
    "from src.utils import env_utils\n",
    "\n",
    "from src.models import load_LM\n",
    "import torch\n",
    "from nnsight import LanguageModel\n",
    "from src.utils import env_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [01:15<00:00,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded /home/local_arnab/Codes/00_MODEL/meta-llama/Meta-Llama-3-70B-Instruct | size: 36650.535 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# model_key = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_key = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "lm = load_LM(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5561\n"
     ]
    }
   ],
   "source": [
    "from src.dataset import DatasetV2, SampleV2\n",
    "\n",
    "with open (os.path.join(env_utils.DEFAULT_DATA_DIR, \"dataset_v2.json\"), \"r\") as f:\n",
    "    dataset_dict = json.load(f)\n",
    "\n",
    "dataset = DatasetV2.from_dict(dataset_dict)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import get_hs, find_token_range, prepare_input, logit_lens\n",
    "def collect_actor_latent_in_question(\n",
    "    lm: LanguageModel,\n",
    "    question: str,\n",
    "    actor: str,\n",
    "    layers: list = list(range(7, 28)),\n",
    "    layer_name_format: str = \"model.layers.{}\"\n",
    ") -> dict[int: torch.Tensor]:\n",
    "    inputs = prepare_input(prompts=question, tokenizer=lm, return_offsets_mapping=True)\n",
    "    action_last_range = find_token_range(\n",
    "        string=question,\n",
    "        substring=actor,\n",
    "        occurrence=-1,\n",
    "        offset_mapping=inputs[\"offset_mapping\"][0],\n",
    "    )\n",
    "    actor_last_idx = action_last_range[1] - 1\n",
    "    inputs.pop(\"offset_mapping\")\n",
    "    print(actor_last_idx, \" >> \", lm.tokenizer.decode(inputs[\"input_ids\"][0][actor_last_idx]))\n",
    "\n",
    "    \n",
    "    last_loc = (layer_name_format.format(lm.config.num_hidden_layers-1), -1)\n",
    "    locations = [(layer_name_format.format(i), actor_last_idx) for i in layers] + [last_loc]\n",
    "\n",
    "    hs = get_hs(\n",
    "        lm=lm,\n",
    "        input=inputs,\n",
    "        locations=locations,\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "    predicted_ans = logit_lens(lm = lm, h = hs[last_loc], k = 2)[0]\n",
    "    print(f\"{predicted_ans=}\")\n",
    "\n",
    "    return hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer (yes/no):\" tag.\n",
      "\n",
      "Story: Rob works in a busy restaurant. A customer asks Rob for aquavit. Rob grabs an opaque pitcher and fills it with aquavit. A coworker, who didn't hear the customer's request, swaps the aquavit in the pitcher with ouzo while Rob is attending to another task. Rob does not see the coworker swapping the contents in the pitcher and also can't see what is in it without opening it.\n",
      "\n",
      "Question: Does Rob believe that there is aquavit in the pitcher?\n",
      "Answer:\n",
      "yes\n",
      "147  >>   Rob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:435: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_ans=PredictedToken(token=' yes', prob=0.5722293257713318, logit=21.953125, token_id=10035)\n"
     ]
    }
   ],
   "source": [
    "idx = 25\n",
    "question, answer = dataset.__getitem__(idx, set_ans=\"yes\")\n",
    "# question = question.replace(\"Answer:\", \"Answer (yes/no):\")\n",
    "actor = dataset.samples[idx].actor\n",
    "print(question)\n",
    "print(answer)\n",
    "\n",
    "hs = collect_actor_latent_in_question(\n",
    "    lm=lm,\n",
    "    question=question,\n",
    "    actor=actor,\n",
    "    layers=list(range(7, 28)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse cached states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5561 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5561/5561 [04:10<00:00, 22.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from scripts.collect_binding_id_states import CachedBindingIDState\n",
    "\n",
    "cache_path = os.path.join(\n",
    "    env_utils.DEFAULT_RESULTS_DIR,\n",
    "    \"binding_id_states_split\",\n",
    "    # lm.config._name_or_path.split(\"/\")[-1],\n",
    "    \"Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    ")\n",
    "\n",
    "all_cached_states: list[CachedBindingIDState] = []\n",
    "for file_name in tqdm(os.listdir(cache_path)):\n",
    "    if not file_name.endswith(\".json\"):\n",
    "        continue\n",
    "    with open(os.path.join(cache_path, file_name), \"r\") as f:\n",
    "        loaded_dict = json.load(f)\n",
    "        # cached_state = CachedBindingIDState.from_dict(loaded_dict)\n",
    "        # all_cached_states.append(cached_state)\n",
    "        all_cached_states.append(loaded_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5561/5561 [01:07<00:00, 82.59it/s]\n"
     ]
    }
   ],
   "source": [
    "def check_actor_position(keys):\n",
    "    positions = list(set([int(k.split(\"_<>_\")[-1]) for k in keys]))\n",
    "    return min(positions)\n",
    "\n",
    "free_gpu_cache()\n",
    "\n",
    "layers = list(range(7, 28))\n",
    "layer_name_format = \"model.layers.{}\"\n",
    "Delta = {\n",
    "    layer_name_format.format(i): [] for i in layers\n",
    "}\n",
    "\n",
    "for cached_state in tqdm(all_cached_states):\n",
    "    actor_position = check_actor_position(cached_state[\"context_informed_actor\"].keys())\n",
    "    for layer in Delta:\n",
    "        d = (\n",
    "                torch.Tensor(cached_state[\"context_informed_actor\"][f\"{layer}_<>_{actor_position}\"]) -\n",
    "                torch.Tensor(cached_state[\"context_unaware_actor\"][f\"{layer}_<>_{actor_position}\"])\n",
    "        )\n",
    "        Delta[layer].append(d) \n",
    "\n",
    "free_gpu_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5561, 8192])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for layer in Delta:\n",
    "    Delta[layer] = torch.stack(Delta[layer])\n",
    "\n",
    "Delta[layer].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.7\n",
      "model.layers.8\n",
      "model.layers.9\n",
      "model.layers.10\n",
      "model.layers.11\n",
      "model.layers.12\n",
      "model.layers.13\n",
      "model.layers.14\n",
      "model.layers.15\n",
      "model.layers.16\n",
      "model.layers.17\n",
      "model.layers.18\n",
      "model.layers.19\n",
      "model.layers.20\n",
      "model.layers.21\n",
      "model.layers.22\n",
      "model.layers.23\n",
      "model.layers.24\n",
      "model.layers.25\n",
      "model.layers.26\n",
      "model.layers.27\n"
     ]
    }
   ],
   "source": [
    "projection_matrices = {}\n",
    "\n",
    "low_rank = 100\n",
    "for layer in Delta:\n",
    "    print(layer)\n",
    "    U, S, Vh = torch.linalg.svd(Delta[layer].cuda())\n",
    "    projection_matrices[layer] = Vh[: low_rank]\n",
    "\n",
    "    free_gpu_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.savez_compressed(\n",
    "    os.path.join(env_utils.DEFAULT_RESULTS_DIR, \"projection_matrices.npz\"),\n",
    "    **{\n",
    "        layer: projection_matrices[layer].cpu().numpy() for layer in projection_matrices\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Causality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.7 torch.Size([100, 8192])\n",
      "model.layers.8 torch.Size([100, 8192])\n",
      "model.layers.9 torch.Size([100, 8192])\n",
      "model.layers.10 torch.Size([100, 8192])\n",
      "model.layers.11 torch.Size([100, 8192])\n",
      "model.layers.12 torch.Size([100, 8192])\n",
      "model.layers.13 torch.Size([100, 8192])\n",
      "model.layers.14 torch.Size([100, 8192])\n",
      "model.layers.15 torch.Size([100, 8192])\n",
      "model.layers.16 torch.Size([100, 8192])\n",
      "model.layers.17 torch.Size([100, 8192])\n",
      "model.layers.18 torch.Size([100, 8192])\n",
      "model.layers.19 torch.Size([100, 8192])\n",
      "model.layers.20 torch.Size([100, 8192])\n",
      "model.layers.21 torch.Size([100, 8192])\n",
      "model.layers.22 torch.Size([100, 8192])\n",
      "model.layers.23 torch.Size([100, 8192])\n",
      "model.layers.24 torch.Size([100, 8192])\n",
      "model.layers.25 torch.Size([100, 8192])\n",
      "model.layers.26 torch.Size([100, 8192])\n",
      "model.layers.27 torch.Size([100, 8192])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "loaded_projections = np.load(\n",
    "    os.path.join(env_utils.DEFAULT_RESULTS_DIR, \"projection_matrices.npz\")\n",
    ")\n",
    "\n",
    "V_projections = {}\n",
    "for layer in loaded_projections:\n",
    "    V_projections[layer] = torch.Tensor(loaded_projections[layer]).cuda()\n",
    "    print(layer, V_projections[layer].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer (yes/no):\" tag.\n",
      "\n",
      "Story: Eve works in a busy restaurant. A customer asks Eve for whiskey. Eve grabs an opaque pitcher and fills it with whiskey. A coworker, who didn't hear the customer's request, swaps the whiskey in the pitcher with brandy while Eve is attending to another task. Eve does not see the coworker swapping the contents in the pitcher and also can't see what is in it without opening it.\n",
      "\n",
      "Question: Does Eve believe that there is whiskey in the pitcher?\n",
      "Answer:\n",
      "yes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' yes', prob=0.5230958461761475, logit=21.34375, token_id=10035),\n",
       " PredictedToken(token=' YES', prob=0.27999305725097656, logit=20.71875, token_id=14410),\n",
       " PredictedToken(token=' Yes', prob=0.10627337545156479, logit=19.75, token_id=7566),\n",
       " PredictedToken(token=' (', prob=0.05513389781117439, logit=19.09375, token_id=320),\n",
       " PredictedToken(token=' **', prob=0.013095449656248093, logit=17.65625, token_id=3146)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import predict_next_token\n",
    "\n",
    "idx = 0\n",
    "question, answer = dataset.__getitem__(idx, set_ans=\"yes\")\n",
    "actor = dataset.samples[idx].actor\n",
    "print(question)\n",
    "print(answer)\n",
    "\n",
    "predict_next_token(lm=lm, input=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer (yes/no):\" tag.\n",
      "\n",
      "Story: Adam works in a busy restaurant. A customer asks Adam for whiskey. Adam grabs an opaque pitcher and fills it with whiskey. A coworker, who didn't hear the customer's request, swaps the whiskey in the pitcher with brandy while Adam is attending to another task. Adam does not see the coworker swapping the contents in the pitcher and also can't see what is in it without opening it.\n",
      "\n",
      "Question: Does Eve believe that there is whiskey in the pitcher?\n",
      "Answer:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([PredictedToken(token=' No', prob=0.20485912263393402, logit=16.359375, token_id=2360),\n",
       "  PredictedToken(token=' There', prob=0.19244733452796936, logit=16.296875, token_id=2684),\n",
       "  PredictedToken(token=' (', prob=0.09019824862480164, logit=15.5390625, token_id=320),\n",
       "  PredictedToken(token=' Not', prob=0.0789802223443985, logit=15.40625, token_id=2876),\n",
       "  PredictedToken(token=' no', prob=0.07135268300771713, logit=15.3046875, token_id=912)],\n",
       " {10035: (20,\n",
       "   PredictedToken(token=' yes', prob=0.004386679269373417, logit=12.515625, token_id=10035))})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unaligned_question, answer = dataset.__getitem__(idx, set_ans=\"yes\", unaligned_actor=\"Adam\")\n",
    "print(unaligned_question)\n",
    "\n",
    "predict_next_token(lm=lm, input=unaligned_question, interested_tokens=[10035])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    }
   ],
   "source": [
    "inputs = prepare_input(prompts=question, tokenizer=lm, return_offsets_mapping=True)\n",
    "action_last_range = find_token_range(\n",
    "    string=question,\n",
    "    substring=actor,\n",
    "    occurrence=-1,\n",
    "    offset_mapping=inputs[\"offset_mapping\"][0],\n",
    ")\n",
    "actor_last_idx = action_last_range[1] - 1\n",
    "inputs.pop(\"offset_mapping\")\n",
    "\n",
    "print(actor_last_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144  >>   Eve\n",
      "predicted_ans=PredictedToken(token=' yes', prob=0.5230958461761475, logit=21.34375, token_id=10035)\n",
      "144  >>   Eve\n",
      "predicted_ans=PredictedToken(token=' No', prob=0.20485912263393402, logit=16.359375, token_id=2360)\n",
      "s_a[(layer_name_format.format(15), actor_last_idx)].shape=torch.Size([8192])\n",
      "s_u[(layer_name_format.format(15), actor_last_idx)].shape=torch.Size([8192])\n"
     ]
    }
   ],
   "source": [
    "layer_name_format = \"model.layers.{}\"\n",
    "\n",
    "s_a = collect_actor_latent_in_question(\n",
    "    lm = lm, \n",
    "    question = question,\n",
    "    actor = actor\n",
    ")\n",
    "\n",
    "s_u = collect_actor_latent_in_question(\n",
    "    lm = lm,\n",
    "    question = unaligned_question,\n",
    "    actor = actor\n",
    ")\n",
    "\n",
    "print(f\"{s_a[(layer_name_format.format(15), actor_last_idx)].shape=}\")\n",
    "print(f\"{s_u[(layer_name_format.format(15), actor_last_idx)].shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_binding_ID(\n",
    "    s_orig: torch.Tensor,\n",
    "    s_other: torch.Tensor,\n",
    "    Proj: torch.Tensor,\n",
    "    rank: int = 10\n",
    "):\n",
    "    s_orig = s_orig.to(torch.float32)\n",
    "    s_other = s_other.to(torch.float32)\n",
    "    Proj = Proj.to(torch.float32)\n",
    "\n",
    "    Proj_r = Proj[:rank].to(lm.device)\n",
    "    Proj_r = Proj_r.T @ Proj_r\n",
    "    return (\n",
    "        (torch.eye(Proj_r.shape[0]).to(lm.device) - Proj_r) @ s_orig # remove patching ID from s_orig\n",
    "        + Proj_r @ s_other # add patching ID from s_other\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import PatchSpec, free_gpu_cache\n",
    "\n",
    "free_gpu_cache()\n",
    "\n",
    "patches = []\n",
    "for layer_idx in range(7, 28):\n",
    "    layer = layer_name_format.format(layer_idx)\n",
    "    location = (layer, actor_last_idx)\n",
    "    patches.append(PatchSpec(\n",
    "        location=location,\n",
    "        patch=patch_binding_ID(\n",
    "            s_orig=s_a[location],\n",
    "            s_other=s_u[location],\n",
    "            Proj=V_projections[layer],\n",
    "            rank=1\n",
    "        )\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PredictedToken(token=' There', prob=0.3007343113422394, logit=16.40625, token_id=2684),\n",
       "  PredictedToken(token=' No', prob=0.14205683767795563, logit=15.65625, token_id=2360),\n",
       "  PredictedToken(token=' (', prob=0.11414587497711182, logit=15.4375, token_id=320),\n",
       "  PredictedToken(token=' NO', prob=0.05968255549669266, logit=14.7890625, token_id=5782),\n",
       "  PredictedToken(token=' no', prob=0.049478575587272644, logit=14.6015625, token_id=912)],\n",
       " {10035: (19,\n",
       "   PredictedToken(token=' yes', prob=0.004937465768307447, logit=12.296875, token_id=10035))})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_token(\n",
    "    lm=lm, \n",
    "    # input=unaligned_question,\n",
    "    input=question,\n",
    "    patches=patches,\n",
    "    interested_tokens=[10035]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_objs(story, obj_1, obj_2):\n",
    "    story = story.replace(obj_1, \"<obj_placeholder>\")\n",
    "    story = story.replace(obj_2, obj_1)\n",
    "    story = story.replace(\"<obj_placeholder>\", obj_2)\n",
    "    return story\n",
    "\n",
    "sample = dataset.samples[idx]\n",
    "sample_obj_swapped = SampleV2(\n",
    "    story=swap_objs(sample.story, sample.obj_belief, sample.obj_true),\n",
    "    actor=sample.actor,\n",
    "    obj_belief=sample.obj_true,\n",
    "    obj_true=sample.obj_belief,\n",
    "    container=sample.container,\n",
    ")\n",
    "\n",
    "temp_d = DatasetV2(samples=[sample, sample_obj_swapped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer (yes/no):\" tag.\n",
      "\n",
      "Story: Eve works in a busy restaurant. A customer asks Eve for whiskey. Eve grabs an opaque pitcher and fills it with whiskey. A coworker, who didn't hear the customer's request, swaps the whiskey in the pitcher with brandy while Eve is attending to another task. Eve does not see the coworker swapping the contents in the pitcher and also can't see what is in it without opening it.\n",
      "\n",
      "Question: Does Eve believe that there is brandy in the pitcher?\n",
      "Answer:\n",
      "no\n",
      "\n",
      "\n",
      "\n",
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer (yes/no):\" tag.\n",
      "\n",
      "Story: Eve works in a busy restaurant. A customer asks Eve for brandy. Eve grabs an opaque pitcher and fills it with brandy. A coworker, who didn't hear the customer's request, swaps the brandy in the pitcher with whiskey while Eve is attending to another task. Eve does not see the coworker swapping the contents in the pitcher and also can't see what is in it without opening it.\n",
      "\n",
      "Question: Does Eve believe that there is brandy in the pitcher?\n",
      "Answer:\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "ques_1, ans_1 = temp_d.__getitem__(0, set_ans=\"no\")\n",
    "print(ques_1)\n",
    "print(ans_1)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "ques_2, ans_2 = temp_d.__getitem__(1, set_ans=\"yes\")\n",
    "print(ques_2)\n",
    "print(ans_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144  >>   Eve\n",
      "predicted_ans=PredictedToken(token=' no', prob=0.47336092591285706, logit=20.40625, token_id=912)\n",
      "146  >>   Eve\n",
      "predicted_ans=PredictedToken(token=' yes', prob=0.5370039343833923, logit=21.4375, token_id=10035)\n"
     ]
    }
   ],
   "source": [
    "s_1 = collect_actor_latent_in_question(\n",
    "    lm = lm, \n",
    "    question = ques_1,\n",
    "    actor = sample.actor\n",
    ")\n",
    "\n",
    "s_2 = collect_actor_latent_in_question(\n",
    "    lm = lm,\n",
    "    question = ques_2,\n",
    "    actor = sample.actor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PredictedToken(token=' no', prob=0.47336092591285706, logit=20.40625, token_id=912),\n",
       "  PredictedToken(token=' No', prob=0.245576411485672, logit=19.75, token_id=2360),\n",
       "  PredictedToken(token=' NO', prob=0.11600209027528763, logit=19.0, token_id=5782),\n",
       "  PredictedToken(token=' (', prob=0.07373541593551636, logit=18.546875, token_id=320),\n",
       "  PredictedToken(token=' **', prob=0.046869080513715744, logit=18.09375, token_id=3146)],\n",
       " {10035: (14,\n",
       "   PredictedToken(token=' yes', prob=0.0005288677639327943, logit=13.609375, token_id=10035))})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_token(\n",
    "    lm=lm, \n",
    "    input=ques_1,\n",
    "    interested_tokens=[10035]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PredictedToken(token=' no', prob=0.4730239808559418, logit=20.390625, token_id=912),\n",
       "  PredictedToken(token=' No', prob=0.24540160596370697, logit=19.734375, token_id=2360),\n",
       "  PredictedToken(token=' NO', prob=0.11591951549053192, logit=18.984375, token_id=5782),\n",
       "  PredictedToken(token=' (', prob=0.07368292659521103, logit=18.53125, token_id=320),\n",
       "  PredictedToken(token=' **', prob=0.04683571681380272, logit=18.078125, token_id=3146)],\n",
       " {10035: (14,\n",
       "   PredictedToken(token=' yes', prob=0.0005410240846686065, logit=13.6171875, token_id=10035))})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_1 = 144\n",
    "idx_2 = 146\n",
    "\n",
    "patches = []\n",
    "for layer_idx in range(7, 28):\n",
    "    layer = layer_name_format.format(layer_idx)\n",
    "    location_1 = (layer, idx_1)\n",
    "    location_2 = (layer, idx_2)\n",
    "    patches.append(PatchSpec(\n",
    "        location=location,\n",
    "        patch=patch_binding_ID(\n",
    "            s_orig=s_1[location_1],\n",
    "            s_other=s_2[location_2],\n",
    "            Proj=V_projections[layer],\n",
    "            rank=50\n",
    "        )\n",
    "    ))\n",
    "\n",
    "predict_next_token(\n",
    "    lm=lm, \n",
    "    input=ques_1,\n",
    "    patches=patches,\n",
    "    interested_tokens=[10035]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
