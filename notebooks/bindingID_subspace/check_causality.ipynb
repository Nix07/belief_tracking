{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "* Low rank swapping is working!\n",
    "* The edit seems effective at different range of layers.\n",
    "    * actor layers 13-17\n",
    "    * object layers 24-28\n",
    "\n",
    "TODO: \n",
    "* Patch across templates\n",
    "\n",
    "Questions:\n",
    "* But patching the whole `h_other` instead of `h_patch` will also flip the answer (Nikhil's experiment). How to make sure that we are only bringing the binding ID and nothing else? \n",
    "   * The same projection can be used to remove the binding ID info. If binding ID is really important for the task the LM should give random answers without it. (?? verify this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "from src.functional import free_gpu_cache, predict_next_token\n",
    "from src.utils import env_utils\n",
    "\n",
    "from src.models import load_LM\n",
    "import torch\n",
    "from nnsight import LanguageModel\n",
    "from src.utils import env_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_KEY = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "svd_path = os.path.join(\n",
    "    env_utils.DEFAULT_RESULTS_DIR, \"SVDs\",\n",
    "    MODEL_KEY.split(\"/\")[-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8192, 8192])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = np.load(os.path.join(svd_path, \"characters\", \"projections\", \"model.layers.10.npz\"))\n",
    "Vh = torch.Tensor(svd['Vh'])\n",
    "Vh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [01:10<00:00,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded /home/local_arnab/Codes/00_MODEL/meta-llama/Meta-Llama-3-70B-Instruct | size: 36650.535 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lm = load_LM(\n",
    "    model_key=MODEL_KEY,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_binding_ID(\n",
    "    h_orig: torch.Tensor,\n",
    "    h_other: torch.Tensor,\n",
    "    Proj: torch.Tensor,\n",
    "    rank: int = 10\n",
    "):\n",
    "    # The prediction will also change if the full h_other is patched in stead of h_orig\n",
    "    h_orig = h_orig.to(torch.float32)\n",
    "    h_other = h_other.to(torch.float32)\n",
    "    Proj = Proj.to(torch.float32)\n",
    "\n",
    "    Proj_r = Proj[:rank].to(lm.device)\n",
    "    Proj_r = Proj_r.T @ Proj_r\n",
    "    return (\n",
    "        (torch.eye(Proj_r.shape[0]).to(lm.device) - Proj_r) @ h_orig # remove patching ID from s_orig\n",
    "        + Proj_r @ h_other # add patching ID from s_other\n",
    "    )\n",
    "\n",
    "\n",
    "def load_Vh(type: str, layer_name: str):\n",
    "    svd = np.load(os.path.join(svd_path, type, \"projections\", f\"{layer_name}.npz\"))\n",
    "    Vh = torch.Tensor(svd['Vh'])\n",
    "    return Vh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample.true_state={'pitcher': 'coffee', 'cup': 'coffee'}\n",
      "sample.protagonist_belief={'pitcher': 'juice', 'cup': 'coffee'}\n"
     ]
    }
   ],
   "source": [
    "from src.dataset import SampleV3, DatasetV3\n",
    "\n",
    "sample = SampleV3(\n",
    "    protagonist=\"Adam\",\n",
    "    perpetrator=\"Bob\",\n",
    "    objects=[\"juice\", \"coffee\"],\n",
    "    containers=[\"pitcher\", \"cup\"],\n",
    "    event_idx=0,\n",
    "    event_noticed=False\n",
    ")\n",
    "\n",
    "print(f\"{sample.true_state=}\")\n",
    "print(f\"{sample.protagonist_belief=}\")\n",
    "\n",
    "dataset = DatasetV3(samples = [sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer:\" tag.\n",
      "\n",
      "Story: Adam is working in a busy restaurant. A customer asks Adam for juice. Adam grabs an opaque pitcher and fills it with juice. Then Adam grabs another opaque cup and fills it with coffee. A coworker named Bob observes Adam pouring the contents in the pitcher and the cup. But Bob didn't hear the customer's request and swaps the juice in the pitcher with coffee while Adam was attending to another task. Adam can't see what is in the pitcher and the cup without opening their lid. Adam didn't see Bob swapping the the contents of pitcher.\n",
      "Question: Does Bob believe the pitcher contains juice?\n",
      "Answer:\n",
      "no\n",
      "169  Bob\n",
      "answer='no' | predicted_ans=PredictedToken(token=' No', prob=0.48156559467315674, logit=19.875, token_id=2360)\n"
     ]
    }
   ],
   "source": [
    "from scripts.collect_binding_id_states import collect_token_latent_in_question\n",
    "\n",
    "prompt_perp, answer_perp = dataset.__getitem__(\n",
    "    0, \n",
    "    set_actor=\"perpetrator\",\n",
    "    # set_ans=\"no\",\n",
    "    set_container=0,\n",
    "    set_obj=0\n",
    ")\n",
    "print(prompt_perp)\n",
    "print(answer_perp)\n",
    "\n",
    "h_perpetrator = collect_token_latent_in_question(\n",
    "    lm = lm,\n",
    "    prompt = prompt_perp,\n",
    "    answer = answer_perp,\n",
    "    token_of_interest= sample.perpetrator,\n",
    "    detensorize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer:\" tag.\n",
      "\n",
      "Story: Adam is working in a busy restaurant. A customer asks Adam for juice. Adam grabs an opaque pitcher and fills it with juice. Then Adam grabs another opaque cup and fills it with coffee. A coworker named Bob observes Adam pouring the contents in the pitcher and the cup. But Bob didn't hear the customer's request and swaps the juice in the pitcher with coffee while Adam was attending to another task. Adam can't see what is in the pitcher and the cup without opening their lid. Adam didn't see Bob swapping the the contents of pitcher.\n",
      "Question: Does Adam believe the pitcher contains juice?\n",
      "Answer:\n",
      "yes\n",
      "169  Adam\n",
      "answer='yes' | predicted_ans=PredictedToken(token=' YES', prob=0.5450194478034973, logit=20.515625, token_id=14410)\n"
     ]
    }
   ],
   "source": [
    "prompt_prot, answer_prot = dataset.__getitem__(\n",
    "    0, \n",
    "    set_actor=\"protagonist\",\n",
    "    # set_ans=\"no\",\n",
    "    set_container=0,\n",
    "    set_obj=0\n",
    ")\n",
    "print(prompt_prot)\n",
    "print(answer_prot)\n",
    "\n",
    "h_protagonist = collect_token_latent_in_question(\n",
    "    lm = lm,\n",
    "    prompt = prompt_prot,\n",
    "    answer = answer_prot,\n",
    "    token_of_interest= sample.protagonist,\n",
    "    detensorize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PredictedToken(token=' No', prob=0.4217674136161804, logit=19.71875, token_id=2360),\n",
       "  PredictedToken(token=' NO', prob=0.31343093514442444, logit=19.421875, token_id=5782),\n",
       "  PredictedToken(token=' no', prob=0.14805428683757782, logit=18.671875, token_id=912),\n",
       "  PredictedToken(token=' **', prob=0.029612718150019646, logit=17.0625, token_id=3146),\n",
       "  PredictedToken(token=' YES', prob=0.02654467150568962, logit=16.953125, token_id=14410)],\n",
       " {2360: (1,\n",
       "   PredictedToken(token=' No', prob=0.4217674136161804, logit=19.71875, token_id=2360))})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import PatchSpec, free_gpu_cache\n",
    "\n",
    "LAYER_NAME_FORMAT = \"model.layers.{}\"\n",
    "QUESTION_ACTOR_IDX = 169\n",
    "\n",
    "free_gpu_cache()\n",
    "\n",
    "patches = []\n",
    "for layer_idx in range(13, 20):\n",
    "    layer = LAYER_NAME_FORMAT.format(layer_idx)\n",
    "    Vh = load_Vh(\"characters\", layer)\n",
    "    location = (layer, QUESTION_ACTOR_IDX)\n",
    "    patches.append(PatchSpec(\n",
    "        location=location,\n",
    "        patch=patch_binding_ID(\n",
    "            h_orig=h_protagonist.states[location],\n",
    "            h_other=h_perpetrator.states[location],\n",
    "            Proj=Vh,\n",
    "            rank=3\n",
    "        )\n",
    "    ))\n",
    "\n",
    "predict_next_token(\n",
    "    lm=lm, \n",
    "    input=prompt_prot,\n",
    "    patches=patches,\n",
    "    interested_tokens=[2360]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PredictedToken(token=' YES', prob=0.5398056507110596, logit=20.46875, token_id=14410),\n",
       "  PredictedToken(token=' Yes', prob=0.22856809198856354, logit=19.609375, token_id=7566),\n",
       "  PredictedToken(token=' yes', prob=0.18948960304260254, logit=19.421875, token_id=10035),\n",
       "  PredictedToken(token=' **', prob=0.010361366905272007, logit=16.515625, token_id=3146),\n",
       "  PredictedToken(token=' NO', prob=0.007821169681847095, logit=16.234375, token_id=5782)],\n",
       " {14410: (1,\n",
       "   PredictedToken(token=' YES', prob=0.5398056507110596, logit=20.46875, token_id=14410))})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free_gpu_cache()\n",
    "\n",
    "patches = []\n",
    "for layer_idx in range(13, 20):\n",
    "    layer = LAYER_NAME_FORMAT.format(layer_idx)\n",
    "    Vh = load_Vh(\"characters\", layer)\n",
    "    location = (layer, QUESTION_ACTOR_IDX)\n",
    "    patches.append(PatchSpec(\n",
    "        location=location,\n",
    "        patch=patch_binding_ID(\n",
    "            h_orig=h_perpetrator.states[location],\n",
    "            h_other=h_protagonist.states[location],\n",
    "            Proj=Vh,\n",
    "            rank=3\n",
    "        )\n",
    "    ))\n",
    "\n",
    "predict_next_token(\n",
    "    lm=lm, \n",
    "    input=prompt_perp,\n",
    "    patches=patches,\n",
    "    interested_tokens=[14410]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer:\" tag.\n",
      "\n",
      "Story: Adam is working in a busy restaurant. A customer asks Adam for juice. Adam grabs an opaque pitcher and fills it with juice. Then Adam grabs another opaque cup and fills it with coffee. A coworker named Bob observes Adam pouring the contents in the pitcher and the cup. But Bob didn't hear the customer's request and swaps the juice in the pitcher with coffee while Adam was attending to another task. Adam can't see what is in the pitcher and the cup without opening their lid. Adam didn't see Bob swapping the the contents of pitcher.\n",
      "Question: Does Adam believe the pitcher contains juice?\n",
      "Answer:\n",
      "yes\n",
      "174  juice\n",
      "answer='yes' | predicted_ans=PredictedToken(token=' YES', prob=0.5450194478034973, logit=20.515625, token_id=14410)\n"
     ]
    }
   ],
   "source": [
    "from scripts.collect_binding_id_states import collect_token_latent_in_question\n",
    "\n",
    "prompt_obj0, answer_obj0 = dataset.__getitem__(\n",
    "    0, \n",
    "    set_actor=\"protagonist\",\n",
    "    # set_ans=\"no\",\n",
    "    set_container=0,\n",
    "    set_obj=0\n",
    ")\n",
    "print(prompt_obj0)\n",
    "print(answer_obj0)\n",
    "\n",
    "h_obj0 = collect_token_latent_in_question(\n",
    "    lm = lm,\n",
    "    prompt = prompt_obj0,\n",
    "    answer = answer_obj0,\n",
    "    token_of_interest= sample.objects[0],\n",
    "    detensorize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer:\" tag.\n",
      "\n",
      "Story: Adam is working in a busy restaurant. A customer asks Adam for juice. Adam grabs an opaque pitcher and fills it with juice. Then Adam grabs another opaque cup and fills it with coffee. A coworker named Bob observes Adam pouring the contents in the pitcher and the cup. But Bob didn't hear the customer's request and swaps the juice in the pitcher with coffee while Adam was attending to another task. Adam can't see what is in the pitcher and the cup without opening their lid. Adam didn't see Bob swapping the the contents of pitcher.\n",
      "Question: Does Adam believe the pitcher contains coffee?\n",
      "Answer:\n",
      "no\n",
      "174  coffee\n",
      "answer='no' | predicted_ans=PredictedToken(token=' No', prob=0.5005128383636475, logit=19.34375, token_id=2360)\n"
     ]
    }
   ],
   "source": [
    "prompt_obj1, answer_obj1= dataset.__getitem__(\n",
    "    0, \n",
    "    set_actor=\"protagonist\",\n",
    "    # set_ans=\"no\",\n",
    "    set_container=0,\n",
    "    set_obj=1\n",
    ")\n",
    "print(prompt_obj1)\n",
    "print(answer_obj1)\n",
    "\n",
    "h_obj1 = collect_token_latent_in_question(\n",
    "    lm = lm,\n",
    "    prompt = prompt_obj1,\n",
    "    answer = answer_obj1,\n",
    "    token_of_interest= sample.objects[1],\n",
    "    detensorize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PredictedToken(token=' No', prob=0.4627632796764374, logit=18.765625, token_id=2360),\n",
       "  PredictedToken(token=' NO', prob=0.28510019183158875, logit=18.28125, token_id=5782),\n",
       "  PredictedToken(token=' no', prob=0.13258390128612518, logit=17.515625, token_id=912),\n",
       "  PredictedToken(token=' **', prob=0.03513123840093613, logit=16.1875, token_id=3146),\n",
       "  PredictedToken(token=' YES', prob=0.01464487612247467, logit=15.3125, token_id=14410)],\n",
       " {2360: (1,\n",
       "   PredictedToken(token=' No', prob=0.4627632796764374, logit=18.765625, token_id=2360))})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free_gpu_cache()\n",
    "\n",
    "QUESTION_OBJ_IDX = 174\n",
    "\n",
    "patches = []\n",
    "for layer_idx in range(24, 28):\n",
    "    layer = LAYER_NAME_FORMAT.format(layer_idx)\n",
    "    Vh = load_Vh(\"objects\", layer)\n",
    "    location = (layer, QUESTION_OBJ_IDX)\n",
    "    patches.append(PatchSpec(\n",
    "        location=location,\n",
    "        patch=patch_binding_ID(\n",
    "            h_orig=h_obj0.states[location],\n",
    "            h_other=h_obj1.states[location],\n",
    "            Proj=Vh,\n",
    "            rank=3\n",
    "        )\n",
    "    ))\n",
    "\n",
    "predict_next_token(\n",
    "    lm=lm, \n",
    "    input=prompt_obj0,\n",
    "    patches=patches,\n",
    "    interested_tokens=[2360]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PredictedToken(token=' YES', prob=0.3233693242073059, logit=18.71875, token_id=14410),\n",
       "  PredictedToken(token=' NO', prob=0.17040367424488068, logit=18.078125, token_id=5782),\n",
       "  PredictedToken(token=' yes', prob=0.13907942175865173, logit=17.875, token_id=10035),\n",
       "  PredictedToken(token=' No', prob=0.1306530237197876, logit=17.8125, token_id=2360),\n",
       "  PredictedToken(token=' Yes', prob=0.11711661517620087, logit=17.703125, token_id=7566)],\n",
       " {14410: (1,\n",
       "   PredictedToken(token=' YES', prob=0.3233693242073059, logit=18.71875, token_id=14410))})"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches = []\n",
    "for layer_idx in range(24, 28):\n",
    "    layer = LAYER_NAME_FORMAT.format(layer_idx)\n",
    "    Vh = load_Vh(\"objects\", layer)\n",
    "    location = (layer, QUESTION_OBJ_IDX)\n",
    "    patches.append(PatchSpec(\n",
    "        location=location,\n",
    "        patch=patch_binding_ID(\n",
    "            h_orig=h_obj1.states[location],\n",
    "            h_other=h_obj0.states[location],\n",
    "            Proj=Vh,\n",
    "            rank=3\n",
    "        )\n",
    "    ))\n",
    "\n",
    "predict_next_token(\n",
    "    lm=lm, \n",
    "    input=prompt_obj1,\n",
    "    patches=patches,\n",
    "    interested_tokens=[14410]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
