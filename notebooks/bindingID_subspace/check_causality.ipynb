{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "* Low rank swapping is working!\n",
    "* The edit seems effective at different range of layers.\n",
    "    * actor layers 13-17 (rank 3)\n",
    "    * object layers 24-28 (rank 3)\n",
    "    * container is successful at a range of layers from 13-28\n",
    "* Projections aren't effective across types - actor projections aren't causaly effective for objects. ==> The subspaces are different.\n",
    "\n",
    "TODO: \n",
    "* Patch with different entities (that doesn't appear in the SVD extraction set) -- DONE\n",
    "* Refine to codebase to be consistent with Tamar's templates.\n",
    "    * Talk with Tamar about the tokenization issues\n",
    "* Patch across templates\n",
    "    * Projections do generalize. But for objects need higher rank projections (rank 10).\n",
    "* Universal mechanisms for binding ID\n",
    "* Calculate across different templates\n",
    "* observation vs the causal event == how do the bindings IDs change?\n",
    "* python psudocode that solves the problem and check equivalency\n",
    "\n",
    "Questions:\n",
    "* But patching the whole `h_other` instead of `h_patch` will also flip the answer (Nikhil's experiment). How to make sure that we are only bringing the binding ID and nothing else? \n",
    "   * The same projection can be used to remove the binding ID info. If binding ID is really important for the task the LM should give random answers without it. (?? verify this) - Not very convincing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "from src.functional import free_gpu_cache, predict_next_token\n",
    "from src.utils import env_utils\n",
    "\n",
    "from src.models import load_LM\n",
    "import torch\n",
    "from nnsight import LanguageModel\n",
    "from src.utils import env_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_KEY = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "svd_path = os.path.join(\n",
    "    env_utils.DEFAULT_RESULTS_DIR, \"SVDs\",\n",
    "    MODEL_KEY.split(\"/\")[-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8192, 8192])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = np.load(os.path.join(svd_path, \"characters\", \"projections\", \"model.layers.10.npz\"))\n",
    "Vh = torch.Tensor(svd['Vh'])\n",
    "Vh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [00:59<00:00,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded /home/local_arnab/Codes/00_MODEL/meta-llama/Meta-Llama-3-70B-Instruct | size: 36650.535 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lm = load_LM(\n",
    "    model_key=MODEL_KEY,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_binding_ID(\n",
    "    h_orig: torch.Tensor,\n",
    "    h_other: torch.Tensor,\n",
    "    Proj: torch.Tensor,\n",
    "    rank: int = 10\n",
    "):\n",
    "    # The prediction will also change if the full h_other is patched in stead of h_orig\n",
    "    h_orig = h_orig.to(torch.float32)\n",
    "    h_other = h_other.to(torch.float32)\n",
    "    Proj = Proj.to(torch.float32)\n",
    "\n",
    "    Proj_r = Proj[:rank].to(lm.device)\n",
    "    Proj_r = Proj_r.T @ Proj_r\n",
    "    return (\n",
    "        (torch.eye(Proj_r.shape[0]).to(lm.device) - Proj_r) @ h_orig # remove patching ID from s_orig\n",
    "        + Proj_r @ h_other # add patching ID from s_other\n",
    "    )\n",
    "\n",
    "\n",
    "def load_Vh(type: str, layer_name: str):\n",
    "    svd = np.load(os.path.join(svd_path, type, \"projections\", f\"{layer_name}.npz\"))\n",
    "    Vh = torch.Tensor(svd['Vh'])\n",
    "    return Vh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f', 'ountain', 'Ġpen']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lm.tokenizer.tokenize(\"fountain pen\")\n",
    "# lm.tokenizer.tokenize(\"fountain pen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample.true_state={'hat': 'blue silk handkerchief', 'box': 'blue silk handkerchief'}\n",
      "sample.protagonist_belief={'hat': 'red velvet handkerchief', 'box': 'blue silk handkerchief'}\n"
     ]
    }
   ],
   "source": [
    "from src.dataset import SampleV3, DatasetV3\n",
    "\n",
    "sample = SampleV3(\n",
    "    protagonist=\"Nora\",\n",
    "    perpetrator=\"Jon\",\n",
    "    objects=[\"red velvet handkerchief\", \"blue silk handkerchief\"],\n",
    "    containers=[\"hat\", \"box\"],\n",
    "    event_idx=0,\n",
    "    event_noticed=False\n",
    ")\n",
    "\n",
    "print(f\"{sample.true_state=}\")\n",
    "print(f\"{sample.protagonist_belief=}\")\n",
    "\n",
    "dataset = DatasetV3(samples = [sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer:\" tag.\n",
      "\n",
      "Story: Nora is a magician performing at a grand theater. Nora wants to amaze the audience with a trick involving a red velvet handkerchief. Nora places the red velvet handkerchief in a hat and sets it on the stage. Then Nora prepares a backup box and places a blue silk handkerchief inside. An assistant named Jon, who thinks the trick should be different, swaps the red velvet handkerchief in the hat with the blue silk handkerchief while Nora is backstage. Nora didn't see Jon swapping the the contents of hat.\n",
      "Question: Does Jon believe the hat contains red velvet handkerchief?\n",
      "Answer:\n",
      "no\n",
      "168  Jon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:435: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer='no' | predicted_ans=PredictedToken(token=' No', prob=0.47647860646247864, logit=21.515625, token_id=2360)\n"
     ]
    }
   ],
   "source": [
    "from scripts.collect_binding_id_states import collect_token_latent_in_question\n",
    "\n",
    "prompt_perp, answer_perp = dataset.__getitem__(\n",
    "    0, \n",
    "    set_actor=\"perpetrator\",\n",
    "    # set_ans=\"no\",\n",
    "    set_container=0,\n",
    "    set_obj=0\n",
    ")\n",
    "print(prompt_perp)\n",
    "print(answer_perp)\n",
    "\n",
    "h_perpetrator = collect_token_latent_in_question(\n",
    "    lm = lm,\n",
    "    prompt = prompt_perp,\n",
    "    answer = answer_perp,\n",
    "    token_of_interest= sample.perpetrator,\n",
    "    detensorize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer:\" tag.\n",
      "\n",
      "Story: Nora is a magician performing at a grand theater. Nora wants to amaze the audience with a trick involving a red velvet handkerchief. Nora places the red velvet handkerchief in a hat and sets it on the stage. Then Nora prepares a backup box and places a blue silk handkerchief inside. An assistant named Jon, who thinks the trick should be different, swaps the red velvet handkerchief in the hat with the blue silk handkerchief while Nora is backstage. Nora didn't see Jon swapping the the contents of hat.\n",
      "Question: Does Nora believe the hat contains red velvet handkerchief?\n",
      "Answer:\n",
      "yes\n",
      "168  Nora\n",
      "answer='yes' | predicted_ans=PredictedToken(token=' YES', prob=0.61894690990448, logit=22.71875, token_id=14410)\n"
     ]
    }
   ],
   "source": [
    "prompt_prot, answer_prot = dataset.__getitem__(\n",
    "    0, \n",
    "    set_actor=\"protagonist\",\n",
    "    # set_ans=\"no\",\n",
    "    set_container=0,\n",
    "    set_obj=0\n",
    ")\n",
    "print(prompt_prot)\n",
    "print(answer_prot)\n",
    "\n",
    "h_protagonist = collect_token_latent_in_question(\n",
    "    lm = lm,\n",
    "    prompt = prompt_prot,\n",
    "    answer = answer_prot,\n",
    "    token_of_interest= sample.protagonist,\n",
    "    detensorize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PredictedToken(token=' No', prob=0.4544861316680908, logit=21.1875, token_id=2360),\n",
       "  PredictedToken(token=' NO', prob=0.41381433606147766, logit=21.09375, token_id=5782),\n",
       "  PredictedToken(token=' no', prob=0.08949362486600876, logit=19.5625, token_id=912),\n",
       "  PredictedToken(token=' **', prob=0.030928170308470726, logit=18.5, token_id=3146),\n",
       "  PredictedToken(token='No', prob=0.0033632824197411537, logit=16.28125, token_id=2822)],\n",
       " {2360: (1,\n",
       "   PredictedToken(token=' No', prob=0.4544861316680908, logit=21.1875, token_id=2360))})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import PatchSpec, free_gpu_cache\n",
    "\n",
    "LAYER_NAME_FORMAT = \"model.layers.{}\"\n",
    "QUESTION_ACTOR_IDX = 168\n",
    "\n",
    "free_gpu_cache()\n",
    "\n",
    "patches = []\n",
    "for layer_idx in range(13, 20):\n",
    "    layer = LAYER_NAME_FORMAT.format(layer_idx)\n",
    "    Vh = load_Vh(\"characters\", layer)\n",
    "    location = (layer, QUESTION_ACTOR_IDX)\n",
    "    patches.append(PatchSpec(\n",
    "        location=location,\n",
    "        patch=patch_binding_ID(\n",
    "            h_orig=h_protagonist.states[location],\n",
    "            h_other=h_perpetrator.states[location],\n",
    "            Proj=Vh,\n",
    "            rank=3\n",
    "        )\n",
    "    ))\n",
    "\n",
    "predict_next_token(\n",
    "    lm=lm, \n",
    "    input=prompt_prot,\n",
    "    patches=patches,\n",
    "    interested_tokens=[2360]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PredictedToken(token=' YES', prob=0.6241050362586975, logit=22.640625, token_id=14410),\n",
       "  PredictedToken(token=' yes', prob=0.1816249042749405, logit=21.40625, token_id=10035),\n",
       "  PredictedToken(token=' Yes', prob=0.17880909144878387, logit=21.390625, token_id=7566),\n",
       "  PredictedToken(token=' **', prob=0.007265910971909761, logit=18.1875, token_id=3146),\n",
       "  PredictedToken(token='Yes', prob=0.003950407262891531, logit=17.578125, token_id=9642)],\n",
       " {14410: (1,\n",
       "   PredictedToken(token=' YES', prob=0.6241050362586975, logit=22.640625, token_id=14410))})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free_gpu_cache()\n",
    "\n",
    "patches = []\n",
    "for layer_idx in range(13, 20):\n",
    "    layer = LAYER_NAME_FORMAT.format(layer_idx)\n",
    "    Vh = load_Vh(\"characters\", layer)\n",
    "    location = (layer, QUESTION_ACTOR_IDX)\n",
    "    patches.append(PatchSpec(\n",
    "        location=location,\n",
    "        patch=patch_binding_ID(\n",
    "            h_orig=h_perpetrator.states[location],\n",
    "            h_other=h_protagonist.states[location],\n",
    "            Proj=Vh,\n",
    "            rank=3\n",
    "        )\n",
    "    ))\n",
    "\n",
    "predict_next_token(\n",
    "    lm=lm, \n",
    "    input=prompt_perp,\n",
    "    patches=patches,\n",
    "    interested_tokens=[14410]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer:\" tag.\n",
      "\n",
      "Story: Nora is a magician performing at a grand theater. Nora wants to amaze the audience with a trick involving a red velvet handkerchief. Nora places the red velvet handkerchief in a hat and sets it on the stage. Then Nora prepares a backup box and places a blue silk handkerchief inside. An assistant named Jon, who thinks the trick should be different, swaps the red velvet handkerchief in the hat with the blue silk handkerchief while Nora is backstage. Nora didn't see Jon swapping the the contents of hat.\n",
      "Question: Does Nora believe the hat contains red velvet handkerchief?\n",
      "Answer:\n",
      "yes\n",
      "(173, 178)  red| velvet| hand|ker|chief\n",
      "answer='yes' | predicted_ans=PredictedToken(token=' YES', prob=0.61894690990448, logit=22.71875, token_id=14410)\n"
     ]
    }
   ],
   "source": [
    "from scripts.collect_binding_id_states import collect_token_latent_in_question\n",
    "\n",
    "prompt_obj0, answer_obj0 = dataset.__getitem__(\n",
    "    0, \n",
    "    set_actor=\"protagonist\",\n",
    "    # set_ans=\"no\",\n",
    "    set_container=0,\n",
    "    set_obj=0\n",
    ")\n",
    "print(prompt_obj0)\n",
    "print(answer_obj0)\n",
    "\n",
    "h_obj0 = collect_token_latent_in_question(\n",
    "    lm = lm,\n",
    "    prompt = prompt_obj0,\n",
    "    answer = answer_obj0,\n",
    "    token_of_interest= sample.objects[0],\n",
    "    detensorize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer:\" tag.\n",
      "\n",
      "Story: Nora is a magician performing at a grand theater. Nora wants to amaze the audience with a trick involving a red velvet handkerchief. Nora places the red velvet handkerchief in a hat and sets it on the stage. Then Nora prepares a backup box and places a blue silk handkerchief inside. An assistant named Jon, who thinks the trick should be different, swaps the red velvet handkerchief in the hat with the blue silk handkerchief while Nora is backstage. Nora didn't see Jon swapping the the contents of hat.\n",
      "Question: Does Nora believe the hat contains blue silk handkerchief?\n",
      "Answer:\n",
      "no\n",
      "(173, 178)  blue| silk| hand|ker|chief\n",
      "answer='no' | predicted_ans=PredictedToken(token=' No', prob=0.552524983882904, logit=21.484375, token_id=2360)\n"
     ]
    }
   ],
   "source": [
    "prompt_obj1, answer_obj1= dataset.__getitem__(\n",
    "    0, \n",
    "    set_actor=\"protagonist\",\n",
    "    # set_ans=\"no\",\n",
    "    set_container=0,\n",
    "    set_obj=1\n",
    ")\n",
    "print(prompt_obj1)\n",
    "print(answer_obj1)\n",
    "\n",
    "h_obj1 = collect_token_latent_in_question(\n",
    "    lm = lm,\n",
    "    prompt = prompt_obj1,\n",
    "    answer = answer_obj1,\n",
    "    token_of_interest= sample.objects[1],\n",
    "    detensorize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173, 178)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_obj1.token_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PredictedToken(token=' No', prob=0.47250068187713623, logit=20.28125, token_id=2360),\n",
       "  PredictedToken(token=' NO', prob=0.3978855013847351, logit=20.109375, token_id=5782),\n",
       "  PredictedToken(token=' no', prob=0.0629546269774437, logit=18.265625, token_id=912),\n",
       "  PredictedToken(token=' **', prob=0.04752064496278763, logit=17.984375, token_id=3146),\n",
       "  PredictedToken(token='No', prob=0.003693138714879751, logit=15.4296875, token_id=2822)],\n",
       " {2360: (1,\n",
       "   PredictedToken(token=' No', prob=0.47250068187713623, logit=20.28125, token_id=2360))})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free_gpu_cache()\n",
    "\n",
    "patches = []\n",
    "for layer_idx in range(20, 24):\n",
    "    layer = LAYER_NAME_FORMAT.format(layer_idx)\n",
    "    Vh = load_Vh(\"objects\", layer)\n",
    "    for t in range(*h_obj1.token_position):\n",
    "        location = (layer, t)\n",
    "        patches.append(PatchSpec(\n",
    "            location=location,\n",
    "            patch=patch_binding_ID(\n",
    "                h_orig=h_obj0.states[location],\n",
    "                h_other=h_obj1.states[location],\n",
    "                Proj=Vh,\n",
    "                rank=10\n",
    "            )\n",
    "        ))\n",
    "\n",
    "predict_next_token(\n",
    "    lm=lm, \n",
    "    input=prompt_obj0,\n",
    "    patches=patches,\n",
    "    interested_tokens=[2360]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PredictedToken(token=' YES', prob=0.6483052968978882, logit=21.953125, token_id=14410),\n",
       "  PredictedToken(token=' Yes', prob=0.17448900640010834, logit=20.640625, token_id=7566),\n",
       "  PredictedToken(token=' yes', prob=0.158874049782753, logit=20.546875, token_id=10035),\n",
       "  PredictedToken(token=' **', prob=0.007666510995477438, logit=17.515625, token_id=3146),\n",
       "  PredictedToken(token='Yes', prob=0.004168210085481405, logit=16.90625, token_id=9642)],\n",
       " {14410: (1,\n",
       "   PredictedToken(token=' YES', prob=0.6483052968978882, logit=21.953125, token_id=14410))})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# patches = []\n",
    "# for layer_idx in range(20, 24):\n",
    "#     layer = LAYER_NAME_FORMAT.format(layer_idx)\n",
    "#     Vh = load_Vh(\"objects\", layer)\n",
    "#     location = (layer, QUESTION_OBJ_IDX)\n",
    "#     patches.append(PatchSpec(\n",
    "#         location=location,\n",
    "#         patch=patch_binding_ID(\n",
    "#             h_orig=h_obj1.states[location],\n",
    "#             h_other=h_obj0.states[location],\n",
    "#             Proj=Vh,\n",
    "#             rank=10\n",
    "#         )\n",
    "#     ))\n",
    "\n",
    "patches = []\n",
    "for layer_idx in range(20, 24):\n",
    "    layer = LAYER_NAME_FORMAT.format(layer_idx)\n",
    "    Vh = load_Vh(\"objects\", layer)\n",
    "    for t in range(*h_obj1.token_position):\n",
    "        location = (layer, t)\n",
    "        patches.append(PatchSpec(\n",
    "            location=location,\n",
    "            patch=patch_binding_ID(\n",
    "                h_orig=h_obj1.states[location],\n",
    "                h_other=h_obj0.states[location],\n",
    "                Proj=Vh,\n",
    "                rank=10\n",
    "            )\n",
    "        ))\n",
    "\n",
    "\n",
    "predict_next_token(\n",
    "    lm=lm, \n",
    "    input=prompt_obj1,\n",
    "    patches=patches,\n",
    "    interested_tokens=[14410]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_next_token(\n",
    "    lm=lm, \n",
    "    input=prompt_obj1,\n",
    "    # patches=patches,\n",
    "    interested_tokens=[14410]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer:\" tag.\n",
      "\n",
      "Story: Nora is a magician performing at a grand theater. Nora wants to amaze the audience with a trick involving a knife. Nora places the knife in a hat and sets it on the stage. Then Nora prepares a backup box and places a pen inside. An assistant named Jon, who thinks the trick should be different, swaps the knife in the hat with the pen while Nora is backstage. Nora didn't see Jon swapping the the contents of hat.\n",
      "Question: Does Nora believe the hat contains knife?\n",
      "Answer:\n",
      "yes\n",
      "151  hat\n",
      "answer='yes' | predicted_ans=PredictedToken(token=' YES', prob=0.5597575306892395, logit=22.15625, token_id=14410)\n"
     ]
    }
   ],
   "source": [
    "prompt_cont0, answer_cont0 = dataset.__getitem__(\n",
    "    0, \n",
    "    set_actor=\"protagonist\",\n",
    "    # set_ans=\"no\",\n",
    "    set_container=0,\n",
    "    set_obj=0\n",
    ")\n",
    "\n",
    "print(prompt_cont0)\n",
    "print(answer_cont0)\n",
    "\n",
    "h_cont0 = collect_token_latent_in_question(\n",
    "    lm = lm,\n",
    "    prompt = prompt_cont0,\n",
    "    answer = answer_cont0,\n",
    "    token_of_interest= sample.containers[0],\n",
    "    detensorize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer:\" tag.\n",
      "\n",
      "Story: Nora is a magician performing at a grand theater. Nora wants to amaze the audience with a trick involving a knife. Nora places the knife in a hat and sets it on the stage. Then Nora prepares a backup box and places a pen inside. An assistant named Jon, who thinks the trick should be different, swaps the knife in the hat with the pen while Nora is backstage. Nora didn't see Jon swapping the the contents of hat.\n",
      "Question: Does Nora believe the box contains knife?\n",
      "Answer:\n",
      "no\n",
      "151  box\n",
      "answer='no' | predicted_ans=PredictedToken(token=' YES', prob=0.5212855339050293, logit=21.25, token_id=14410)\n"
     ]
    }
   ],
   "source": [
    "prompt_cont1, answer_cont1 = dataset.__getitem__(\n",
    "    0, \n",
    "    set_actor=\"protagonist\",\n",
    "    # set_ans=\"no\",\n",
    "    set_container=1,\n",
    "    set_obj=0\n",
    ")\n",
    "\n",
    "print(prompt_cont1)\n",
    "print(answer_cont1)\n",
    "\n",
    "h_cont1 = collect_token_latent_in_question(\n",
    "    lm = lm,\n",
    "    prompt = prompt_cont1,\n",
    "    answer = answer_cont1,\n",
    "    token_of_interest= sample.containers[1],\n",
    "    detensorize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PredictedToken(token=' YES', prob=0.5657712817192078, logit=22.203125, token_id=14410),\n",
       "  PredictedToken(token=' yes', prob=0.2471671849489212, logit=21.375, token_id=10035),\n",
       "  PredictedToken(token=' Yes', prob=0.1698753386735916, logit=21.0, token_id=7566),\n",
       "  PredictedToken(token=' **', prob=0.007011593785136938, logit=17.8125, token_id=3146),\n",
       "  PredictedToken(token='Yes', prob=0.003995085600763559, logit=17.25, token_id=9642)],\n",
       " {2360: (23,\n",
       "   PredictedToken(token=' No', prob=3.538393139024265e-05, logit=12.5234375, token_id=2360))})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free_gpu_cache()\n",
    "\n",
    "QUESTION_CONT_IDX = 151\n",
    "\n",
    "patches = []\n",
    "for layer_idx in range(24, 28):\n",
    "    layer = LAYER_NAME_FORMAT.format(layer_idx)\n",
    "    Vh = load_Vh(\"containers\", layer)\n",
    "    location = (layer, QUESTION_CONT_IDX)\n",
    "    patches.append(PatchSpec(\n",
    "        location=location,\n",
    "        patch=patch_binding_ID(\n",
    "            h_orig=h_cont0.states[location],\n",
    "            h_other=h_cont1.states[location],\n",
    "            Proj=Vh,\n",
    "            rank=10\n",
    "        )\n",
    "    ))\n",
    "\n",
    "predict_next_token(\n",
    "    lm=lm, \n",
    "    input=prompt_cont0,\n",
    "    patches=patches,\n",
    "    interested_tokens=[2360]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PredictedToken(token=' YES', prob=0.5225372314453125, logit=21.53125, token_id=14410),\n",
       "  PredictedToken(token=' yes', prob=0.25071609020233154, logit=20.796875, token_id=10035),\n",
       "  PredictedToken(token=' Yes', prob=0.20145605504512787, logit=20.578125, token_id=7566),\n",
       "  PredictedToken(token=' **', prob=0.009570603258907795, logit=17.53125, token_id=3146),\n",
       "  PredictedToken(token='Yes', prob=0.004812401253730059, logit=16.84375, token_id=9642)],\n",
       " {14410: (1,\n",
       "   PredictedToken(token=' YES', prob=0.5225372314453125, logit=21.53125, token_id=14410))})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free_gpu_cache()\n",
    "\n",
    "patches = []\n",
    "for layer_idx in range(24, 28):\n",
    "    layer = LAYER_NAME_FORMAT.format(layer_idx)\n",
    "    Vh = load_Vh(\"containers\", layer)\n",
    "    location = (layer, QUESTION_CONT_IDX)\n",
    "    patches.append(PatchSpec(\n",
    "        location=location,\n",
    "        patch=patch_binding_ID(\n",
    "            h_orig=h_cont1.states[location],\n",
    "            h_other=h_cont0.states[location],\n",
    "            Proj=Vh,\n",
    "            rank=10\n",
    "        )\n",
    "    ))\n",
    "\n",
    "predict_next_token(\n",
    "    lm=lm, \n",
    "    input=prompt_cont1,\n",
    "    patches=patches,\n",
    "    interested_tokens=[14410]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PredictedToken(token=' YES', prob=0.5212855339050293, logit=21.25, token_id=14410),\n",
       "  PredictedToken(token=' yes', prob=0.2462378740310669, logit=20.5, token_id=10035),\n",
       "  PredictedToken(token=' Yes', prob=0.19785767793655396, logit=20.28125, token_id=7566),\n",
       "  PredictedToken(token=' **', prob=0.012259461916983128, logit=17.5, token_id=3146),\n",
       "  PredictedToken(token=' NO', prob=0.005272728856652975, logit=16.65625, token_id=5782)],\n",
       " {14410: (1,\n",
       "   PredictedToken(token=' YES', prob=0.5212855339050293, logit=21.25, token_id=14410))})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_token(\n",
    "    lm=lm, \n",
    "    input=prompt_cont1,\n",
    "    # patches=patches,\n",
    "    interested_tokens=[14410]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
