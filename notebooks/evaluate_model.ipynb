{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [01:15<00:00,  2.51s/it]\n"
     ]
    }
   ],
   "source": [
    "from src.models import load_LM\n",
    "import torch\n",
    "from nnsight import LanguageModel\n",
    "from src.utils import env_utils\n",
    "\n",
    "# model_key = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_key = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "# lm = load_LM(\n",
    "#     model_key=model_key,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     load_in_4bit=True,\n",
    "# )\n",
    "lm = LanguageModel(\n",
    "    os.path.join(env_utils.DEFAULT_MODELS_DIR, model_key), \n",
    "    device_map=\"auto\", \n",
    "    load_in_4bit=True, \n",
    "    torch_dtype=torch.float16, \n",
    "    dispatch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose the correct option by predicting the answer option (either A or B) after the \"Answer:\" tag.\n",
      "\n",
      "Story: Ravi is a farmer in a small Indian village, tending to his mango orchard. Ravi wants to harvest ripe mangoes to sell at the local market. He notices a mango tree with a heavy branch laden with ripe mangoes. A mischievous monkey comes along and eats all the ripe mangoes on the branch while Ravi is gathering his harvesting tools. Ravi sees the monkey eating the ripe mangoes.\n",
      "\n",
      "Question: Does the branch have ripe mangoes or no ripe mangoes?\n",
      "A) The branch has ripe mangoes.\n",
      "B) The branch has no ripe mangoes.\n",
      "Answer:\n",
      "B\n"
     ]
    }
   ],
   "source": [
    "from src.dataset import load_worldstate_dataset\n",
    "dataset = load_worldstate_dataset()\n",
    "q, a = dataset.__getitem__(5, tags=[\"A\", \"B\"])\n",
    "\n",
    "print(q)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm._model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:435: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'B'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import prepare_input\n",
    "from src.functional import interpret_logits\n",
    "from src.functional import free_gpu_cache\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_lm_answer(\n",
    "    lm: LanguageModel,\n",
    "    question: str, \n",
    "):\n",
    "    free_gpu_cache()\n",
    "\n",
    "    inputs = prepare_input(\n",
    "        prompts = question,\n",
    "        tokenizer= lm\n",
    "    )\n",
    "    with lm.trace(inputs, scan=False, validate=False) as tr:\n",
    "        logits = lm.output.logits.save()\n",
    "\n",
    "    predicted_token = interpret_logits(\n",
    "        tokenizer=lm,\n",
    "        logits = logits[:, -1],\n",
    "        k = 10, \n",
    "        get_proba=True\n",
    "    )[0]\n",
    "\n",
    "    return predicted_token[0].strip()\n",
    "\n",
    "get_lm_answer(lm, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200/200 => acc: 0.99(correct=198): 100%|██████████| 200/200 [10:01<00:00,  3.01s/it]              \n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "tag_options = [\n",
    "    (\"a\", \"b\"),\n",
    "    (\"x\", \"y\"), \n",
    "    (\"p\", \"q\"),\n",
    "]\n",
    "\n",
    "progress = tqdm(range(len(dataset)))\n",
    "correct = 0\n",
    "for i in progress:\n",
    "    question, answer = dataset.__getitem__(i, tags=random.choice(tag_options))\n",
    "    lm_ans = get_lm_answer(lm=lm, question=question)\n",
    "    correct += lm_ans.strip() == answer.strip()\n",
    "    # print(f\"expected: {answer.strip()} | got: {lm_ans.strip()}\")\n",
    "    progress.set_description(f\"{i+1}/{len(dataset)} => acc: {correct/(i+1)}({correct=})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
