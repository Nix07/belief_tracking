{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "env.yml not found in /disk/u/nikhil/mind!\n",
      "Setting MODEL_ROOT=\"\". Models will now be downloaded to conda env cache, if not already there\n",
      "Other defaults are set to:\n",
      "    DATA_DIR = \"data\"\n",
      "    RESULTS_DIR = \"results\"\n",
      "    HPARAMS_DIR = \"hparams\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import Any, List, Optional\n",
    "import nnsight\n",
    "from nnsight import CONFIG, LanguageModel\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from einops import einsum\n",
    "import time\n",
    "from einops import rearrange, reduce\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from src.dataset import SampleV3, DatasetV3, STORY_TEMPLATES\n",
    "from src.utils import env_utils\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "random.seed(10)\n",
    "\n",
    "CONFIG.set_default_api_key(\"d9e00ab7d4f74643b3176de0913f24a7\")\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "CONFIG.APP.REMOTE_LOGGING = False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = {}\n",
    "all_containers= {}\n",
    "all_characters = json.load(open(os.path.join(env_utils.DEFAULT_DATA_DIR, \"synthetic_entities\", \"characters.json\"), \"r\"))\n",
    "\n",
    "for TYPE, DCT in {\"states\": all_states, \"containers\": all_containers}.items():\n",
    "    ROOT = os.path.join(\n",
    "        env_utils.DEFAULT_DATA_DIR, \"synthetic_entities\", TYPE\n",
    "    )\n",
    "    for file in os.listdir(ROOT):\n",
    "        file_path = os.path.join(ROOT, file)\n",
    "        with open(file_path, \"r\") as f:\n",
    "            names = json.load(f)\n",
    "        DCT[file.split(\".\")[0]] = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 30/30 [00:44<00:00,  1.49s/it]\n"
     ]
    }
   ],
   "source": [
    "# model = LanguageModel(\"meta-llama/Meta-Llama-3.1-405B-Instruct\")\n",
    "model = LanguageModel(\"meta-llama/Meta-Llama-3-70B-Instruct\", cache_dir=\"/disk/u/nikhil/.cache/huggingface/hub/\", device_map=\"auto\", torch_dtype=torch.float16, dispatch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State token tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50\n",
    "batch_size = 50\n",
    "\n",
    "dataset = get_state_tracing_exps(STORY_TEMPLATES,\n",
    "                             all_characters,\n",
    "                             all_containers,\n",
    "                             all_states,\n",
    "                             n_samples)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any beliefs about the container and its contents which they cannot observe. 4. To answer the question, predict only what is inside the queried container, strictly based on the belief of the character, mentioned in the question. 5. If the queried character has no belief about the container in question, then predict 'unknown'. 6. Do not predict container or character as the final output.\n",
      "\n",
      "Story: Max and Karen are working in a busy restaurant. To complete an order, Max grabs an opaque tun and fills it with port. Then Karen grabs another opaque dispenser and fills it with water.\n",
      "Question: What does Max believe the tun contains?\n",
      "Answer: port\n",
      "Instruction: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any beliefs about the container and its contents which they cannot observe. 4. To answer the question, predict only what is inside the queried container, strictly based on the belief of the character, mentioned in the question. 5. If the queried character has no belief about the container in question, then predict 'unknown'. 6. Do not predict container or character as the final output.\n",
      "\n",
      "Story: Max and Karen are working in a busy restaurant. To complete an order, Max grabs an opaque tun and fills it with coffee. Then Karen grabs another opaque dispenser and fills it with water.\n",
      "Question: What does Max believe the tun contains?\n",
      "Answer: coffee\n",
      "Target: ' port'\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(dataset[idx]['corrupt_prompt'], dataset[idx]['corrupt_ans'])\n",
    "print(dataset[idx]['clean_prompt'], dataset[idx]['clean_ans'])\n",
    "print(f\"Target: '{dataset[idx]['target']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":Answer?\n",
      " contains tun the believe Max does What:Question.\n",
      " water with it fills and dispenser opaque another grabs Karen Then. port with it fills and tun opaque an grabs Max, order an complete To. restaurant busy a in working are Karen and Max:Story\n"
     ]
    }
   ],
   "source": [
    "tokens = model.tokenizer(dataset[idx]['corrupt_prompt'], return_tensors=\"pt\").input_ids\n",
    "print(model.tokenizer.decode(tokens[0][[i for i in range(180, 128, -1)]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/52 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 129 | Layer: 0 | Accuracy: 0.0\n",
      "Token: 129 | Layer: 4 | Accuracy: 0.0\n",
      "Token: 129 | Layer: 8 | Accuracy: 0.0\n",
      "Token: 129 | Layer: 12 | Accuracy: 0.0\n",
      "Token: 129 | Layer: 16 | Accuracy: 0.0\n",
      "Token: 129 | Layer: 20 | Accuracy: 0.0\n",
      "Token: 129 | Layer: 24 | Accuracy: 0.0\n",
      "Token: 129 | Layer: 28 | Accuracy: 0.0\n",
      "Token: 129 | Layer: 32 | Accuracy: 0.0\n",
      "Token: 129 | Layer: 36 | Accuracy: 0.0\n",
      "Token: 129 | Layer: 40 | Accuracy: 0.0\n",
      "Token: 129 | Layer: 44 | Accuracy: 0.0\n",
      "Token: 129 | Layer: 48 | Accuracy: 0.0\n",
      "Token: 129 | Layer: 52 | Accuracy: 0.0\n",
      "Token: 129 | Layer: 56 | Accuracy: 0.0\n",
      "Token: 129 | Layer: 60 | Accuracy: 0.0\n",
      "Token: 129 | Layer: 64 | Accuracy: 0.0\n",
      "Token: 129 | Layer: 68 | Accuracy: 0.0\n",
      "Token: 129 | Layer: 72 | Accuracy: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/52 [03:43<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 129 | Layer: 76 | Accuracy: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tracing_results/state.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Layer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Save tracing results to disk as json file\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtracing_results/state.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     42\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(tracing_results, f)\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tracing_results/state.json'"
     ]
    }
   ],
   "source": [
    "tracing_results = defaultdict(dict)\n",
    "\n",
    "for t in tqdm(range(129, 181)):\n",
    "    for layer_idx in range(0, model.config.num_hidden_layers):\n",
    "        correct, total = 0, 0\n",
    "        \n",
    "        for bi, batch in enumerate(dataloader):\n",
    "            corrupt_prompt = batch[\"corrupt_prompt\"]\n",
    "            clean_prompt = batch[\"clean_prompt\"]\n",
    "            target = batch[\"target\"]\n",
    "            batch_size = len(target)\n",
    "\n",
    "            corrupt_layer_out = defaultdict(dict)\n",
    "            with torch.no_grad():\n",
    "\n",
    "                with model.trace() as tracer:\n",
    "\n",
    "                    with tracer.invoke(corrupt_prompt):\n",
    "                        corrupt_layer_out = model.model.layers[layer_idx].output[0][:, t].clone()\n",
    "\n",
    "                    with tracer.invoke(clean_prompt):\n",
    "                        model.model.layers[layer_idx].output[0][:, t] = corrupt_layer_out\n",
    "\n",
    "                        pred = model.lm_head.output[:, -1].argmax(dim=-1).save()\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                pred_token = model.tokenizer.decode([pred[i]]).lower().strip()\n",
    "                target_token = target[i].lower().strip()\n",
    "                if pred_token == target_token:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "            del corrupt_layer_out, pred\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        acc = round(correct / total, 2)\n",
    "        tracing_results[t][layer_idx] = acc\n",
    "        print(f\"Token: {t} | Layer: {layer_idx} | Accuracy: {acc}\")\n",
    "    \n",
    "    # Save tracing results to disk as json file\n",
    "    with open(\"../tracing_results/state.json\", \"w\") as f:\n",
    "        json.dump(tracing_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Token Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "batch_size = 10\n",
    "\n",
    "dataset = get_obj_tracing_exps(STORY_TEMPLATES,\n",
    "                             all_characters,\n",
    "                             all_containers,\n",
    "                             all_states,\n",
    "                             n_samples)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any beliefs about the container and its contents which they cannot observe. 4. To answer the question, predict only what is inside the queried container, strictly based on the belief of the character, mentioned in the question. 5. If the queried character has no belief about the container in question, then predict 'unknown'. 6. Do not predict container or character as the final output.\n",
      "\n",
      "Story: Rachel and Megan are working in a busy restaurant. To complete an order, Rachel grabs an opaque flask and fills it with cocktail. Then Megan grabs another opaque tank and fills it with wine.\n",
      "Question: What does Rachel believe the flask contains?\n",
      "Answer: cocktail\n",
      "Instruction: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any beliefs about the container and its contents which they cannot observe. 4. To answer the question, predict only what is inside the queried container, strictly based on the belief of the character, mentioned in the question. 5. If the queried character has no belief about the container in question, then predict 'unknown'. 6. Do not predict container or character as the final output.\n",
      "\n",
      "Story: Rachel and Megan are working in a busy restaurant. To complete an order, Rachel grabs an opaque pint and fills it with cocktail. Then Megan grabs another opaque tank and fills it with wine.\n",
      "Question: What does Rachel believe the flask contains?\n",
      "Answer: unknown\n",
      "Target: ' cocktail'\n"
     ]
    }
   ],
   "source": [
    "idx = 6\n",
    "print(dataset[idx]['corrupt_prompt'], dataset[idx]['corrupt_ans'])\n",
    "print(dataset[idx]['clean_prompt'], dataset[idx]['clean_ans'])\n",
    "print(f\"Target: '{dataset[idx]['target']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/52 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 180 | Layer: 0 | Accuracy: 0.1\n",
      "Token: 180 | Layer: 1 | Accuracy: 0.1\n",
      "Token: 180 | Layer: 2 | Accuracy: 0.1\n",
      "Token: 180 | Layer: 3 | Accuracy: 0.1\n",
      "Token: 180 | Layer: 4 | Accuracy: 0.1\n",
      "Token: 180 | Layer: 5 | Accuracy: 0.1\n",
      "Token: 180 | Layer: 6 | Accuracy: 0.1\n",
      "Token: 180 | Layer: 7 | Accuracy: 0.1\n",
      "Token: 180 | Layer: 8 | Accuracy: 0.1\n",
      "Token: 180 | Layer: 9 | Accuracy: 0.1\n",
      "Token: 180 | Layer: 10 | Accuracy: 0.1\n",
      "Token: 180 | Layer: 11 | Accuracy: 0.1\n",
      "Token: 180 | Layer: 12 | Accuracy: 0.1\n",
      "Token: 180 | Layer: 13 | Accuracy: 0.1\n",
      "Token: 180 | Layer: 14 | Accuracy: 0.1\n",
      "Token: 180 | Layer: 15 | Accuracy: 0.1\n",
      "Token: 180 | Layer: 16 | Accuracy: 0.1\n",
      "Token: 180 | Layer: 17 | Accuracy: 0.1\n",
      "Token: 180 | Layer: 18 | Accuracy: 0.2\n",
      "Token: 180 | Layer: 19 | Accuracy: 0.2\n",
      "Token: 180 | Layer: 20 | Accuracy: 0.2\n",
      "Token: 180 | Layer: 21 | Accuracy: 0.2\n",
      "Token: 180 | Layer: 22 | Accuracy: 0.2\n",
      "Token: 180 | Layer: 23 | Accuracy: 0.2\n",
      "Token: 180 | Layer: 24 | Accuracy: 0.2\n",
      "Token: 180 | Layer: 25 | Accuracy: 0.2\n",
      "Token: 180 | Layer: 26 | Accuracy: 0.3\n",
      "Token: 180 | Layer: 27 | Accuracy: 0.3\n",
      "Token: 180 | Layer: 28 | Accuracy: 0.4\n",
      "Token: 180 | Layer: 29 | Accuracy: 0.5\n",
      "Token: 180 | Layer: 30 | Accuracy: 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/52 [01:45<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m             pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39moutput[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[0;32m---> 27\u001b[0m     pred_token \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     28\u001b[0m     target_token \u001b[38;5;241m=\u001b[39m target[i]\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pred_token \u001b[38;5;241m==\u001b[39m target_token:\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3849\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3828\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3829\u001b[0m \u001b[38;5;124;03mConverts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\u001b[39;00m\n\u001b[1;32m   3830\u001b[0m \u001b[38;5;124;03mtokens and clean up tokenization spaces.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3846\u001b[0m \u001b[38;5;124;03m    `str`: The decoded sentence.\u001b[39;00m\n\u001b[1;32m   3847\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3848\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[0;32m-> 3849\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m \u001b[43mto_py_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[1;32m   3852\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[1;32m   3853\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[1;32m   3854\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[1;32m   3855\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3856\u001b[0m )\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/utils/generic.py:271\u001b[0m, in \u001b[0;36mto_py_obj\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: to_py_obj(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [to_py_obj(o) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m obj]\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# This gives us a smart order to test the frameworks with the corresponding tests.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m framework_to_test_func \u001b[38;5;241m=\u001b[39m _get_frameworks_and_test_func(obj)\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/utils/generic.py:271\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: to_py_obj(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mto_py_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m obj]\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# This gives us a smart order to test the frameworks with the corresponding tests.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m framework_to_test_func \u001b[38;5;241m=\u001b[39m _get_frameworks_and_test_func(obj)\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/utils/generic.py:277\u001b[0m, in \u001b[0;36mto_py_obj\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m framework, test_func \u001b[38;5;129;01min\u001b[39;00m framework_to_test_func\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m test_func(obj):\n\u001b[0;32m--> 277\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mframework_to_py_obj\u001b[49m\u001b[43m[\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# tolist also works on 0d np arrays\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, np\u001b[38;5;241m.\u001b[39mnumber):\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/utils/generic.py:262\u001b[0m, in \u001b[0;36mto_py_obj.<locals>.<lambda>\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mto_py_obj\u001b[39m(obj):\n\u001b[1;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m    Convert a TensorFlow tensor, PyTorch tensor, Numpy array or python list to a python list.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     framework_to_py_obj \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: obj\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: np\u001b[38;5;241m.\u001b[39masarray(obj)\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: obj\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    266\u001b[0m     }\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mdict\u001b[39m, UserDict)):\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {k: to_py_obj(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tracing_results = defaultdict(dict)\n",
    "\n",
    "for t in tqdm(range(180, 128, -1)):\n",
    "    for layer_idx in range(0, model.config.num_hidden_layers):\n",
    "        correct, total = 0, 0\n",
    "        \n",
    "        for bi, batch in enumerate(dataloader):\n",
    "            corrupt_prompt = batch[\"corrupt_prompt\"]\n",
    "            clean_prompt = batch[\"clean_prompt\"]\n",
    "            target = batch[\"target\"]\n",
    "            batch_size = len(target)\n",
    "\n",
    "            corrupt_layer_out = defaultdict(dict)\n",
    "            with torch.no_grad():\n",
    "\n",
    "                with model.trace() as tracer:\n",
    "\n",
    "                    with tracer.invoke(corrupt_prompt):\n",
    "                        corrupt_layer_out = model.model.layers[layer_idx].output[0][:, t].clone()\n",
    "\n",
    "                    with tracer.invoke(clean_prompt):\n",
    "                        model.model.layers[layer_idx].output[0][:, t] = corrupt_layer_out\n",
    "\n",
    "                        pred = model.lm_head.output[:, -1].argmax(dim=-1).save()\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                pred_token = model.tokenizer.decode([pred[i]]).lower().strip()\n",
    "                target_token = target[i].lower().strip()\n",
    "                if pred_token == target_token:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "            del corrupt_layer_out, pred\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        acc = round(correct / total, 2)\n",
    "        tracing_results[t][layer_idx] = acc\n",
    "        print(f\"Token: {t} | Layer: {layer_idx} | Accuracy: {acc}\")\n",
    "    \n",
    "    # Save tracing results to disk as json file\n",
    "    with open(\"../tracing_results/object.json\", \"w\") as f:\n",
    "        json.dump(tracing_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Token Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "batch_size = 10\n",
    "\n",
    "dataset = get_character_tracing_exps(STORY_TEMPLATES,\n",
    "                                    all_characters,\n",
    "                                    all_containers,\n",
    "                                    all_states,\n",
    "                                    n_samples)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any beliefs about the container and its contents which they cannot observe. 4. To answer the question, predict only what is inside the queried container, strictly based on the belief of the character, mentioned in the question. 5. If the queried character has no belief about the container in question, then predict 'unknown'. 6. Do not predict container or character as the final output.\n",
      "\n",
      "Story: Nancy and Ray are working in a busy restaurant. To complete an order, Nancy grabs an opaque can and fills it with milk. Then Ray grabs another opaque pitcher and fills it with monster.\n",
      "Question: What does Nancy believe the can contains?\n",
      "Answer: milk\n",
      "Instruction: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any beliefs about the container and its contents which they cannot observe. 4. To answer the question, predict only what is inside the queried container, strictly based on the belief of the character, mentioned in the question. 5. If the queried character has no belief about the container in question, then predict 'unknown'. 6. Do not predict container or character as the final output.\n",
      "\n",
      "Story: James and Ray are working in a busy restaurant. To complete an order, James grabs an opaque can and fills it with milk. Then Ray grabs another opaque pitcher and fills it with monster.\n",
      "Question: What does Nancy believe the can contains?\n",
      "Answer: unknown\n",
      "Target: ' milk'\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "print(dataset[idx]['corrupt_prompt'], dataset[idx]['corrupt_ans'])\n",
    "print(dataset[idx]['clean_prompt'], dataset[idx]['clean_ans'])\n",
    "print(f\"Target: '{dataset[idx]['target']}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
