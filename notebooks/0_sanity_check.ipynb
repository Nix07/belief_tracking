{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "from src.functional import free_gpu_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [01:00<00:00,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded /home/local_arnab/Codes/00_MODEL/meta-llama/Meta-Llama-3-70B-Instruct | size: 36650.535 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.models import load_LM\n",
    "import torch\n",
    "from nnsight import LanguageModel\n",
    "from src.utils import env_utils\n",
    "\n",
    "# model_key = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_key = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "lm = load_LM(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_1.true_state={'pitcher': 'milk', 'mug': 'juice'}\n",
      "sample_1.protagonist_belief={'pitcher': 'milk', 'mug': 'juice'}\n",
      "sample_2.true_state={'pitcher': 'juice', 'mug': 'milk'}\n",
      "sample_2.protagonist_belief={'pitcher': 'juice', 'mug': 'milk'}\n"
     ]
    }
   ],
   "source": [
    "from src.dataset import SampleV3, DatasetV3\n",
    "from src.functional import predict_next_token, get_hs \n",
    "\n",
    "sample_1 = SampleV3(\n",
    "    protagonist=\"Nora\",\n",
    "    perpetrator=\"Jon\",\n",
    "    objects=[\"milk\", \"juice\"],\n",
    "    containers=[\"pitcher\", \"mug\"],\n",
    "    event_idx=None,\n",
    "    event_noticed=False\n",
    ")\n",
    "\n",
    "print(f\"{sample_1.true_state=}\")\n",
    "print(f\"{sample_1.protagonist_belief=}\")\n",
    "\n",
    "sample_2 = SampleV3(\n",
    "    protagonist=\"Nora\",\n",
    "    perpetrator=\"Jon\",\n",
    "    objects=[\"juice\", \"milk\"],\n",
    "    containers=[\"pitcher\", \"mug\"],\n",
    "    event_idx=None,\n",
    "    event_noticed=False\n",
    ")\n",
    "\n",
    "print(f\"{sample_2.true_state=}\")\n",
    "print(f\"{sample_2.protagonist_belief=}\")\n",
    "\n",
    "dataset = DatasetV3(samples = [sample_1, sample_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer:\" tag.\n",
      "\n",
      "Story: Nora is working in a busy restaurant. A customer asks Nora for milk. Nora grabs an opaque pitcher and fills it with milk. Then Nora grabs another opaque mug and fills it with juice.\n",
      "Question: Does Nora believe the pitcher contains juice?\n",
      "Answer:\n",
      "no\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' No', prob=0.5644365549087524, logit=22.046875, token_id=2360),\n",
       " PredictedToken(token=' no', prob=0.22103652358055115, logit=21.109375, token_id=912),\n",
       " PredictedToken(token=' NO', prob=0.1920398771762848, logit=20.96875, token_id=5782),\n",
       " PredictedToken(token=' **', prob=0.014130393043160439, logit=18.359375, token_id=3146),\n",
       " PredictedToken(token='No', prob=0.0035173327196389437, logit=16.96875, token_id=2822)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_1, answer_1 = dataset.__getitem__(\n",
    "    0, \n",
    "    set_actor=\"protagonist\",\n",
    "    # set_ans=\"no\",\n",
    "    set_container=0,\n",
    "    set_obj=1\n",
    ")\n",
    "print(prompt_1)\n",
    "print(answer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose \"yes\" or \"no\" after the \"Answer:\" tag.\n",
      "\n",
      "Story: Nora is working in a busy restaurant. A customer asks Nora for juice. Nora grabs an opaque pitcher and fills it with juice. Then Nora grabs another opaque mug and fills it with milk.\n",
      "Question: Does Nora believe the pitcher contains juice?\n",
      "Answer:\n",
      "yes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' YES', prob=0.6052390336990356, logit=23.1875, token_id=14410),\n",
       " PredictedToken(token=' yes', prob=0.2483895868062973, logit=22.296875, token_id=10035),\n",
       " PredictedToken(token=' Yes', prob=0.13933393359184265, logit=21.71875, token_id=7566),\n",
       " PredictedToken(token='Yes', prob=0.0024351258762180805, logit=17.671875, token_id=9642),\n",
       " PredictedToken(token='yes', prob=0.0015969945816323161, logit=17.25, token_id=9891)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_2, answer_2 = dataset.__getitem__(\n",
    "    1, \n",
    "    set_actor=\"protagonist\",\n",
    "    # set_ans=\"no\",\n",
    "    set_container=0,\n",
    "    set_obj=0\n",
    ")\n",
    "print(prompt_2)\n",
    "print(answer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92  juice\n",
      "80  juice\n"
     ]
    }
   ],
   "source": [
    "from src.functional import prepare_input, find_token_range\n",
    "\n",
    "interested_word = \"juice\"\n",
    "\n",
    "input_1 = prepare_input(\n",
    "    prompts = prompt_1,\n",
    "    tokenizer=lm,\n",
    "    return_offsets_mapping=True\n",
    ")\n",
    "token_pos_1 = find_token_range(\n",
    "    string=prompt_1,\n",
    "    substring=interested_word,\n",
    "    occurrence=-2,\n",
    "    tokenizer=lm,\n",
    "    offset_mapping=input_1[\"offset_mapping\"][0]\n",
    ")[1] - 1 # last token position\n",
    "print(token_pos_1, lm.tokenizer.decode(input_1[\"input_ids\"][0][token_pos_1]))\n",
    "\n",
    "\n",
    "input_2 = prepare_input(\n",
    "    prompts = prompt_2,\n",
    "    tokenizer=lm,\n",
    "    return_offsets_mapping=True\n",
    ")\n",
    "token_pos_2 = find_token_range(\n",
    "    string=prompt_2,\n",
    "    substring=interested_word,\n",
    "    occurrence=-2,\n",
    "    tokenizer=lm,\n",
    "    offset_mapping=input_2[\"offset_mapping\"][0]\n",
    ")[1] - 1 # last token position\n",
    "print(token_pos_2, lm.tokenizer.decode(input_2[\"input_ids\"][0][token_pos_2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' No', prob=0.5644365549087524, logit=22.046875, token_id=2360),\n",
       " PredictedToken(token=' no', prob=0.22103652358055115, logit=21.109375, token_id=912),\n",
       " PredictedToken(token=' NO', prob=0.1920398771762848, logit=20.96875, token_id=5782),\n",
       " PredictedToken(token=' **', prob=0.014130393043160439, logit=18.359375, token_id=3146),\n",
       " PredictedToken(token='No', prob=0.0035173327196389437, logit=16.96875, token_id=2822)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_token(lm, input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' YES', prob=0.6052390336990356, logit=23.1875, token_id=14410),\n",
       " PredictedToken(token=' yes', prob=0.2483895868062973, logit=22.296875, token_id=10035),\n",
       " PredictedToken(token=' Yes', prob=0.13933393359184265, logit=21.71875, token_id=7566),\n",
       " PredictedToken(token='Yes', prob=0.0024351258762180805, logit=17.671875, token_id=9642),\n",
       " PredictedToken(token='yes', prob=0.0015969945816323161, logit=17.25, token_id=9891)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_token(lm, input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import get_hs, free_gpu_cache, PatchSpec\n",
    "\n",
    "free_gpu_cache()\n",
    "layer_name_format = \"model.layers.{}\"\n",
    "layers = [\n",
    "    layer_name_format.format(i) \n",
    "    for i in range(0, 10, 1)\n",
    "]\n",
    "\n",
    "hs_1 = get_hs(\n",
    "    lm=lm, input=input_1,\n",
    "    locations = [(l, token_pos_1) for l in layers],\n",
    ")\n",
    "hs_2 = get_hs(\n",
    "    lm=lm, input=input_2,\n",
    "    locations = [(l, token_pos_2) for l in layers],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0 => \" YES\" (p=0.605)\n",
      "model.layers.1 => \" YES\" (p=0.605)\n",
      "model.layers.2 => \" YES\" (p=0.608)\n",
      "model.layers.3 => \" YES\" (p=0.611)\n",
      "model.layers.4 => \" YES\" (p=0.609)\n",
      "model.layers.5 => \" YES\" (p=0.614)\n",
      "model.layers.6 => \" YES\" (p=0.616)\n",
      "model.layers.7 => \" YES\" (p=0.616)\n",
      "model.layers.8 => \" YES\" (p=0.622)\n",
      "model.layers.9 => \" YES\" (p=0.625)\n"
     ]
    }
   ],
   "source": [
    "patches = []\n",
    "\n",
    "for layer in layers:\n",
    "    patches.append(\n",
    "        PatchSpec(\n",
    "            location=(layer, token_pos_2),\n",
    "            patch=hs_1[(layer, token_pos_1)],\n",
    "        )\n",
    "    )\n",
    "    pred = predict_next_token(lm, input_2, patches=patches, k=2)[0]\n",
    "    print(f\"{layer} => {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
