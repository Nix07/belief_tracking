{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "from src.functional import free_gpu_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [01:08<00:00,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded /home/local_arnab/Codes/00_MODEL/meta-llama/Meta-Llama-3-70B-Instruct | size: 36650.535 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.models import load_LM\n",
    "import torch\n",
    "from nnsight import LanguageModel\n",
    "from src.utils import env_utils\n",
    "from src.functional import free_gpu_cache\n",
    "\n",
    "# model_key = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_key = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "lm = load_LM(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import reset_forwards\n",
    "from src.dataset import load_worldstate_dataset\n",
    "from src.functional import predict_next_token \n",
    "\n",
    "dataset = load_worldstate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose the correct option by predicting the answer option after the \"Answer:\" tag.\n",
      "\n",
      "Story: Luka is a gardener working in a beautiful park in the middle of the city. Luka wants to water the plants in the park to ensure they stay healthy and vibrant. The sky is cloudy, and Luka assumes that it might rain soon. However, the wind picks up and blows the clouds away, revealing a bright and sunny sky. Luka notices the clouds clearing and the sun shining.\n",
      "\n",
      "Question: Will it rain soon, or has the weather become sunny?\n",
      "a) It will rain soon.\n",
      "b) The weather has become sunny.\n",
      "Choose one of the following:\n",
      "Answer:\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "#! why the experiment doesn't work if b is the correct answer?\n",
    "reset_forwards(lm)\n",
    "correct_ans_idx = 1\n",
    "question, answer = dataset.__getitem__(22, tags=(\"a\", \"b\"), correct_ans_idx=correct_ans_idx)\n",
    "print(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_1 = \"\"\"Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose the correct option by predicting the answer option after the \"Answer:\" tag.\n",
    "\n",
    "# Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A decorative mug falls from a shelf in the coffee shop. Noor does not see the decorative mug falling from the shelf.\n",
    "\n",
    "# Question: Does Noor believe the milk pitcher contains oat milk or almond milk?\n",
    "# a) Noor believes the milk pitcher contains almond milk.\n",
    "# b) Noor believes the milk pitcher contains oat milk.\n",
    "# Answer:\"\"\"\n",
    "\n",
    "# answer = \" b\"\n",
    "\n",
    "# question == question_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:435: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' b', prob=0.9804607033729553, logit=19.890625, token_id=293),\n",
       " PredictedToken(token=' ', prob=0.0028192196041345596, logit=14.0390625, token_id=220),\n",
       " PredictedToken(token=' **', prob=0.002648411551490426, logit=13.9765625, token_id=3146),\n",
       " PredictedToken(token=' __', prob=0.002230186015367508, logit=13.8046875, token_id=1328),\n",
       " PredictedToken(token=' (', prob=0.0016834328416734934, logit=13.5234375, token_id=320)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import predict_next_token\n",
    "predict_next_token(\n",
    "    lm = lm,\n",
    "    input = question\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_gpu_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 .\n",
      "\n",
      "166 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.models import prepare_input\n",
    "from src.functional import find_token_range\n",
    "input = prepare_input(\n",
    "    prompts = question,\n",
    "    tokenizer=lm,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "\n",
    "option_1_ends = find_token_range(\n",
    "    string=question,\n",
    "    substring = \".\\n\",\n",
    "    tokenizer=lm,\n",
    "    occurrence=-2,\n",
    "    offset_mapping=input[\"offset_mapping\"][0],\n",
    ")[1] - 1\n",
    "\n",
    "option_2_ends = find_token_range(\n",
    "    string=question,\n",
    "    substring = \".\\n\",\n",
    "    tokenizer=lm,\n",
    "    occurrence=-1,\n",
    "    offset_mapping=input[\"offset_mapping\"][0],\n",
    ")[1] - 1\n",
    "\n",
    "input.pop(\"offset_mapping\")\n",
    "print(option_1_ends, lm.tokenizer.decode(input[\"input_ids\"][0][option_1_ends]))\n",
    "print(option_2_ends, lm.tokenizer.decode(input[\"input_ids\"][0][option_2_ends]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"<|begin_of_text|>Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose the correct option by predicting the answer option after the \"Answer:\" tag.\n",
      "\n",
      "Story: Luka is a gardener working in a beautiful park in the middle of the city. Luka wants to water the plants in the park to ensure they stay healthy and vibrant. The sky is cloudy, and Luka assumes that it might rain soon. However, the wind picks up and blows the clouds away, revealing a bright and sunny sky. Luka notices the clouds clearing and the sun shining.\n",
      "\n",
      "Question: Will it rain soon, or has the weather become sunny?\n",
      "a) It will rain soon\"\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\\"{lm.tokenizer.decode(input['input_ids'][0][:option_1_ends])}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.hooking.llama_attention import LlamaAttentionPatcher, KQV_patch\n",
    "# import types\n",
    "# import baukit\n",
    "# from src.functional import interpret_logits\n",
    "\n",
    "\n",
    "# free_gpu_cache()\n",
    "\n",
    "# patch_range = list(range(35, 39))\n",
    "# module_name_format = \"model.layers.{}.self_attn\"\n",
    "\n",
    "# kqv_states = {layer_idx : {} for layer_idx in patch_range}\n",
    "# kqv_spec = (\n",
    "#     [(h_idx, option_1_ends) for h_idx in range(lm.config.num_attention_heads)] +\n",
    "#     [(h_idx, option_2_ends) for h_idx in range(lm.config.num_attention_heads)]\n",
    "# ) \n",
    "\n",
    "# reset_forwards(lm)\n",
    "# for layer_idx in patch_range:\n",
    "#     attn_module = baukit.get_module(\n",
    "#         lm._model, module_name_format.format(layer_idx)\n",
    "#     )\n",
    "#     attn_module.forward = types.MethodType(\n",
    "#         LlamaAttentionPatcher(\n",
    "#             block_name=module_name_format.format(layer_idx),\n",
    "#             save_kqv_for=kqv_spec,\n",
    "#             kqv_states=kqv_states[layer_idx],\n",
    "#         ), attn_module\n",
    "#     )\n",
    "    \n",
    "# with torch.inference_mode():\n",
    "#     with lm.trace(input, scan=False, validate=False) as trace:\n",
    "#         logits = lm.output.logits[0, -1, :].save()\n",
    "\n",
    "# free_gpu_cache()\n",
    "# reset_forwards(lm)\n",
    "\n",
    "# interpret_logits(\n",
    "#     logits=logits,\n",
    "#     tokenizer=lm\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_gpu_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kqv_patches = {layer_idx : {} for layer_idx in patch_range}\n",
    "# for layer_idx in kqv_states:\n",
    "#     for h_idx in range(lm.config.num_attention_heads):\n",
    "#         kqv_2 = kqv_states[layer_idx][(h_idx, option_2_ends)]\n",
    "#         kqv_patches[layer_idx][(h_idx, option_1_ends)] = KQV_patch(\n",
    "#             # k = kqv_2[\"key\"],\n",
    "#             v = kqv_2[\"value\"],\n",
    "#         )\n",
    "\n",
    "#         kqv_1 = kqv_states[layer_idx][(h_idx, option_1_ends)]\n",
    "#         kqv_patches[layer_idx][(h_idx, option_2_ends)] = KQV_patch(\n",
    "#             # k = kqv_1[\"key\"],\n",
    "#             v = kqv_1[\"value\"],\n",
    "#         )\n",
    "    \n",
    "\n",
    "# reset_forwards(lm)\n",
    "# for layer_idx in patch_range:\n",
    "#     attn_module = baukit.get_module(\n",
    "#         lm._model, module_name_format.format(layer_idx)\n",
    "#     )\n",
    "#     attn_module.forward = types.MethodType(\n",
    "#         LlamaAttentionPatcher(\n",
    "#             block_name=module_name_format.format(layer_idx),\n",
    "#             kqv_patches=kqv_patches[layer_idx],\n",
    "#         ), attn_module\n",
    "#     )\n",
    "# with torch.inference_mode():\n",
    "#     with lm.trace(input, scan=False, validate=False) as trace:\n",
    "#         logits = lm.output.logits[0, -1, :].save()\n",
    "\n",
    "# reset_forwards(lm)\n",
    "# free_gpu_cache()\n",
    "\n",
    "# interpret_logits(\n",
    "#     logits=logits,\n",
    "#     tokenizer=lm\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import get_module_nnsight, interpret_logits\n",
    "from src.utils.typing import TokenizerOutput\n",
    "\n",
    "@torch.inference_mode()\n",
    "def cache_kqv_states(\n",
    "    input: TokenizerOutput,\n",
    "    lm: LanguageModel,\n",
    "    patch_range: list[int],\n",
    "    attn_module_name_format: str = \"model.layers.{}.self_attn\",\n",
    ") -> dict[int, dict[str, torch.Tensor]]:\n",
    "    kqv_states = {layer_idx : {} for layer_idx in patch_range}\n",
    "    with lm.trace(input, scan=False, validate=False) as trace:\n",
    "        for layer_idx in patch_range:\n",
    "            module = get_module_nnsight(lm, attn_module_name_format.format(layer_idx))\n",
    "            kqv_states[layer_idx] = {\n",
    "                \"q\" : module.q_proj.output.clone().save(),\n",
    "                \"k\" : module.k_proj.output.clone().save(),\n",
    "                \"v\" : module.v_proj.output.clone().save(),\n",
    "            }\n",
    "    return kqv_states\n",
    "\n",
    "patch_range = list(range(33, 40))\n",
    "kqv_states = cache_kqv_states(input, lm, patch_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PredictedToken(token=' a', prob=0.9598780870437622, logit=19.046875, token_id=264),\n",
       "  PredictedToken(token=' b', prob=0.00897938385605812, logit=14.375, token_id=293),\n",
       "  PredictedToken(token=' The', prob=0.0053618354722857475, logit=13.859375, token_id=578),\n",
       "  PredictedToken(token=' ', prob=0.004806318320333958, logit=13.75, token_id=220),\n",
       "  PredictedToken(token=' (', prob=0.0034081898629665375, logit=13.40625, token_id=320)],\n",
       " {264: {'rank': 1,\n",
       "   'prob': PredictedToken(token=' a', prob=0.9598780870437622, logit=19.046875, token_id=264)},\n",
       "  293: {'rank': 2,\n",
       "   'prob': PredictedToken(token=' b', prob=0.00897938385605812, logit=14.375, token_id=293)}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.utils.typing import PredictedToken\n",
    "from typing import Literal\n",
    "\n",
    "@torch.inference_mode()\n",
    "def check_kqv_patching_effect(\n",
    "    input: TokenizerOutput,\n",
    "    lm: LanguageModel,\n",
    "    swap_positions: tuple[int, int],\n",
    "    kqv_states: dict[int, dict[str, torch.Tensor]],\n",
    "    patch_type: list[Literal[\"k\", \"q\", \"v\"]] = [\"k\"],\n",
    "    attn_module_name_format: str = \"model.layers.{}.self_attn\",\n",
    "    interested_tokens: list[int] = [],\n",
    ") -> tuple[list[PredictedToken], dict]:\n",
    "    option_1_ends, option_2_ends = swap_positions\n",
    "    with lm.trace(input, scan=False, validate=False) as trace:\n",
    "        for layer_idx in kqv_states:\n",
    "            module = get_module_nnsight(lm, attn_module_name_format.format(layer_idx))\n",
    "            \n",
    "            if \"q\" in patch_type:\n",
    "                module.q_proj.output[:, option_1_ends, :] = kqv_states[layer_idx][\"q\"][:, option_2_ends, :]\n",
    "                module.q_proj.output[:, option_2_ends, :] = kqv_states[layer_idx][\"q\"][:, option_1_ends, :]\n",
    "\n",
    "            if \"k\" in patch_type:\n",
    "                module.k_proj.output[:, option_1_ends, :] = kqv_states[layer_idx][\"k\"][:, option_2_ends, :]\n",
    "                module.k_proj.output[:, option_2_ends, :] = kqv_states[layer_idx][\"k\"][:, option_1_ends, :]\n",
    "\n",
    "            if \"v\" in patch_type:\n",
    "                module.v_proj.output[:, option_1_ends, :] = kqv_states[layer_idx][\"v\"][:, option_2_ends, :]\n",
    "                module.v_proj.output[:, option_2_ends, :] = kqv_states[layer_idx][\"v\"][:, option_1_ends, :]\n",
    "        \n",
    "        logits = lm.output.logits[0, -1, :].save()\n",
    "    \n",
    "    predicted_tokens = interpret_logits(tokenizer = lm, logits = logits)\n",
    "    \n",
    "    for i in range(len(interested_tokens)):\n",
    "        t = interested_tokens[i]\n",
    "        if isinstance(t, str):\n",
    "            interested_tokens[i] = lm.tokenizer.encode(t)[-1]\n",
    "    rank_tokens = logits.argsort(descending=True).tolist()\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    track_interested_tokens = {\n",
    "        t : {\n",
    "            \"rank\": rank_tokens.index(t) + 1,\n",
    "            \"prob\": PredictedToken(\n",
    "                token = lm.tokenizer.decode(t),\n",
    "                prob = probs[t].item(),\n",
    "                logit = logits[t].item(),\n",
    "                token_id = t,\n",
    "            ),\n",
    "        } for t in interested_tokens\n",
    "    }\n",
    "\n",
    "    return predicted_tokens, track_interested_tokens\n",
    "\n",
    "check_kqv_patching_effect(\n",
    "    input=input,\n",
    "    lm=lm,\n",
    "    swap_positions=(option_1_ends, option_2_ends),\n",
    "    kqv_states=kqv_states,\n",
    "    patch_type=[\"k\"],\n",
    "    interested_tokens=[\" a\", \" b\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx=0 | answer='p' | clean_prediction=PredictedToken(token=' p', prob=0.9829173684120178, logit=20.078125, token_id=281)\n",
      "swap_positions=[167, 176]\n",
      "patch_prediction=PredictedToken(token=' q', prob=0.8689221143722534, logit=19.0, token_id=2874)\n",
      "{'rank': 1, 'prob': PredictedToken(token=' q', prob=0.8689221143722534, logit=19.0, token_id=2874)}\n",
      "\n",
      "idx=1 | answer='y' | clean_prediction=PredictedToken(token=' y', prob=0.9763201475143433, logit=19.5625, token_id=379)\n",
      "swap_positions=[177, 189]\n",
      "patch_prediction=PredictedToken(token=' y', prob=0.9680306315422058, logit=19.578125, token_id=379)\n",
      "{'rank': 2, 'prob': PredictedToken(token=' x', prob=0.009639687836170197, logit=14.96875, token_id=865)}\n",
      "\n",
      "idx=2 | answer='p' | clean_prediction=PredictedToken(token=' p', prob=0.9817273616790771, logit=20.28125, token_id=281)\n",
      "swap_positions=[146, 154]\n",
      "patch_prediction=PredictedToken(token=' q', prob=0.8963739275932312, logit=19.1875, token_id=2874)\n",
      "{'rank': 1, 'prob': PredictedToken(token=' q', prob=0.8963739275932312, logit=19.1875, token_id=2874)}\n",
      "\n",
      "idx=3 | answer='x' | clean_prediction=PredictedToken(token=' x', prob=0.9893071055412292, logit=20.5625, token_id=865)\n",
      "swap_positions=[157, 167]\n",
      "patch_prediction=PredictedToken(token=' y', prob=0.9525511264801025, logit=20.390625, token_id=379)\n",
      "{'rank': 1, 'prob': PredictedToken(token=' y', prob=0.9525511264801025, logit=20.390625, token_id=379)}\n",
      "\n",
      "idx=4 | answer='p' | clean_prediction=PredictedToken(token=' p', prob=0.9828943610191345, logit=21.15625, token_id=281)\n",
      "swap_positions=[167, 178]\n",
      "patch_prediction=PredictedToken(token=' q', prob=0.9393095374107361, logit=20.25, token_id=2874)\n",
      "{'rank': 1, 'prob': PredictedToken(token=' q', prob=0.9393095374107361, logit=20.25, token_id=2874)}\n",
      "\n",
      "idx=5 | answer='a' | clean_prediction=PredictedToken(token=' a', prob=0.9745476841926575, logit=19.3125, token_id=264)\n",
      "swap_positions=[167, 176]\n",
      "patch_prediction=PredictedToken(token=' b', prob=0.9214139580726624, logit=19.59375, token_id=293)\n",
      "{'rank': 1, 'prob': PredictedToken(token=' b', prob=0.9214139580726624, logit=19.59375, token_id=293)}\n",
      "\n",
      "idx=6 | answer='a' | clean_prediction=PredictedToken(token=' a', prob=0.977062463760376, logit=19.75, token_id=264)\n",
      "swap_positions=[156, 164]\n",
      "patch_prediction=PredictedToken(token=' b', prob=0.8961290121078491, logit=19.703125, token_id=293)\n",
      "{'rank': 1, 'prob': PredictedToken(token=' b', prob=0.8961290121078491, logit=19.703125, token_id=293)}\n",
      "\n",
      "idx=7 | answer='y' | clean_prediction=PredictedToken(token=' y', prob=0.9897997379302979, logit=20.53125, token_id=379)\n",
      "swap_positions=[163, 172]\n",
      "patch_prediction=PredictedToken(token=' y', prob=0.9820901155471802, logit=20.640625, token_id=379)\n",
      "{'rank': 2, 'prob': PredictedToken(token=' x', prob=0.00785820372402668, logit=15.8125, token_id=865)}\n",
      "\n",
      "idx=8 | answer='a' | clean_prediction=PredictedToken(token=' a', prob=0.9686166644096375, logit=19.21875, token_id=264)\n",
      "swap_positions=[185, 196]\n",
      "patch_prediction=PredictedToken(token=' b', prob=0.9264970421791077, logit=19.453125, token_id=293)\n",
      "{'rank': 1, 'prob': PredictedToken(token=' b', prob=0.9264970421791077, logit=19.453125, token_id=293)}\n",
      "\n",
      "idx=9 | answer='b' | clean_prediction=PredictedToken(token=' b', prob=0.9687978029251099, logit=19.390625, token_id=293)\n",
      "swap_positions=[157, 164]\n",
      "patch_prediction=PredictedToken(token=' b', prob=0.5828506350517273, logit=18.4375, token_id=293)\n",
      "{'rank': 2, 'prob': PredictedToken(token=' a', prob=0.3647386431694031, logit=17.96875, token_id=264)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "limit = 10\n",
    "tag_options = [\n",
    "    (\"a\", \"b\"),\n",
    "    (\"x\", \"y\"), \n",
    "    (\"p\", \"q\"),\n",
    "]\n",
    "patch_range = list(range(35, 40))\n",
    "\n",
    "patch_effects = []\n",
    "limit = limit if limit < len(dataset) else len(dataset)\n",
    "\n",
    "\n",
    "for idx in range(limit):\n",
    "    option_labels = random.choice(tag_options)\n",
    "    correct_ans_idx = random.choice([0, 1])\n",
    "    \n",
    "    question, answer = dataset.__getitem__(idx, tags=option_labels, correct_ans_idx=correct_ans_idx)\n",
    "    clean_prediction = predict_next_token(lm=lm, input=question)[0]\n",
    "    print(f\"{idx=} | {answer=} | {clean_prediction=}\")\n",
    "    if clean_prediction.token.strip() != answer.strip():\n",
    "        print(f\"{idx=} model prediction is wrong, expected {answer.strip()} but got {clean_prediction} ... skipping\")\n",
    "        continue\n",
    "\n",
    "    wrong_ans = option_labels[1 - correct_ans_idx]\n",
    "    input = prepare_input(\n",
    "        prompts = question,\n",
    "        tokenizer=lm,\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "\n",
    "    swap_positions = [\n",
    "        (\n",
    "            find_token_range(\n",
    "                string=question,\n",
    "                substring = \".\\n\",\n",
    "                tokenizer=lm,\n",
    "                occurrence=pos,\n",
    "                offset_mapping=input[\"offset_mapping\"][0],\n",
    "            )[1] - 1\n",
    "        ) for pos in [-2, -1]\n",
    "    ]\n",
    "    print(f\"{swap_positions=}\")\n",
    "\n",
    "    kqv_states = cache_kqv_states(input=input, lm=lm, patch_range=patch_range)\n",
    "    predicted_tokens, track_wrong_ans = check_kqv_patching_effect(\n",
    "        input = input, \n",
    "        lm = lm, \n",
    "        swap_positions = swap_positions,\n",
    "        kqv_states = kqv_states,\n",
    "        patch_type=[\"k\"],\n",
    "        interested_tokens=[f\" {wrong_ans}\"]\n",
    "    )\n",
    "    patch_prediction = predicted_tokens[0]\n",
    "    print(f\"{patch_prediction=}\")\n",
    "    track_wrong_ans = list(track_wrong_ans.values())[0]\n",
    "    print(track_wrong_ans)\n",
    "    patch_effects.append(track_wrong_ans)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch efficacy: 0.7\n"
     ]
    }
   ],
   "source": [
    "efficacy = sum([1 for p in patch_effects if p[\"rank\"] == 1]) / len(patch_effects)\n",
    "print(f\"Patch efficacy: {efficacy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
