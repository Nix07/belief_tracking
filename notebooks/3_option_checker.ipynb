{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "from src.functional import free_gpu_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [01:07<00:00,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded /home/local_arnab/Codes/00_MODEL/meta-llama/Meta-Llama-3-70B-Instruct | size: 36650.535 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.models import load_LM\n",
    "import torch\n",
    "from nnsight import LanguageModel\n",
    "from src.utils import env_utils\n",
    "from src.functional import free_gpu_cache\n",
    "\n",
    "# model_key = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_key = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "lm = load_LM(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import reset_forwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import load_worldstate_dataset\n",
    "from src.functional import predict_next_token \n",
    "\n",
    "dataset = load_worldstate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose the correct option by predicting the answer option (either a or b) after the \"Answer:\" tag.\n",
      "\n",
      "Story: Luka is a gardener working in a beautiful park in the middle of the city. Luka wants to water the plants in the park to ensure they stay healthy and vibrant. The sky is cloudy, and Luka assumes that it might rain soon. However, the wind picks up and blows the clouds away, revealing a bright and sunny sky. Luka does not notice the clouds clearing and the sun shining.\n",
      "\n",
      "Question: Does Luka believe that it will rain soon, or does he think the weather has become sunny?\n",
      "a) Luka believes the weather has become sunny.\n",
      "b) Luka believes that it will rain soon.\n",
      "Answer:\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "#! why the experiment doesn't work if b is the correct answer?\n",
    "reset_forwards(lm)\n",
    "correct_ans_idx = 0\n",
    "question, answer = dataset.__getitem__(22, tags=(\"a\", \"b\"), correct_ans_idx=correct_ans_idx)\n",
    "print(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.functional import predict_next_token\n",
    "# predict_next_token(\n",
    "#     lm = lm,\n",
    "#     input = question\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_gpu_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 .\n",
      "\n",
      "189 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.models import prepare_input\n",
    "from src.functional import find_token_range\n",
    "input = prepare_input(\n",
    "    prompts = question,\n",
    "    tokenizer=lm,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "\n",
    "option_1_ends = find_token_range(\n",
    "    string=question,\n",
    "    substring = \".\\n\",\n",
    "    tokenizer=lm,\n",
    "    occurrence=-2,\n",
    "    offset_mapping=input[\"offset_mapping\"][0],\n",
    ")[1] - 1\n",
    "\n",
    "option_2_ends = find_token_range(\n",
    "    string=question,\n",
    "    substring = \".\\n\",\n",
    "    tokenizer=lm,\n",
    "    occurrence=-1,\n",
    "    offset_mapping=input[\"offset_mapping\"][0],\n",
    ")[1] - 1\n",
    "\n",
    "input.pop(\"offset_mapping\")\n",
    "print(option_1_ends, lm.tokenizer.decode(input[\"input_ids\"][0][option_1_ends]))\n",
    "print(option_2_ends, lm.tokenizer.decode(input[\"input_ids\"][0][option_2_ends]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"<|begin_of_text|>Instruction: Keep track of people's knowledge defined in the story. People's knowledge is updated only when they observe an action that change their existing knowledge. To answer the question following the story, choose the correct option by predicting the answer option (either a or b) after the \"Answer:\" tag.\n",
      "\n",
      "Story: Luka is a gardener working in a beautiful park in the middle of the city. Luka wants to water the plants in the park to ensure they stay healthy and vibrant. The sky is cloudy, and Luka assumes that it might rain soon. However, the wind picks up and blows the clouds away, revealing a bright and sunny sky. Luka does not notice the clouds clearing and the sun shining.\n",
      "\n",
      "Question: Does Luka believe that it will rain soon, or does he think the weather has become sunny?\n",
      "a) Luka believes the weather has become sunny\"\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\\"{lm.tokenizer.decode(input['input_ids'][0][:option_1_ends])}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.hooking.llama_attention import LlamaAttentionPatcher, KQV_patch\n",
    "# import types\n",
    "# import baukit\n",
    "# from src.functional import interpret_logits\n",
    "\n",
    "\n",
    "# free_gpu_cache()\n",
    "\n",
    "# patch_range = list(range(35, 39))\n",
    "# module_name_format = \"model.layers.{}.self_attn\"\n",
    "\n",
    "# kqv_states = {layer_idx : {} for layer_idx in patch_range}\n",
    "# kqv_spec = (\n",
    "#     [(h_idx, option_1_ends) for h_idx in range(lm.config.num_attention_heads)] +\n",
    "#     [(h_idx, option_2_ends) for h_idx in range(lm.config.num_attention_heads)]\n",
    "# ) \n",
    "\n",
    "# reset_forwards(lm)\n",
    "# for layer_idx in patch_range:\n",
    "#     attn_module = baukit.get_module(\n",
    "#         lm._model, module_name_format.format(layer_idx)\n",
    "#     )\n",
    "#     attn_module.forward = types.MethodType(\n",
    "#         LlamaAttentionPatcher(\n",
    "#             block_name=module_name_format.format(layer_idx),\n",
    "#             save_kqv_for=kqv_spec,\n",
    "#             kqv_states=kqv_states[layer_idx],\n",
    "#         ), attn_module\n",
    "#     )\n",
    "    \n",
    "# with torch.inference_mode():\n",
    "#     with lm.trace(input, scan=False, validate=False) as trace:\n",
    "#         logits = lm.output.logits[0, -1, :].save()\n",
    "\n",
    "# free_gpu_cache()\n",
    "# reset_forwards(lm)\n",
    "\n",
    "# interpret_logits(\n",
    "#     logits=logits,\n",
    "#     tokenizer=lm\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_gpu_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kqv_patches = {layer_idx : {} for layer_idx in patch_range}\n",
    "# for layer_idx in kqv_states:\n",
    "#     for h_idx in range(lm.config.num_attention_heads):\n",
    "#         kqv_2 = kqv_states[layer_idx][(h_idx, option_2_ends)]\n",
    "#         kqv_patches[layer_idx][(h_idx, option_1_ends)] = KQV_patch(\n",
    "#             # k = kqv_2[\"key\"],\n",
    "#             v = kqv_2[\"value\"],\n",
    "#         )\n",
    "\n",
    "#         kqv_1 = kqv_states[layer_idx][(h_idx, option_1_ends)]\n",
    "#         kqv_patches[layer_idx][(h_idx, option_2_ends)] = KQV_patch(\n",
    "#             # k = kqv_1[\"key\"],\n",
    "#             v = kqv_1[\"value\"],\n",
    "#         )\n",
    "    \n",
    "\n",
    "# reset_forwards(lm)\n",
    "# for layer_idx in patch_range:\n",
    "#     attn_module = baukit.get_module(\n",
    "#         lm._model, module_name_format.format(layer_idx)\n",
    "#     )\n",
    "#     attn_module.forward = types.MethodType(\n",
    "#         LlamaAttentionPatcher(\n",
    "#             block_name=module_name_format.format(layer_idx),\n",
    "#             kqv_patches=kqv_patches[layer_idx],\n",
    "#         ), attn_module\n",
    "#     )\n",
    "# with torch.inference_mode():\n",
    "#     with lm.trace(input, scan=False, validate=False) as trace:\n",
    "#         logits = lm.output.logits[0, -1, :].save()\n",
    "\n",
    "# reset_forwards(lm)\n",
    "# free_gpu_cache()\n",
    "\n",
    "# interpret_logits(\n",
    "#     logits=logits,\n",
    "#     tokenizer=lm\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' b', prob=0.9983530044555664, logit=22.546875, token_id=293),\n",
       " PredictedToken(token=' ', prob=0.0003482514584902674, logit=14.5859375, token_id=220),\n",
       " PredictedToken(token=' **', prob=0.00023380493803415447, logit=14.1875, token_id=3146),\n",
       " PredictedToken(token='\\xa0b', prob=0.00011941587581532076, logit=13.515625, token_id=115648),\n",
       " PredictedToken(token=' Instruction', prob=0.00011394743341952562, logit=13.46875, token_id=30151)]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import get_module_nnsight, interpret_logits\n",
    "\n",
    "patch_range = list(range(33, 40))\n",
    "attn_module_name_format = \"model.layers.{}.self_attn\"\n",
    "kqv_states = {layer_idx : {} for layer_idx in patch_range}\n",
    "\n",
    "with torch.inference_mode():\n",
    "    with lm.trace(input, scan=False, validate=False) as trace:\n",
    "        for layer_idx in patch_range:\n",
    "            module = get_module_nnsight(lm, attn_module_name_format.format(layer_idx))\n",
    "            kqv_states[layer_idx] = {\n",
    "                \"q\" : module.q_proj.output.clone().save(),\n",
    "                \"k\" : module.k_proj.output.clone().save(),\n",
    "                \"v\" : module.v_proj.output.clone().save(),\n",
    "            }\n",
    "\n",
    "            # option_a_key = lm.model.layers[layer_idx].self_attn.k_proj.output[0, option_1_ends].clone()\n",
    "            # option_a_value = lm.model.layers[layer_idx].self_attn.v_proj.output[0, option_1_ends].clone()\n",
    "            # option_b_key = lm.model.layers[layer_idx].self_attn.k_proj.output[0, option_2_ends].clone()\n",
    "            # option_b_value = lm.model.layers[layer_idx].self_attn.v_proj.output[0, option_2_ends].clone()\n",
    "\n",
    "            # # model.model.layers[layer_idx].self_attn.k_proj.output[0, period_token_indices[0]] = option_b_key\n",
    "            # lm.model.layers[layer_idx].self_attn.v_proj.output[0, option_1_ends] = option_b_value * 1\n",
    "            # # model.model.layers[layer_idx].self_attn.k_proj.output[0, period_token_indices[1]] = option_a_key\n",
    "            # lm.model.layers[layer_idx].self_attn.v_proj.output[0, option_2_ends] = option_a_value * 1\n",
    "        \n",
    "        logits = lm.output.logits[0, -1, :].save()\n",
    "\n",
    "interpret_logits(tokenizer = lm, logits = logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 192, 1024])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kqv_states[35][\"k\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' b', prob=0.7647252082824707, logit=23.15625, token_id=293),\n",
       " PredictedToken(token=' a', prob=0.23322799801826477, logit=21.96875, token_id=264),\n",
       " PredictedToken(token=' ', prob=0.0007308052154257894, logit=16.203125, token_id=220),\n",
       " PredictedToken(token=' \\n', prob=0.00023911852622404695, logit=15.0859375, token_id=720),\n",
       " PredictedToken(token=' **', prob=0.00019979098578915, logit=14.90625, token_id=3146)]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    with lm.trace(input, scan=False, validate=False) as trace:\n",
    "        for layer_idx in patch_range:\n",
    "            module = get_module_nnsight(lm, attn_module_name_format.format(layer_idx))\n",
    "\n",
    "            # module.q_proj.output[:, -1, :] = kqv_states[layer_idx][\"q\"][:, -1, :]\n",
    "            # module.q_proj.output[:, option_2_ends, :] = kqv_states[layer_idx][\"q\"][:, option_1_ends, :]\n",
    "\n",
    "            # module.k_proj.output[:, option_1_ends, :] = kqv_states[layer_idx][\"k\"][:, option_2_ends, :]\n",
    "            # module.k_proj.output[:, option_2_ends, :] = kqv_states[layer_idx][\"k\"][:, option_1_ends, :]\n",
    "\n",
    "            module.v_proj.output[:, option_1_ends, :] = kqv_states[layer_idx][\"v\"][:, option_2_ends, :]\n",
    "            module.v_proj.output[:, option_2_ends, :] = kqv_states[layer_idx][\"v\"][:, option_1_ends, :]\n",
    "\n",
    "        logits = lm.output.logits[0, -1, :].save()\n",
    "\n",
    "interpret_logits(tokenizer = lm, logits = logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
