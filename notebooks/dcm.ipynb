{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/u/nikhil/miniconda3/envs/mind/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "env.yml not found in /disk/u/nikhil/mind!\n",
      "Setting MODEL_ROOT=\"\". Models will now be downloaded to conda env cache, if not already there\n",
      "Other defaults are set to:\n",
      "    DATA_DIR = \"data\"\n",
      "    RESULTS_DIR = \"results\"\n",
      "    HPARAMS_DIR = \"hparams\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import Any, List, Optional\n",
    "import nnsight\n",
    "from nnsight import CONFIG, LanguageModel\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from einops import einsum\n",
    "import time\n",
    "from einops import rearrange, reduce\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from src.dataset import SampleV3, DatasetV3, STORY_TEMPLATES\n",
    "from src.utils import env_utils\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "random.seed(10)\n",
    "\n",
    "CONFIG.set_default_api_key(\"d9e00ab7d4f74643b3176de0913f24a7\")\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "CONFIG.APP.REMOTE_LOGGING = False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = {}\n",
    "all_containers= {}\n",
    "all_characters = json.load(open(os.path.join(env_utils.DEFAULT_DATA_DIR, \"synthetic_entities\", \"characters.json\"), \"r\"))\n",
    "\n",
    "for TYPE, DCT in {\"states\": all_states, \"containers\": all_containers}.items():\n",
    "    ROOT = os.path.join(\n",
    "        env_utils.DEFAULT_DATA_DIR, \"synthetic_entities\", TYPE\n",
    "    )\n",
    "    for file in os.listdir(ROOT):\n",
    "        file_path = os.path.join(ROOT, file)\n",
    "        with open(file_path, \"r\") as f:\n",
    "            names = json.load(f)\n",
    "        DCT[file.split(\".\")[0]] = names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [01:25<00:00,  2.84s/it]\n"
     ]
    }
   ],
   "source": [
    "# model = LanguageModel(\"meta-llama/Meta-Llama-3.1-405B\")\n",
    "model = LanguageModel(\"meta-llama/Meta-Llama-3-70B-Instruct\", cache_dir=\"/disk/u/nikhil/.cache/huggingface/hub/\", device_map=\"auto\", load_in_4bit=True, torch_dtype=torch.float16, dispatch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ques_start_token_idx(tokenizer, prompt):\n",
    "    input_tokens = tokenizer.encode(prompt, return_tensors=\"pt\").squeeze()\n",
    "    corrolary_token = tokenizer.encode(\":\", return_tensors=\"pt\").squeeze()[-1].item()\n",
    "    ques_start_idx = (input_tokens == corrolary_token).nonzero()[2].item()\n",
    "\n",
    "    return ques_start_idx-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_token_len(tokenizer, prompt):\n",
    "    input_tokens = tokenizer.encode(prompt, return_tensors=\"pt\").squeeze()\n",
    "    return len(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pred(pred, target, verbose=False):\n",
    "    prompt = f\"Instruction: Check if the following ground truth and prediction of the state of the object mean the same thing or different. If they mean the same, then predict 'Yes', else 'No' \\n\\nGround truth: {target}\\nPrediction: {pred}\\nAnswer:\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(prompt)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with model.generate(prompt, max_new_tokens=5, do_sample=False, num_return_sequences=1, pad_token_id=model.tokenizer.pad_token_id):\n",
    "            out = model.generator.output.save()\n",
    "\n",
    "    prompt_len = get_prompt_token_len(model.tokenizer, prompt)\n",
    "\n",
    "    return model.tokenizer.decode(out[0][prompt_len:-1]).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading BigToM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a csv file\n",
    "df_false = pd.read_csv(\"../data/bigtom/0_forward_belief_false_belief/stories.csv\", delimiter=\";\")\n",
    "df_true = pd.read_csv(\"../data/bigtom/0_forward_belief_true_belief/stories.csv\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each row in the dataframe extract story, answer, and distractor\n",
    "true_stories, false_stories = [], []\n",
    "for i in range(len(df_true)):\n",
    "    story = df_true.iloc[i]['story']\n",
    "    question = df_true.iloc[i]['question']\n",
    "    answer = df_true.iloc[i]['answer']\n",
    "    distractor = df_true.iloc[i]['distractor']\n",
    "    true_stories.append({\"story\": story, \"question\": question, \"answer\": answer, \"distractor\": distractor})\n",
    "\n",
    "for i in range(len(df_false)):\n",
    "    story = df_false.iloc[i]['story']\n",
    "    question = df_true.iloc[i]['question']\n",
    "    answer = df_false.iloc[i]['answer']\n",
    "    distractor = df_false.iloc[i]['distractor']\n",
    "    false_stories.append({\"story\": story, \"question\": question, \"answer\": answer, \"distractor\": distractor})\n",
    "\n",
    "dataset = []\n",
    "instruction = \"1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any beliefs about the container and its contents which they cannot observe. 4. To answer the question, predict only the final state of the queried object in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict container or character as the final output.\"\n",
    "\n",
    "for i in range(min(len(true_stories), len(false_stories))):\n",
    "    question = true_stories[i]['question']\n",
    "    alt_prompt = f\"Instructions: {instruction}\\n\\nStory: {true_stories[i]['story']}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "    question = false_stories[i]['question']\n",
    "    org_prompt = f\"Instructions: {instruction}\\n\\nStory: {false_stories[i]['story']}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "    dataset.append({\n",
    "        \"visible_prompt\": alt_prompt,\n",
    "        \"visible_ans\": true_stories[i]['answer'],\n",
    "        \"invisible_prompt\": org_prompt,\n",
    "        \"invisible_ans\": false_stories[i]['answer'],\n",
    "    })\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any beliefs about the container and its contents which they cannot observe. 4. To answer the question, predict only the final state of the queried object in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict container or character as the final output.\n",
      "\n",
      "Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor sees her coworker swapping the milk.\n",
      "Question: Does Noor believe the milk pitcher contains oat milk or almond milk?\n",
      "Answer: Noor believes the milk pitcher contains almond milk.\n",
      "Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any beliefs about the container and its contents which they cannot observe. 4. To answer the question, predict only the final state of the queried object in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict container or character as the final output.\n",
      "\n",
      "Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor does not see her coworker swapping the milk.\n",
      "Question: Does Noor believe the milk pitcher contains oat milk or almond milk?\n",
      "Answer: Noor believes the milk pitcher contains oat milk.\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(dataset[idx]['visible_prompt'], dataset[idx]['visible_ans'])\n",
    "print(dataset[idx]['invisible_prompt'], dataset[idx]['invisible_ans'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Custom Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 40\n",
    "valid_size = 20\n",
    "batch_size = 4\n",
    "\n",
    "train_dataset = get_visibility_align_exps(STORY_TEMPLATES,\n",
    "                             all_characters,\n",
    "                             all_containers,\n",
    "                             all_states,\n",
    "                             train_size,\n",
    "                             question_type=\"belief_question\",\n",
    "                             diff_visibility=True)\n",
    "valid_dataset = get_visibility_align_exps(STORY_TEMPLATES,\n",
    "                             all_characters,\n",
    "                             all_containers,\n",
    "                             all_states,\n",
    "                             valid_size,\n",
    "                             question_type=\"belief_question\",\n",
    "                             diff_visibility=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=valid_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any beliefs about the container and its contents which they cannot observe. 4. To answer the question, predict only what is inside the queried container, strictly based on the belief of the character, mentioned in the question. 5. If the queried character has no belief about the container in question, then predict 'unknown'. 6. Do not predict container or character as the final output.\n",
      "\n",
      "Story: Max and Karen are working in a busy restaurant. To complete an order, Max grabs an opaque tun and fills it with coffee. Then Karen grabs another opaque dispenser and fills it with cocoa. They are working side by side and can clearly observe each other's actions.\n",
      "Question: What does Karen believe the tun contains?\n",
      "Answer: coffee\n",
      "Instruction: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any beliefs about the container and its contents which they cannot observe. 4. To answer the question, predict only what is inside the queried container, strictly based on the belief of the character, mentioned in the question. 5. If the queried character has no belief about the container in question, then predict 'unknown'. 6. Do not predict container or character as the final output.\n",
      "\n",
      "Story: Max and Karen are working in a busy restaurant. To complete an order, Max grabs an opaque tun and fills it with port. Then Karen grabs another opaque dispenser and fills it with water. They are working in the entirely separate sections, with no visibility between them.\n",
      "Question: What does Karen believe the tun contains?\n",
      "Answer: unknown\n",
      " port\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(train_dataset[idx]['corrupt_prompt'], train_dataset[idx]['corrupt_ans'])\n",
    "print(train_dataset[idx]['clean_prompt'], train_dataset[idx]['clean_ans'])\n",
    "print(train_dataset[idx]['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Singular Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sing_vecs = defaultdict(dict)\n",
    "for l in range(41):\n",
    "    # sing_vecs[l] = torch.load(f\"/media/sda/bigtom/singular_vecs/{l}.pt\").cpu()\n",
    "    sing_vecs[l] = torch.load(f\"/disk/u/nikhil/mind/selected_tokens_diff/singular_vecs/{l}.pt\").cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_layers = [i for i in range(0, 40, 10)] + [i for i in range(22, 30, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Task Loss: -19.2500, L1 Loss: 250.0000, Total Loss: 230.7500\n",
      "#Causal SVs: 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:41<02:47, 20.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 2, Task Loss: -18.7812, L1 Loss: 199.0000, Total Loss: 205.4688\n",
      "#Causal SVs: 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:25<02:08, 21.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 4, Task Loss: -18.5469, L1 Loss: 150.0000, Total Loss: 180.5500\n",
      "#Causal SVs: 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [02:08<01:26, 21.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 6, Task Loss: -19.0000, L1 Loss: 99.5000, Total Loss: 155.4509\n",
      "#Causal SVs: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [02:52<00:43, 21.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 8, Task Loss: -19.1719, L1 Loss: 49.7500, Total Loss: 130.5451\n",
      "#Causal SVs: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:35<00:00, 21.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete for 32!\n",
      "Validation started for 32\n",
      "#Causal SVs: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:02<00:00, 62.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# valid_accs, rank = {}, {}\n",
    "for layer_idx in range(32, 34, 2):\n",
    "    modules = [i for i in range(sing_vecs[0].shape[0])]\n",
    "    mask = torch.ones(len(modules), requires_grad=True, device=\"cuda\", dtype=torch.bfloat16)\n",
    "    optimizer = torch.optim.Adam([mask], lr=1e-1)\n",
    "    n_epochs = 1\n",
    "    lamb = 0.1\n",
    "\n",
    "    print(f\"Training layer: {layer_idx}\")\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for bi, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "            alt_prompt = batch[\"corrupt_prompt\"]\n",
    "            org_prompt = batch[\"clean_prompt\"]\n",
    "            target = batch[\"target\"]\n",
    "            target_token = model.tokenizer(target, return_tensors=\"pt\").to(\"cuda\").input_ids[:, -1]\n",
    "            batch_size = target_token.size(0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with model.trace() as tracer:\n",
    "                # Cache alternative activations\n",
    "                alt_acts = defaultdict(dict)\n",
    "                with tracer.invoke(alt_prompt):\n",
    "                    for l in range(layer_idx, layer_idx+1):\n",
    "                        for t in range(-8, 0):\n",
    "                            alt_acts[l][t] = model.model.layers[l].output[0][:, t].clone()\n",
    "                \n",
    "                # Process original prompt with modifications\n",
    "                with tracer.invoke(org_prompt):\n",
    "                    for l in range(layer_idx, layer_idx+1):\n",
    "                        sing_vec = sing_vecs[l].cuda()\n",
    "                        # Apply mask and ensure gradients flow\n",
    "                        masked_vec = sing_vec * mask.unsqueeze(-1)\n",
    "                        proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "                        \n",
    "                        for t in range(-8, 0):\n",
    "                            curr_output = model.model.layers[l].output[0][:, t].clone()\n",
    "                            \n",
    "                            # Compute projections while maintaining gradients\n",
    "                            alt_proj = torch.matmul(alt_acts[l][t], proj_matrix)\n",
    "                            org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "                            \n",
    "                            modified_out = curr_output - org_proj + alt_proj\n",
    "                            model.model.layers[l].output[0][:, t] = modified_out\n",
    "                        \n",
    "                        del sing_vec, proj_matrix, masked_vec\n",
    "                        torch.cuda.empty_cache()\n",
    "                    \n",
    "                    logits = model.lm_head.output[:, -1].save()\n",
    "            \n",
    "            # Compute loss with L1 regularization\n",
    "            target_logit = logits[torch.arange(batch_size).cuda(), target_token]\n",
    "            task_loss = -torch.mean(target_logit)\n",
    "            l1_loss = lamb * torch.norm(mask, p=1)\n",
    "            loss = task_loss + l1_loss\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if bi % 2 == 0:\n",
    "                mean_loss = epoch_loss / (bi + 1)\n",
    "                print(f\"Epoch: {epoch}, Batch: {bi}, Task Loss: {task_loss.item():.4f}, \"\n",
    "                    f\"L1 Loss: {l1_loss.item():.4f}, Total Loss: {mean_loss:.4f}\")\n",
    "                with torch.no_grad():\n",
    "                    mask.data.clamp_(0, 1)\n",
    "                    rounded = torch.round(mask)\n",
    "                    print(f\"#Causal SVs: {(rounded == 1).sum().item()}\")\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Clamp after optimizer step\n",
    "            with torch.no_grad():\n",
    "                mask.data.clamp_(0, 1)\n",
    "\n",
    "    print(f\"Training complete for {layer_idx}!\")\n",
    "\n",
    "\n",
    "    print(f\"Validation started for {layer_idx}\")\n",
    "    correct, total = 0, 0\n",
    "    with torch.inference_mode():\n",
    "        mask_data = mask.data.clone()\n",
    "        mask_data.clamp_(0, 1)\n",
    "        rounded = torch.round(mask_data)\n",
    "        print(f\"#Causal SVs: {(rounded == 1).sum().item()}\")\n",
    "        rank[layer_idx] = (rounded == 1).sum().item()\n",
    "\n",
    "        for bi, batch in tqdm(enumerate(valid_dataloader), total=len(valid_dataloader)):\n",
    "            alt_prompt = batch[\"corrupt_prompt\"]\n",
    "            org_prompt = batch[\"clean_prompt\"]\n",
    "            target = batch[\"target\"]\n",
    "            target_token = model.tokenizer(target, return_tensors=\"pt\").input_ids[:, -1]\n",
    "            batch_size = target_token.size(0)\n",
    "\n",
    "            with model.trace() as tracer:\n",
    "                # Cache alternative activations\n",
    "                alt_acts = defaultdict(dict)\n",
    "                with tracer.invoke(alt_prompt):\n",
    "                    for l in range(layer_idx, layer_idx+1):\n",
    "                        for t in range(-8, 0):\n",
    "                            alt_acts[l][t] = model.model.layers[l].output[0][:, t]\n",
    "\n",
    "                # Process original prompt with modifications\n",
    "                with tracer.invoke(org_prompt):\n",
    "                    for l in range(layer_idx, layer_idx+1):\n",
    "                        sing_vec = sing_vecs[l].cuda()\n",
    "                        # Apply mask and ensure gradients flow\n",
    "                        masked_vec = sing_vec.to(rounded.device) * rounded.unsqueeze(-1)\n",
    "                        proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "\n",
    "                        for t in range(-8, 0):\n",
    "                            curr_output = model.model.layers[l].output[0][:, t].clone()\n",
    "\n",
    "                            # Compute projections while maintaining gradients\n",
    "                            alt_proj = torch.matmul(alt_acts[l][t], proj_matrix)\n",
    "                            org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "\n",
    "                            modified_out = curr_output - org_proj + alt_proj\n",
    "                            model.model.layers[l].output[0][:, t] = modified_out\n",
    "\n",
    "                        del sing_vec, proj_matrix, masked_vec\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                    logits = model.lm_head.output[:, -1].save()\n",
    "\n",
    "            pred = torch.argmax(logits, dim=-1).to(target_token.device).cpu()\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                pred_token = model.tokenizer.decode(pred[i])\n",
    "                # print(f\"Predicted: {pred_token.lower().strip()}, Target: {target[i].lower().strip()}\")\n",
    "                if pred_token.lower().strip() == target[i].lower().strip():\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "            \n",
    "            del alt_acts, alt_prompt, org_prompt, target, target_token, logits, pred\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Validation accuracy: {correct / total:.2f}\")\n",
    "    valid_accs[layer_idx] = round(correct / total, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 0.0,\n",
       "  10: 0.0,\n",
       "  20: 0.05,\n",
       "  22: 0.05,\n",
       "  24: 0.4,\n",
       "  26: 0.85,\n",
       "  28: 1.0,\n",
       "  30: 1.0,\n",
       "  32: 1.0},\n",
       " {0: 0, 10: 0, 20: 2, 22: 3, 24: 3, 26: 5, 28: 4, 30: 3, 32: 4})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort valid_accs and rank by key\n",
    "valid_accs = dict(sorted(valid_accs.items()))\n",
    "rank = dict(sorted(rank.items()))\n",
    "valid_accs, rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 5], device='cuda:7')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the indices in rounded which are 1\n",
    "idx = rounded.nonzero().squeeze()\n",
    "idx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
