{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from nnsight import CONFIG, LanguageModel\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "random.seed(10)\n",
    "\n",
    "CONFIG.set_default_api_key(\"d9e00ab7d4f74643b3176de0913f24a7\")\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "CONFIG.APP.REMOTE_LOGGING = False\n",
    "\n",
    "# Define random seed\n",
    "seed = 10\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [00:40<00:00,  1.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# model = LanguageModel(\"meta-llama/Meta-Llama-3.1-405B\")\n",
    "model = LanguageModel(\"meta-llama/Meta-Llama-3-70B-Instruct\", cache_dir=\"/disk/u/nikhil/.cache/huggingface/hub/\", device_map=\"auto\", load_in_4bit=True, torch_dtype=torch.float16, dispatch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ques_start_token_idx(batch_size, tokenizer, prompt):\n",
    "    input_tokens = tokenizer(prompt, return_tensors=\"pt\", padding=True).input_ids\n",
    "    colon_token = tokenizer.encode(\":\", return_tensors=\"pt\").squeeze()[-1].item()\n",
    "    ques_start_idx = (input_tokens == colon_token).nonzero()[torch.arange(2, 4*batch_size, 4)][:, 1] - 1\n",
    "\n",
    "    return ques_start_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_visitibility_sent_start_idx(tokenizer, prompt):\n",
    "    input_tokens = tokenizer(prompt, return_tensors=\"pt\").input_ids.squeeze()\n",
    "\n",
    "    story_start_idx = (input_tokens == 18422).nonzero()[0].item()\n",
    "    period_idx = (input_tokens == 13).nonzero(as_tuple=True)[0]\n",
    "    period_idx = period_idx[period_idx > story_start_idx]\n",
    "\n",
    "    return period_idx[-1] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_token_len(tokenizer, prompt):\n",
    "    input_tokens = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    return input_tokens.attention_mask.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pred(pred, target, verbose=False):\n",
    "    prompt = f\"Instruction: Check if the following ground truth and prediction are similar or not. If they are the same, then predict 'Yes', else 'No'.\\nGround truth: {target}\\nPrediction: {pred}\\nAnswer:\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with model.generate(prompt, max_new_tokens=5, do_sample=False, num_return_sequences=1, pad_token_id=model.tokenizer.pad_token_id):\n",
    "            out = model.generator.output.save()\n",
    "\n",
    "    prompt_len = get_prompt_token_len(model.tokenizer, prompt)\n",
    "    \n",
    "    if verbose:\n",
    "        print(model.tokenizer.decode(out[0]))\n",
    "\n",
    "    return out, prompt_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading BigToM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a csv file\n",
    "df_false = pd.read_csv(\"../data/bigtom/0_forward_belief_false_belief/stories.csv\", delimiter=\";\")\n",
    "df_true = pd.read_csv(\"../data/bigtom/0_forward_belief_true_belief/stories.csv\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sing_vecs = defaultdict(dict)\n",
    "for l in range(model.config.num_hidden_layers):\n",
    "    sing_vecs[l] = torch.load(f\"../svd_results/BigToM/last_token/singular_vecs/{l}.pt\").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = defaultdict(dict)\n",
    "path = \"/disk/u/nikhil/mind/patching_results/BigToM/Meta-Llama-3-70B-Instruct/answer\"\n",
    "# Find all files in the directory\n",
    "files = os.listdir(path)\n",
    "\n",
    "for file in files:\n",
    "    data = json.load(open(os.path.join(path, file), \"r\"))\n",
    "    masks[int(file.split(\".\")[0])] = data[\"singular_vector\"][\"metadata\"][\"mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 80\n",
    "valid_size = 40\n",
    "batch_size = 1\n",
    "\n",
    "dataset = get_bigtom_value_fetcher_exps(df_false, df_true, train_size+valid_size)\n",
    "train_dataset = dataset[:train_size]\n",
    "valid_dataset = dataset[train_size:]\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\n",
      "\n",
      "Story: Amara is a botanist exploring a dense rainforest in search of a rare orchid species. Amara's goal is to find the rare orchid and study its unique characteristics. She spots an orchid with vibrant purple petals that she thinks might be the one she is searching for. As she continues to explore, a sudden downpour washes away the purple pigment from the orchid, revealing that it is actually a common white orchid. Amara witnesses the downpour and the color change in the orchid.\n",
      "Question: Does Amara believe the orchid with vibrant purple petals is the rare species or a common white orchid?\n",
      "Answer: is a common white orchid\n",
      "Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\n",
      "\n",
      "Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor sees her coworker swapping the milk.\n",
      "Question: Does Noor believe the milk pitcher contains oat milk or almond milk?\n",
      "Answer: almond milk\n",
      "Target: is a common white orchid\n"
     ]
    }
   ],
   "source": [
    "idx=0\n",
    "print(dataset[idx]['alt_prompt'], dataset[idx]['alt_ans'])\n",
    "print(dataset[idx]['org_prompt'], dataset[idx]['org_ans'])\n",
    "print(f\"Target: {dataset[idx]['target']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer: 70, lambda: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1/80 [00:04<06:16,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Task Loss: -38.0938, L1 Loss: 20.0000, Total Loss: -18.0938\n",
      "#Rank: 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 5/80 [00:25<06:31,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 4, Task Loss: -30.2344, L1 Loss: 16.1250, Total Loss: -18.0219\n",
      "#Rank: 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 9/80 [00:46<06:08,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 8, Task Loss: -56.2500, L1 Loss: 13.6875, Total Loss: -26.2118\n",
      "#Rank: 274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 13/80 [01:07<05:51,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 12, Task Loss: -32.6875, L1 Loss: 11.3750, Total Loss: -22.4507\n",
      "#Rank: 234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 17/80 [01:27<05:22,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 16, Task Loss: -30.1250, L1 Loss: 10.2500, Total Loss: -20.2206\n",
      "#Rank: 213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 21/80 [01:48<05:05,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 20, Task Loss: -18.6250, L1 Loss: 9.4375, Total Loss: -19.9182\n",
      "#Rank: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 25/80 [02:09<04:49,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 24, Task Loss: -16.7812, L1 Loss: 8.8125, Total Loss: -19.1106\n",
      "#Rank: 183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 29/80 [02:30<04:28,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 28, Task Loss: -70.0000, L1 Loss: 8.3750, Total Loss: -19.9065\n",
      "#Rank: 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 33/80 [02:51<04:07,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 32, Task Loss: -30.3750, L1 Loss: 8.0625, Total Loss: -21.6357\n",
      "#Rank: 167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 37/80 [03:12<03:40,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 36, Task Loss: -44.8750, L1 Loss: 7.8438, Total Loss: -24.0990\n",
      "#Rank: 161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 41/80 [03:32<03:15,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 40, Task Loss: -35.5312, L1 Loss: 7.6875, Total Loss: -25.6317\n",
      "#Rank: 153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 45/80 [03:53<03:03,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 44, Task Loss: -32.8750, L1 Loss: 7.5625, Total Loss: -25.0398\n",
      "#Rank: 152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 49/80 [04:14<02:44,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 48, Task Loss: -31.3281, L1 Loss: 7.4062, Total Loss: -24.5120\n",
      "#Rank: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 53/80 [04:35<02:19,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 52, Task Loss: -20.9375, L1 Loss: 7.4062, Total Loss: -24.6826\n",
      "#Rank: 152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 57/80 [04:56<01:57,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 56, Task Loss: -22.5625, L1 Loss: 7.4062, Total Loss: -24.8930\n",
      "#Rank: 154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 61/80 [05:17<01:39,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 60, Task Loss: -41.3750, L1 Loss: 7.4062, Total Loss: -25.8090\n",
      "#Rank: 154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 65/80 [05:38<01:19,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 64, Task Loss: -33.0625, L1 Loss: 7.4375, Total Loss: -25.8843\n",
      "#Rank: 154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 69/80 [05:59<00:58,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 68, Task Loss: -16.5156, L1 Loss: 7.4062, Total Loss: -25.3851\n",
      "#Rank: 154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 73/80 [06:20<00:36,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 72, Task Loss: -15.8750, L1 Loss: 7.3438, Total Loss: -25.1125\n",
      "#Rank: 152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 77/80 [06:41<00:15,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 76, Task Loss: -44.6250, L1 Loss: 7.2500, Total Loss: -25.8136\n",
      "#Rank: 151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [06:57<00:00,  5.22s/it]\n",
      "  1%|▏         | 1/80 [00:05<07:01,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 0, Task Loss: -36.1250, L1 Loss: 7.1562, Total Loss: -28.9688\n",
      "#Rank: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 5/80 [00:26<06:33,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 4, Task Loss: -29.3594, L1 Loss: 7.0938, Total Loss: -26.2719\n",
      "#Rank: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 9/80 [00:46<06:07,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 8, Task Loss: -59.0000, L1 Loss: 7.1562, Total Loss: -35.1337\n",
      "#Rank: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 13/80 [01:07<05:51,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 12, Task Loss: -30.9375, L1 Loss: 7.0938, Total Loss: -29.9724\n",
      "#Rank: 147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 17/80 [01:28<05:25,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 16, Task Loss: -31.1406, L1 Loss: 7.0938, Total Loss: -26.8722\n",
      "#Rank: 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 21/80 [01:48<05:06,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 20, Task Loss: -19.0312, L1 Loss: 7.0625, Total Loss: -26.0424\n",
      "#Rank: 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 25/80 [02:10<04:50,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 24, Task Loss: -17.3750, L1 Loss: 7.0000, Total Loss: -24.7078\n",
      "#Rank: 145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 29/80 [02:31<04:28,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 28, Task Loss: -69.1250, L1 Loss: 7.0000, Total Loss: -25.0482\n",
      "#Rank: 144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 33/80 [02:52<04:07,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 32, Task Loss: -30.5469, L1 Loss: 7.0000, Total Loss: -26.3857\n",
      "#Rank: 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 37/80 [03:13<03:41,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 36, Task Loss: -44.2500, L1 Loss: 7.0000, Total Loss: -28.5353\n",
      "#Rank: 144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 41/80 [03:33<03:16,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 40, Task Loss: -35.9375, L1 Loss: 7.0000, Total Loss: -29.8306\n",
      "#Rank: 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 45/80 [03:54<03:03,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 44, Task Loss: -32.3125, L1 Loss: 6.9375, Total Loss: -28.9106\n",
      "#Rank: 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 49/80 [04:15<02:43,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 48, Task Loss: -31.7500, L1 Loss: 6.9062, Total Loss: -28.1143\n",
      "#Rank: 145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 53/80 [04:36<02:19,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 52, Task Loss: -21.1875, L1 Loss: 6.9062, Total Loss: -28.1263\n",
      "#Rank: 143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 57/80 [04:56<01:58,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 56, Task Loss: -22.2500, L1 Loss: 6.9062, Total Loss: -28.1575\n",
      "#Rank: 143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 61/80 [05:18<01:39,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 60, Task Loss: -41.6875, L1 Loss: 6.9375, Total Loss: -28.8836\n",
      "#Rank: 143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 65/80 [05:39<01:19,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 64, Task Loss: -32.9375, L1 Loss: 7.0000, Total Loss: -28.7963\n",
      "#Rank: 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 69/80 [06:00<00:58,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 68, Task Loss: -16.1406, L1 Loss: 7.0000, Total Loss: -28.1541\n",
      "#Rank: 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 73/80 [06:21<00:37,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 72, Task Loss: -16.0469, L1 Loss: 6.9375, Total Loss: -27.7548\n",
      "#Rank: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 77/80 [06:42<00:15,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 76, Task Loss: -44.5000, L1 Loss: 6.8438, Total Loss: -28.3524\n",
      "#Rank: 139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [06:58<00:00,  5.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for layer: 70, lambda: 0.05\n",
      "Validation started for layer: 70, lambda: 0.05\n",
      "Rank: 139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1/40 [00:05<03:44,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: N | Target: murky and polluted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [00:11<03:40,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Frag | Target: fragile branches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/40 [00:17<03:35,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Healthy | Target: healthy state\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [00:23<03:31,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: wilt | Target: wilted and less ideal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:29<03:26,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Dis | Target: disrupted by fallen leaves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [00:35<03:21,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: hot | Target: hot sauce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 7/40 [00:41<03:15,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Takes | Target: nearly empty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 8/40 [00:47<03:10,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: with | Target: withered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 9/40 [00:53<03:03,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Frag | Target: fragile branches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 10/40 [00:59<02:57,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Wet | Target: wet and difficult to ignite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 11/40 [01:04<02:51,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: dry | Target: dry and stale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 12/40 [01:10<02:45,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: St | Target: stained and damaged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 13/40 [01:16<02:40,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Green | Target: water\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 14/40 [01:22<02:34,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  | Target: 18°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 15/40 [01:28<02:27,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Off | Target: turned off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 16/40 [01:34<02:21,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: wilt | Target: wilted flowers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 17/40 [01:40<02:15,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Bur | Target: buried under the sand\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 18/40 [01:46<02:10,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: hard | Target: hard and brittle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 19/40 [01:52<02:04,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: washed | Target: washed away and diluted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 20/40 [01:58<01:59,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Enrique | Target: sultanas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 21/40 [02:04<01:52,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Frag | Target: fragile branches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 22/40 [02:10<01:45,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: torn | Target: torn apart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 23/40 [02:15<01:40,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: The | Target: fox has stolen the eggs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 24/40 [02:21<01:34,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: K | Target: Mount Fuji is covered by fog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 25/40 [02:27<01:28,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: flour | Target: flour\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 26/40 [02:33<01:22,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Am | Target: very high temperature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 27/40 [02:39<01:16,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Dam | Target: damaged by the monkey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 28/40 [02:45<01:10,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: severely | Target: severely damaged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 29/40 [02:51<01:04,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Dr | Target: a state of disrepair\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 30/40 [02:56<00:58,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: rough | Target: rough and choppy due to the storm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 31/40 [03:02<00:52,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Has | Target: has shifted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 32/40 [03:08<00:46,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: She | Target: knows about the concealed door\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 33/40 [03:14<00:40,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Fl | Target: been flattened\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 34/40 [03:20<00:35,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Ex | Target: affected by the power outage and has cooled down\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 35/40 [03:26<00:29,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Not | Target: has a broken string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 36/40 [03:31<00:23,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: wilt | Target: wilted and less ideal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 37/40 [03:37<00:17,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Bian | Target: almond milk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 38/40 [03:43<00:11,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Coll | Target: has collapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 39/40 [03:49<00:05,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: wet | Target: wet and wilted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [03:55<00:00,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Ol | Target: underripe\n",
      "Validation accuracy: 0.72 | Correct: 29 | Total: 40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.tokenizer.padding_side = \"left\"\n",
    "\n",
    "valid_accs = defaultdict(dict)\n",
    "for layer_idx in range(70, 72, 2):\n",
    "\n",
    "    n_epochs = 2\n",
    "    lambs = [0.05]\n",
    "    for lamb in lambs:\n",
    "        modules = [i for i in range(sing_vecs[layer_idx].size(0))]\n",
    "        mask = torch.ones(len(modules), requires_grad=True, device=\"cuda\", dtype=torch.bfloat16)\n",
    "        optimizer = torch.optim.Adam([mask], lr=1e-1)\n",
    "\n",
    "        print(f\"Training layer: {layer_idx}, lambda: {lamb}\")\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0\n",
    "\n",
    "            for bi, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "                alt_prompt = batch[\"alt_prompt\"]\n",
    "                org_prompt = batch[\"org_prompt\"]\n",
    "                target = batch[\"target\"]\n",
    "                target_token = model.tokenizer(target, return_tensors=\"pt\", padding=True, padding_side=\"right\")\n",
    "                target_input_ids = target_token.input_ids[:, 1:]\n",
    "                batch_size = target_input_ids.size(0)\n",
    "\n",
    "                alt_ques_start_idx = get_ques_start_token_idx(batch_size, model.tokenizer, alt_prompt, padding_side=\"right\")\n",
    "                alt_prompt_len = get_prompt_token_len(model.tokenizer, alt_prompt, padding_side=\"right\")\n",
    "                org_ques_start_idx = get_ques_start_token_idx(batch_size, model.tokenizer, org_prompt, padding_side=\"right\")\n",
    "                org_prompt_len = get_prompt_token_len(model.tokenizer, org_prompt, padding_side=\"right\")\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with model.trace() as tracer:\n",
    "\n",
    "                    with tracer.invoke(alt_prompt):\n",
    "                        alt_acts = model.model.layers[layer_idx].output[0][0, -1].clone().save()\n",
    "\n",
    "                    with tracer.invoke(org_prompt):\n",
    "                        sing_vec = sing_vecs[layer_idx].cuda()\n",
    "                        masked_vec = sing_vec * mask.unsqueeze(-1)\n",
    "                        proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "\n",
    "                        curr_output = model.model.layers[layer_idx].output[0][0, -1].clone()\n",
    "\n",
    "                        alt_proj = torch.matmul(alt_acts, proj_matrix)\n",
    "                        org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "\n",
    "                        modified_out = curr_output - org_proj + alt_proj\n",
    "                        model.model.layers[layer_idx].output[0][0, -1] = modified_out\n",
    "\n",
    "                        logits = model.lm_head.output[0, -1].save()\n",
    "\n",
    "                        del sing_vec, proj_matrix, masked_vec\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                target_logit = logits[target_input_ids[0]].sum()\n",
    "\n",
    "                task_loss = -(target_logit/batch_size)\n",
    "                l1_loss = lamb * torch.norm(mask, p=1)\n",
    "                loss = task_loss + l1_loss.to(task_loss.device)\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                if bi % 4 == 0:\n",
    "                    mean_loss = epoch_loss / (bi + 1)\n",
    "                    print(f\"Epoch: {epoch}, Batch: {bi}, Task Loss: {task_loss.item():.4f}, \"\n",
    "                        f\"L1 Loss: {l1_loss.item():.4f}, Total Loss: {mean_loss:.4f}\")\n",
    "                    with torch.no_grad():\n",
    "                        mask.data.clamp_(0, 1)\n",
    "                        rounded = torch.round(mask)\n",
    "                        print(f\"#Rank: {(rounded == 1).sum().item()}\")\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    mask.data.clamp_(0, 1)\n",
    "\n",
    "        print(f\"Training finished for layer: {layer_idx}, lambda: {lamb}\")\n",
    "\n",
    "        print(f\"Validation started for layer: {layer_idx}, lambda: {lamb}\")\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            mask_data = mask.data.clone()\n",
    "            mask_data.clamp_(0, 1)\n",
    "            rounded = torch.round(mask)\n",
    "\n",
    "            print(f\"Rank: {(rounded == 1).sum().item()}\")\n",
    "\n",
    "            # Save the mask\n",
    "            # torch.save(mask_data, f\"../masks/bigtom/{layer_idx}.pt\")\n",
    "\n",
    "            for bi, batch in tqdm(enumerate(valid_dataloader), total=len(valid_dataloader)):\n",
    "                alt_prompt = batch[\"alt_prompt\"]\n",
    "                org_prompt = batch[\"org_prompt\"]\n",
    "                alt_ans = batch[\"alt_ans\"]\n",
    "                target = batch[\"target\"][0]\n",
    "                batch_size = len(alt_ans)\n",
    "\n",
    "                alt_ques_start_idx = get_ques_start_token_idx(batch_size, model.tokenizer, alt_prompt, padding_side=\"left\")\n",
    "                alt_prompt_len = get_prompt_token_len(model.tokenizer, alt_prompt, padding_side=\"left\")\n",
    "                org_ques_start_idx = get_ques_start_token_idx(batch_size, model.tokenizer, org_prompt, padding_side=\"left\")\n",
    "                org_prompt_len = get_prompt_token_len(model.tokenizer, org_prompt, padding_side=\"left\")\n",
    "\n",
    "                with model.session() as session:\n",
    "\n",
    "                    with model.trace(alt_prompt):\n",
    "                        alt_acts = model.model.layers[layer_idx].output[0][0, -1].save()\n",
    "\n",
    "                    with model.generate(org_prompt, max_new_tokens=2, do_sample=False, num_return_sequences=1, pad_token_id=model.tokenizer.pad_token_id, eos_token_id=model.tokenizer.eos_token_id):\n",
    "                        sing_vec = sing_vecs[layer_idx].cuda()\n",
    "                        masked_vec = sing_vec * rounded.unsqueeze(-1)\n",
    "                        proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "\n",
    "                        curr_output = model.model.layers[layer_idx].output[0][0, -1].clone()\n",
    "\n",
    "                        alt_proj = torch.matmul(alt_acts, proj_matrix)\n",
    "                        org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "\n",
    "                        modified_out = curr_output - org_proj + alt_proj\n",
    "\n",
    "                        model.model.layers[layer_idx].output[0][0, -1] = modified_out\n",
    "\n",
    "                        out = model.generator.output.save()\n",
    "\n",
    "                    del alt_acts\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                pred = model.tokenizer.decode(out[0][org_prompt_len:-1]).strip()\n",
    "                print(f\"Prediction: {pred} | Target: {target}\")\n",
    "                if pred.lower() in target.lower():\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "            print(f\"Validation accuracy: {correct / total:.2f} | Correct: {correct} | Total: {total}\\n\")\n",
    "            valid_accs[lamb][layer_idx] = round(correct / total, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1/40 [00:06<04:03,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: N | Target: warming up due to the power outage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [00:13<04:11,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: has | Target: has a hairline crack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/40 [00:19<04:02,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Not | Target: have already been found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [00:26<03:57,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: No | Target: has melted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:33<03:56,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: The | Target: fish have moved away\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [00:40<03:49,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: corrupted | Target: corrupted due to a server malfunction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 7/40 [00:47<03:48,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: corrupted | Target: corrupted and difficult to use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 8/40 [00:54<03:39,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: R | Target: rough and dangerous\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 9/40 [01:01<03:37,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Ex | Target: exposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 10/40 [01:08<03:26,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Ru | Target: the paintbrushes are ruined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 11/40 [01:14<03:16,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Dis | Target: disrupted by wind and leaves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 12/40 [01:21<03:10,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: De | Target: devoid of fish due to the volcanic eruption\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 13/40 [01:27<03:01,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: So | Target: soaked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 14/40 [01:35<02:58,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: N | Target: half-baked cookies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 15/40 [01:41<02:47,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Not | Target: not heating at all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 16/40 [01:48<02:40,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: hard | Target: hard and unworkable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 17/40 [01:54<02:31,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  | Target: over-fermented\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 18/40 [02:01<02:29,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: The | Target: become indistinguishable from any other piece of clay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 19/40 [02:08<02:22,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: J | Target: severely damaged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 20/40 [02:15<02:15,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Broken | Target: has a broken string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 21/40 [02:22<02:10,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Am | Target: extremely out of tune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 22/40 [02:29<02:02,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: W | Target: washed away and diluted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 23/40 [02:36<01:57,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Fat | Target: knows about the concealed door\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 24/40 [02:42<01:49,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: T | Target: tangled and difficult to use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 25/40 [02:50<01:44,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Healthy | Target: healthy state\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 26/40 [02:56<01:36,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: slippery | Target: floor is slippery due to the oil spill\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 27/40 [03:03<01:29,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Ru | Target: the paintbrushes are ruined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 28/40 [03:10<01:20,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: torn | Target: torn with a large hole\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 29/40 [03:16<01:13,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Dam | Target: damaged by the storm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 30/40 [03:23<01:07,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: A | Target: filled with sand\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 31/40 [03:30<01:00,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Cr | Target: has crashed and become unresponsive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 32/40 [03:37<00:54,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: W | Target: washed away and diluted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 33/40 [03:43<00:47,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: wilt | Target: have wilted due to the rainstorm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 34/40 [03:50<00:40,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: corrupted | Target: corrupted and difficult to use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 35/40 [03:57<00:33,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Dam | Target: has been badly damaged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 36/40 [04:04<00:27,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: has | Target: that the fishing net has a large hole in it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 37/40 [04:10<00:19,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: wilt | Target: wilted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 38/40 [04:17<00:13,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: We | Target: weakened and infested with termites\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 39/40 [04:23<00:06,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Am | Target: an emerging artist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [04:30<00:00,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: sold | Target: sold out\n",
      "Validation accuracy: 0.80 | Correct: 32 | Total: 40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "layer_idx = 70\n",
    "mask = torch.tensor(masks[layer_idx]).cuda()\n",
    "correct, total = 0, 0\n",
    "\n",
    "with torch.inference_mode():\n",
    "    mask_data = mask.data.clone()\n",
    "    mask_data.clamp_(0, 1)\n",
    "    rounded = torch.round(mask)\n",
    "    # rounded = torch.ones_like(rounded)\n",
    "\n",
    "    print(f\"Rank: {(rounded == 1).sum().item()}\")\n",
    "\n",
    "    # Save the mask\n",
    "    # torch.save(mask_data, f\"../masks/bigtom/{layer_idx}.pt\")\n",
    "\n",
    "    for bi, batch in tqdm(enumerate(valid_dataloader), total=len(valid_dataloader)):\n",
    "        alt_prompt = batch[\"alt_prompt\"]\n",
    "        org_prompt = batch[\"org_prompt\"]\n",
    "        alt_ans = batch[\"alt_ans\"]\n",
    "        target = batch[\"target\"][0]\n",
    "        batch_size = len(alt_ans)\n",
    "\n",
    "        alt_ques_start_idx = get_ques_start_token_idx(batch_size, model.tokenizer, alt_prompt)\n",
    "        alt_prompt_len = get_prompt_token_len(model.tokenizer, alt_prompt)\n",
    "        org_ques_start_idx = get_ques_start_token_idx(batch_size, model.tokenizer, org_prompt)\n",
    "        org_prompt_len = get_prompt_token_len(model.tokenizer, org_prompt)\n",
    "\n",
    "        with model.session() as session:\n",
    "\n",
    "            with model.trace(alt_prompt):\n",
    "                alt_acts = model.model.layers[layer_idx].output[0][0, -1].save()\n",
    "\n",
    "            with model.generate(org_prompt, max_new_tokens=2, do_sample=False, num_return_sequences=1, pad_token_id=model.tokenizer.pad_token_id, eos_token_id=model.tokenizer.eos_token_id):\n",
    "                sing_vec = sing_vecs[layer_idx].cuda()\n",
    "                masked_vec = sing_vec * rounded.unsqueeze(-1)\n",
    "                proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "\n",
    "                curr_output = model.model.layers[layer_idx].output[0][0, -1].clone()\n",
    "\n",
    "                alt_proj = torch.matmul(alt_acts, proj_matrix)\n",
    "                org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "\n",
    "                modified_out = curr_output - org_proj + alt_proj\n",
    "\n",
    "                model.model.layers[layer_idx].output[0][0, -1] = modified_out\n",
    "\n",
    "                out = model.generator.output.save()\n",
    "\n",
    "            del alt_acts\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        pred = model.tokenizer.decode(out[0][org_prompt_len:-1]).strip()\n",
    "        print(f\"Prediction: {pred} | Target: {target}\")\n",
    "        if pred.lower() in target.lower():\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    print(f\"Validation accuracy: {correct / total:.2f} | Correct: {correct} | Total: {total}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer State OID Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sing_vecs = defaultdict(dict)\n",
    "for l in range(model.config.num_hidden_layers):\n",
    "    sing_vecs[l] = torch.load(f\"../svd_results/BigToM/last_token/singular_vecs/{l}.pt\").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 80\n",
    "valid_size = 40\n",
    "batch_size = 1\n",
    "\n",
    "dataset = get_bigtom_answer_state_exps(df_false, df_true, train_size+valid_size)\n",
    "train_dataset = dataset[:train_size]\n",
    "valid_dataset = dataset[train_size:]\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\n",
      "\n",
      "Story: Amara is a botanist exploring a dense rainforest in search of a rare orchid species. Amara's goal is to find the rare orchid and study its unique characteristics. She spots an orchid with vibrant purple petals that she thinks might be the one she is searching for. As she continues to explore, a sudden downpour washes away the purple pigment from the orchid, revealing that it is actually a common white orchid. Amara does not witness the downpour and the color change in the orchid.\n",
      "Question: Does Amara believe the orchid with vibrant purple petals is the rare species or a common white orchid?\n",
      "Answer: with vibrant purple petals is the rare species\n",
      "Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\n",
      "\n",
      "Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor sees her coworker swapping the milk.\n",
      "Question: Does Noor believe the milk pitcher contains oat milk or almond milk?\n",
      "Answer: almond milk\n",
      "Target: oat milk\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(train_dataset[idx]['alt_prompt'], train_dataset[idx]['alt_ans'])\n",
    "print(train_dataset[idx]['org_prompt'], train_dataset[idx]['org_ans'])\n",
    "print(f\"Target: {train_dataset[idx]['target']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer: 40, lambda: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Task Loss: -18.2969, L1 Loss: 2.5000, Total Loss: -15.7969\n",
      "#Rank: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4/80 [00:28<08:54,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 4, Task Loss: -41.1250, L1 Loss: 2.2031, Total Loss: -24.0500\n",
      "#Rank: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 8/80 [00:57<08:24,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 8, Task Loss: -24.0625, L1 Loss: 2.0938, Total Loss: -23.9497\n",
      "#Rank: 465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 12/80 [01:25<07:59,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 12, Task Loss: -103.2500, L1 Loss: 1.9766, Total Loss: -29.3317\n",
      "#Rank: 415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 16/80 [01:53<07:33,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 16, Task Loss: -33.6875, L1 Loss: 1.8906, Total Loss: -30.3116\n",
      "#Rank: 397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 20/80 [02:24<07:52,  7.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 20, Task Loss: -22.3594, L1 Loss: 1.8125, Total Loss: -29.9234\n",
      "#Rank: 380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 24/80 [02:59<08:32,  9.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 24, Task Loss: -27.9375, L1 Loss: 1.7422, Total Loss: -28.7766\n",
      "#Rank: 362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 28/80 [03:28<06:36,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 28, Task Loss: -94.0000, L1 Loss: 1.6875, Total Loss: -30.6654\n",
      "#Rank: 347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 32/80 [03:56<05:47,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 32, Task Loss: -31.0000, L1 Loss: 1.6406, Total Loss: -33.0005\n",
      "#Rank: 336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 36/80 [04:25<05:13,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 36, Task Loss: -52.9375, L1 Loss: 1.6094, Total Loss: -32.9088\n",
      "#Rank: 329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 40/80 [04:52<04:36,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 40, Task Loss: -82.3750, L1 Loss: 1.5781, Total Loss: -35.4611\n",
      "#Rank: 322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 44/80 [05:20<04:12,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 44, Task Loss: -63.0312, L1 Loss: 1.5469, Total Loss: -37.1061\n",
      "#Rank: 310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 48/80 [05:48<03:43,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 48, Task Loss: -28.2188, L1 Loss: 1.5234, Total Loss: -36.7479\n",
      "#Rank: 305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 52/80 [06:17<03:20,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 52, Task Loss: -71.5000, L1 Loss: 1.5078, Total Loss: -37.5789\n",
      "#Rank: 301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 56/80 [06:45<02:49,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 56, Task Loss: -33.1250, L1 Loss: 1.5000, Total Loss: -37.0626\n",
      "#Rank: 301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 60/80 [07:13<02:21,  7.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 60, Task Loss: -117.2500, L1 Loss: 1.4922, Total Loss: -38.2405\n",
      "#Rank: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 64/80 [07:42<01:54,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 64, Task Loss: -27.2188, L1 Loss: 1.4766, Total Loss: -37.9118\n",
      "#Rank: 297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 68/80 [08:11<01:25,  7.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 68, Task Loss: -36.7500, L1 Loss: 1.4688, Total Loss: -37.8049\n",
      "#Rank: 293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 72/80 [08:39<00:56,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 72, Task Loss: -52.4688, L1 Loss: 1.4531, Total Loss: -38.5483\n",
      "#Rank: 293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 76/80 [09:07<00:28,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 76, Task Loss: -9.1953, L1 Loss: 1.4375, Total Loss: -38.5854\n",
      "#Rank: 292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [09:35<00:00,  7.20s/it]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 0, Task Loss: -20.5781, L1 Loss: 1.4297, Total Loss: -19.1484\n",
      "#Rank: 289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4/80 [00:28<08:48,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 4, Task Loss: -46.1562, L1 Loss: 1.4219, Total Loss: -27.7359\n",
      "#Rank: 288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 8/80 [00:57<08:48,  7.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 8, Task Loss: -24.7656, L1 Loss: 1.4062, Total Loss: -26.7700\n",
      "#Rank: 287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 12/80 [01:27<08:22,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 12, Task Loss: -104.3750, L1 Loss: 1.4062, Total Loss: -31.7542\n",
      "#Rank: 286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 16/80 [01:56<07:47,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 16, Task Loss: -35.3125, L1 Loss: 1.3984, Total Loss: -32.8681\n",
      "#Rank: 285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 20/80 [02:25<07:15,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 20, Task Loss: -22.5781, L1 Loss: 1.3984, Total Loss: -32.1410\n",
      "#Rank: 284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 24/80 [02:53<06:37,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 24, Task Loss: -29.4531, L1 Loss: 1.3906, Total Loss: -30.8241\n",
      "#Rank: 283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 28/80 [03:22<06:13,  7.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 28, Task Loss: -94.7500, L1 Loss: 1.3828, Total Loss: -32.6393\n",
      "#Rank: 283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 32/80 [03:50<05:42,  7.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 32, Task Loss: -31.9062, L1 Loss: 1.3828, Total Loss: -35.1773\n",
      "#Rank: 284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 36/80 [04:19<05:20,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 36, Task Loss: -56.3125, L1 Loss: 1.3828, Total Loss: -35.0260\n",
      "#Rank: 281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 40/80 [04:48<04:52,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 40, Task Loss: -85.0625, L1 Loss: 1.3828, Total Loss: -37.5692\n",
      "#Rank: 283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 44/80 [05:17<04:22,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 44, Task Loss: -63.9688, L1 Loss: 1.3828, Total Loss: -39.1311\n",
      "#Rank: 282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 48/80 [05:44<03:43,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 48, Task Loss: -29.0625, L1 Loss: 1.3828, Total Loss: -38.6532\n",
      "#Rank: 281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 52/80 [06:14<03:23,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 52, Task Loss: -72.3125, L1 Loss: 1.3906, Total Loss: -39.3797\n",
      "#Rank: 280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 56/80 [06:42<02:52,  7.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 56, Task Loss: -33.0312, L1 Loss: 1.3906, Total Loss: -38.7527\n",
      "#Rank: 282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 60/80 [07:11<02:22,  7.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 60, Task Loss: -117.9375, L1 Loss: 1.3906, Total Loss: -39.8522\n",
      "#Rank: 283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 64/80 [07:40<01:56,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 64, Task Loss: -27.3750, L1 Loss: 1.3906, Total Loss: -39.4421\n",
      "#Rank: 282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 68/80 [08:09<01:26,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 68, Task Loss: -36.8750, L1 Loss: 1.3828, Total Loss: -39.2557\n",
      "#Rank: 281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 72/80 [08:36<00:55,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 72, Task Loss: -52.6875, L1 Loss: 1.3828, Total Loss: -39.9750\n",
      "#Rank: 281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 76/80 [09:06<00:29,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 76, Task Loss: -9.1016, L1 Loss: 1.3672, Total Loss: -39.9500\n",
      "#Rank: 280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [09:35<00:00,  7.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for layer: 40, lambda: 0.005\n",
      "Validation started for layer: 40, lambda: 0.005\n",
      "Rank: 279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_ques_start_token_idx() got an unexpected keyword argument 'padding_side'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 100\u001b[0m\n\u001b[1;32m     97\u001b[0m target \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     98\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(alt_ans)\n\u001b[0;32m--> 100\u001b[0m alt_ques_start_idx \u001b[38;5;241m=\u001b[39m \u001b[43mget_ques_start_token_idx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malt_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m alt_prompt_len \u001b[38;5;241m=\u001b[39m get_prompt_token_len(model\u001b[38;5;241m.\u001b[39mtokenizer, alt_prompt, padding_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m org_ques_start_idx \u001b[38;5;241m=\u001b[39m get_ques_start_token_idx(batch_size, model\u001b[38;5;241m.\u001b[39mtokenizer, org_prompt, padding_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: get_ques_start_token_idx() got an unexpected keyword argument 'padding_side'"
     ]
    }
   ],
   "source": [
    "model.tokenizer.padding_side = \"left\"\n",
    "\n",
    "valid_accs = defaultdict(dict)\n",
    "for layer_idx in range(40, 42, 2):\n",
    "\n",
    "    n_epochs = 2\n",
    "    lambs = [0.005]\n",
    "    for lamb in lambs:\n",
    "        modules = [i for i in range(sing_vecs[layer_idx].size(0))]\n",
    "        mask = torch.ones(len(modules), requires_grad=True, device=\"cuda\", dtype=torch.bfloat16)\n",
    "        optimizer = torch.optim.Adam([mask], lr=1e-1)\n",
    "\n",
    "        print(f\"Training layer: {layer_idx}, lambda: {lamb}\")\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0\n",
    "\n",
    "            for bi, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "                alt_prompt = batch[\"alt_prompt\"]\n",
    "                org_prompt = batch[\"org_prompt\"]\n",
    "                target = batch[\"target\"]\n",
    "                target_token = model.tokenizer(target, return_tensors=\"pt\", padding=True)\n",
    "                target_input_ids = target_token.input_ids[:, 1:]\n",
    "                batch_size = target_input_ids.size(0)\n",
    "\n",
    "                alt_ques_start_idx = get_ques_start_token_idx(batch_size, model.tokenizer, alt_prompt)\n",
    "                alt_prompt_len = get_prompt_token_len(model.tokenizer, alt_prompt)\n",
    "                org_ques_start_idx = get_ques_start_token_idx(batch_size, model.tokenizer, org_prompt)\n",
    "                org_prompt_len = get_prompt_token_len(model.tokenizer, org_prompt)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with model.trace() as tracer:\n",
    "\n",
    "                    with tracer.invoke(alt_prompt):\n",
    "                        alt_acts = model.model.layers[layer_idx].output[0][0, -1].clone().save()\n",
    "\n",
    "                    with tracer.invoke(org_prompt):\n",
    "                        sing_vec = sing_vecs[layer_idx].cuda()\n",
    "                        masked_vec = sing_vec * mask.unsqueeze(-1)\n",
    "                        proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "\n",
    "                        curr_output = model.model.layers[layer_idx].output[0][0, -1].clone()\n",
    "\n",
    "                        alt_proj = torch.matmul(alt_acts, proj_matrix)\n",
    "                        org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "\n",
    "                        modified_out = curr_output - org_proj + alt_proj\n",
    "                        model.model.layers[layer_idx].output[0][0, -1] = modified_out\n",
    "\n",
    "                        logits = model.lm_head.output[0, -1].save()\n",
    "\n",
    "                        del sing_vec, proj_matrix, masked_vec\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                target_logit = logits[target_input_ids[0]].sum()\n",
    "\n",
    "                task_loss = -(target_logit/batch_size)\n",
    "                l1_loss = lamb * torch.norm(mask, p=1)\n",
    "                loss = task_loss + l1_loss.to(task_loss.device)\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                if bi % 4 == 0:\n",
    "                    mean_loss = epoch_loss / (bi + 1)\n",
    "                    print(f\"Epoch: {epoch}, Batch: {bi}, Task Loss: {task_loss.item():.4f}, \"\n",
    "                        f\"L1 Loss: {l1_loss.item():.4f}, Total Loss: {mean_loss:.4f}\")\n",
    "                    with torch.no_grad():\n",
    "                        mask.data.clamp_(0, 1)\n",
    "                        rounded = torch.round(mask)\n",
    "                        print(f\"#Rank: {(rounded == 1).sum().item()}\")\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    mask.data.clamp_(0, 1)\n",
    "\n",
    "        print(f\"Training finished for layer: {layer_idx}, lambda: {lamb}\")\n",
    "\n",
    "        print(f\"Validation started for layer: {layer_idx}, lambda: {lamb}\")\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            mask_data = mask.data.clone()\n",
    "            mask_data.clamp_(0, 1)\n",
    "            rounded = torch.round(mask)\n",
    "\n",
    "            print(f\"Rank: {(rounded == 1).sum().item()}\")\n",
    "\n",
    "            # Save the mask\n",
    "            # torch.save(mask_data, f\"../masks/bigtom/{layer_idx}.pt\")\n",
    "\n",
    "            for bi, batch in tqdm(enumerate(valid_dataloader), total=len(valid_dataloader)):\n",
    "                alt_prompt = batch[\"alt_prompt\"]\n",
    "                org_prompt = batch[\"org_prompt\"]\n",
    "                alt_ans = batch[\"alt_ans\"]\n",
    "                target = batch[\"target\"][0]\n",
    "                batch_size = len(alt_ans)\n",
    "\n",
    "                alt_ques_start_idx = get_ques_start_token_idx(batch_size, model.tokenizer, alt_prompt)\n",
    "                alt_prompt_len = get_prompt_token_len(model.tokenizer, alt_prompt)\n",
    "                org_ques_start_idx = get_ques_start_token_idx(batch_size, model.tokenizer, org_prompt)\n",
    "                org_prompt_len = get_prompt_token_len(model.tokenizer, org_prompt)\n",
    "\n",
    "                with model.session() as session:\n",
    "\n",
    "                    with model.trace(alt_prompt):\n",
    "                        alt_acts = model.model.layers[layer_idx].output[0][0, -1].save()\n",
    "\n",
    "                    with model.generate(org_prompt, max_new_tokens=2, do_sample=False, num_return_sequences=1, pad_token_id=model.tokenizer.pad_token_id, eos_token_id=model.tokenizer.eos_token_id):\n",
    "                        sing_vec = sing_vecs[layer_idx].cuda()\n",
    "                        masked_vec = sing_vec * rounded.unsqueeze(-1)\n",
    "                        proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "\n",
    "                        curr_output = model.model.layers[layer_idx].output[0][0, -1].clone()\n",
    "\n",
    "                        alt_proj = torch.matmul(alt_acts, proj_matrix)\n",
    "                        org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "\n",
    "                        modified_out = curr_output - org_proj + alt_proj\n",
    "\n",
    "                        model.model.layers[layer_idx].output[0][0, -1] = modified_out\n",
    "\n",
    "                        out = model.generator.output.save()\n",
    "\n",
    "                    del alt_acts\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                pred = model.tokenizer.decode(out[0][org_prompt_len:-1]).strip()\n",
    "                print(f\"Prediction: {pred} | Target: {target}\")\n",
    "                if pred.lower() in target.lower():\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "            print(f\"Validation accuracy: {correct / total:.2f} | Correct: {correct} | Total: {total}\\n\")\n",
    "            valid_accs[lamb][layer_idx] = round(correct / total, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sing_vecs = defaultdict(dict)\n",
    "for l in range(model.config.num_hidden_layers):\n",
    "    sing_vecs[l] = torch.load(f\"../svd_results/BigToM/query_charac_new/singular_vecs/{l}.pt\").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 80\n",
    "valid_size = 40\n",
    "batch_size = 1\n",
    "\n",
    "dataset = get_bigtom_query_charac(df_false, df_true, train_size+valid_size)\n",
    "train_dataset = dataset[:train_size]\n",
    "valid_dataset = dataset[train_size:]\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\n",
      "\n",
      "Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor sees her coworker swapping the milk.\n",
      "Question: Does Noor believe the milk pitcher contains oat milk or almond milk?\n",
      "Answer: almond milk\n",
      "Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\n",
      "\n",
      "Story: Amara is a botanist exploring a dense rainforest in search of a rare orchid species. Amara's goal is to find the rare orchid and study its unique characteristics. She spots an orchid with vibrant purple petals that she thinks might be the one she is searching for. As she continues to explore, a sudden downpour washes away the purple pigment from the orchid, revealing that it is actually a common white orchid. Amara witnesses the downpour and the color change in the orchid.\n",
      "Question: Does Noor believe the orchid with vibrant purple petals is the rare species or a common white orchid?\n",
      "Answer: unknown\n",
      "Target: is a common white orchid\n"
     ]
    }
   ],
   "source": [
    "idx=0\n",
    "print(dataset[idx]['alt_prompt'], dataset[idx]['alt_ans'])\n",
    "print(dataset[idx]['org_prompt'], dataset[idx]['org_ans'])\n",
    "print(f\"Target: {dataset[idx]['target']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer: 14, lambda: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Task Loss: -14.8203, L1 Loss: 4.0000, Total Loss: -10.8203\n",
      "#Rank: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4/80 [00:32<10:24,  8.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 4, Task Loss: -10.4141, L1 Loss: 3.4844, Total Loss: -8.1102\n",
      "#Rank: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 8/80 [01:07<09:57,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 8, Task Loss: -5.6055, L1 Loss: 3.1562, Total Loss: -13.5634\n",
      "#Rank: 699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 12/80 [01:42<09:46,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 12, Task Loss: -21.2031, L1 Loss: 2.9375, Total Loss: -15.6364\n",
      "#Rank: 584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 16/80 [02:18<09:30,  8.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 16, Task Loss: -49.2188, L1 Loss: 2.7344, Total Loss: -23.9784\n",
      "#Rank: 554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 20/80 [02:53<08:53,  8.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 20, Task Loss: -10.2578, L1 Loss: 2.5469, Total Loss: -20.2484\n",
      "#Rank: 525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 24/80 [03:30<08:21,  8.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 24, Task Loss: -30.8125, L1 Loss: 2.4062, Total Loss: -18.9494\n",
      "#Rank: 495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 28/80 [04:06<07:46,  8.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 28, Task Loss: -13.6172, L1 Loss: 2.2656, Total Loss: -20.5725\n",
      "#Rank: 465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 32/80 [04:41<06:58,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 32, Task Loss: -89.8750, L1 Loss: 2.1562, Total Loss: -22.7676\n",
      "#Rank: 432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 36/80 [05:16<06:24,  8.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 36, Task Loss: -4.6680, L1 Loss: 2.0625, Total Loss: -21.8538\n",
      "#Rank: 424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 40/80 [05:51<06:03,  9.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 40, Task Loss: -36.5000, L1 Loss: 1.9922, Total Loss: -22.0757\n",
      "#Rank: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 44/80 [06:25<05:16,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 44, Task Loss: -36.3750, L1 Loss: 1.9375, Total Loss: -22.6069\n",
      "#Rank: 396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 48/80 [07:03<04:58,  9.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 48, Task Loss: -42.7188, L1 Loss: 1.8672, Total Loss: -22.6900\n",
      "#Rank: 377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 52/80 [07:39<04:20,  9.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 52, Task Loss: -40.5625, L1 Loss: 1.8203, Total Loss: -22.8216\n",
      "#Rank: 367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 56/80 [08:14<03:41,  9.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 56, Task Loss: -10.4609, L1 Loss: 1.7812, Total Loss: -22.6715\n",
      "#Rank: 353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 60/80 [08:51<03:05,  9.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 60, Task Loss: -10.3203, L1 Loss: 1.7500, Total Loss: -21.4731\n",
      "#Rank: 350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 64/80 [09:27<02:27,  9.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 64, Task Loss: -59.9375, L1 Loss: 1.7266, Total Loss: -22.4138\n",
      "#Rank: 344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 68/80 [10:06<01:55,  9.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 68, Task Loss: -11.9141, L1 Loss: 1.7109, Total Loss: -22.0800\n",
      "#Rank: 340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 72/80 [10:43<01:12,  9.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 72, Task Loss: -41.9688, L1 Loss: 1.6875, Total Loss: -21.6590\n",
      "#Rank: 334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 76/80 [11:18<00:36,  9.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 76, Task Loss: -9.9688, L1 Loss: 1.6719, Total Loss: -21.6988\n",
      "#Rank: 333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [11:54<00:00,  8.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for layer: 14, lambda: 0.005\n",
      "Validation started for layer: 14, lambda: 0.005\n",
      "Rank: 327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1/40 [00:05<03:45,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: N | Target: warming up due to the power outage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [00:12<04:00,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: has | Target: has a hairline crack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/40 [00:19<04:04,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Already | Target: have already been found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [00:26<04:01,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: melted | Target: has melted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:33<03:56,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Fish | Target: fish have moved away\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [00:39<03:50,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: A | Target: corrupted due to a server malfunction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 7/40 [00:46<03:44,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: corrupted | Target: corrupted and difficult to use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 8/40 [00:53<03:38,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: rough | Target: rough and dangerous\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 9/40 [01:00<03:31,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Not | Target: exposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 10/40 [01:07<03:24,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Ru | Target: the paintbrushes are ruined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 11/40 [01:14<03:18,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Dis | Target: disrupted by wind and leaves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 12/40 [01:21<03:11,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: De | Target: devoid of fish due to the volcanic eruption\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 13/40 [01:27<03:04,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: N | Target: soaked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 14/40 [01:34<02:58,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: N | Target: half-baked cookies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 15/40 [01:41<02:53,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Not | Target: not heating at all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 16/40 [01:48<02:45,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Am | Target: hard and unworkable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 17/40 [01:55<02:37,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Over | Target: over-fermented\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 18/40 [02:02<02:31,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Mei | Target: become indistinguishable from any other piece of clay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 19/40 [02:09<02:25,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: severely | Target: severely damaged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 20/40 [02:16<02:19,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Enrique | Target: has a broken string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 21/40 [02:23<02:12,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: extremely | Target: extremely out of tune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 22/40 [02:30<02:04,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Kw | Target: washed away and diluted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 23/40 [02:37<01:57,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: concealed | Target: knows about the concealed door\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 24/40 [02:44<01:50,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: tangled | Target: tangled and difficult to use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 25/40 [02:50<01:43,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Healthy | Target: healthy state\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 26/40 [02:57<01:36,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Slip | Target: floor is slippery due to the oil spill\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 27/40 [03:04<01:29,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Ru | Target: the paintbrushes are ruined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 28/40 [03:11<01:22,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: torn | Target: torn with a large hole\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 29/40 [03:18<01:16,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: N | Target: damaged by the storm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 30/40 [03:25<01:09,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: F | Target: filled with sand\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 31/40 [03:32<01:02,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: There | Target: has crashed and become unresponsive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 32/40 [03:39<00:55,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Wash | Target: washed away and diluted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 33/40 [03:46<00:48,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: W | Target: have wilted due to the rainstorm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 34/40 [03:53<00:41,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: corrupted | Target: corrupted and difficult to use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 35/40 [03:59<00:34,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Rash | Target: has been badly damaged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 36/40 [04:06<00:27,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Im | Target: that the fishing net has a large hole in it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 37/40 [04:13<00:20,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: W | Target: wilted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 38/40 [04:20<00:13,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: weakened | Target: weakened and infested with termites\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 39/40 [04:27<00:06,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: emerging | Target: an emerging artist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [04:34<00:00,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Not | Target: sold out\n",
      "Validation accuracy: 0.70 | Correct: 28 | Total: 40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.tokenizer.padding_side = \"left\"\n",
    "\n",
    "valid_accs = defaultdict(dict)\n",
    "for layer_idx in range(14, 16, 2):\n",
    "\n",
    "    n_epochs = 1\n",
    "    lambs = [0.005]\n",
    "    for lamb in lambs:\n",
    "        modules = [i for i in range(sing_vecs[layer_idx].size(0))]\n",
    "        mask = torch.ones(len(modules), requires_grad=True, device=\"cuda\", dtype=torch.bfloat16)\n",
    "        optimizer = torch.optim.Adam([mask], lr=1e-1)\n",
    "\n",
    "        print(f\"Training layer: {layer_idx}, lambda: {lamb}\")\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0\n",
    "\n",
    "            for bi, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "                alt_prompt = batch[\"alt_prompt\"]\n",
    "                org_prompt = batch[\"org_prompt\"]\n",
    "                target = batch[\"target\"]\n",
    "                target_token = model.tokenizer(target, return_tensors=\"pt\", padding=True, padding_side=\"right\")\n",
    "                target_input_ids = target_token.input_ids[:, 1:]\n",
    "                batch_size = target_input_ids.size(0)\n",
    "\n",
    "                alt_ques_start_idx = get_ques_start_token_idx(batch_size, model.tokenizer, alt_prompt, padding_side=\"right\")\n",
    "                alt_prompt_len = get_prompt_token_len(model.tokenizer, alt_prompt, padding_side=\"right\")\n",
    "                org_ques_start_idx = get_ques_start_token_idx(batch_size, model.tokenizer, org_prompt, padding_side=\"right\")\n",
    "                org_prompt_len = get_prompt_token_len(model.tokenizer, org_prompt, padding_side=\"right\")\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with model.trace() as tracer:\n",
    "                    alt_acts = defaultdict(dict)\n",
    "                    with tracer.invoke(alt_prompt):\n",
    "                        for t_idx, t in enumerate([i for i in range(alt_ques_start_idx+3, alt_ques_start_idx+5)]):\n",
    "                            alt_acts[t_idx] = model.model.layers[layer_idx].output[0][0, t].clone().save()\n",
    "\n",
    "                    with tracer.invoke(org_prompt):\n",
    "                        sing_vec = sing_vecs[layer_idx].cuda()\n",
    "                        masked_vec = sing_vec * mask.unsqueeze(-1)\n",
    "                        proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "\n",
    "                        for t_idx, t in enumerate([i for i in range(org_ques_start_idx+3, org_ques_start_idx+5)]):\n",
    "                            curr_output = model.model.layers[layer_idx].output[0][0, t].clone()\n",
    "\n",
    "                            alt_proj = torch.matmul(alt_acts[t_idx], proj_matrix)\n",
    "                            org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "\n",
    "                            modified_out = curr_output - org_proj + alt_proj\n",
    "                            model.model.layers[layer_idx].output[0][0, t] = modified_out\n",
    "\n",
    "                        logits = model.lm_head.output[0, -1].save()\n",
    "\n",
    "                        del sing_vec, proj_matrix, masked_vec\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                target_logit = logits[target_input_ids[0]].sum()\n",
    "\n",
    "                task_loss = -(target_logit/batch_size)\n",
    "                l1_loss = lamb * torch.norm(mask, p=1)\n",
    "                loss = task_loss + l1_loss.to(task_loss.device)\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                if bi % 4 == 0:\n",
    "                    mean_loss = epoch_loss / (bi + 1)\n",
    "                    print(f\"Epoch: {epoch}, Batch: {bi}, Task Loss: {task_loss.item():.4f}, \"\n",
    "                        f\"L1 Loss: {l1_loss.item():.4f}, Total Loss: {mean_loss:.4f}\")\n",
    "                    with torch.no_grad():\n",
    "                        mask.data.clamp_(0, 1)\n",
    "                        rounded = torch.round(mask)\n",
    "                        print(f\"#Rank: {(rounded == 1).sum().item()}\")\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    mask.data.clamp_(0, 1)\n",
    "\n",
    "        print(f\"Training finished for layer: {layer_idx}, lambda: {lamb}\")\n",
    "\n",
    "        print(f\"Validation started for layer: {layer_idx}, lambda: {lamb}\")\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            mask_data = mask.data.clone()\n",
    "            mask_data.clamp_(0, 1)\n",
    "            rounded = torch.round(mask)\n",
    "\n",
    "            print(f\"Rank: {(rounded == 1).sum().item()}\")\n",
    "\n",
    "            # Save the mask\n",
    "            # torch.save(mask_data, f\"../masks/bigtom/{layer_idx}.pt\")\n",
    "\n",
    "            for bi, batch in tqdm(enumerate(valid_dataloader), total=len(valid_dataloader)):\n",
    "                alt_prompt = batch[\"alt_prompt\"]\n",
    "                org_prompt = batch[\"org_prompt\"]\n",
    "                alt_ans = batch[\"alt_ans\"]\n",
    "                target = batch[\"target\"][0]\n",
    "                batch_size = len(alt_ans)\n",
    "\n",
    "                alt_ques_start_idx = get_ques_start_token_idx(batch_size, model.tokenizer, alt_prompt, padding_side=\"left\")\n",
    "                alt_prompt_len = get_prompt_token_len(model.tokenizer, alt_prompt, padding_side=\"left\")\n",
    "                org_ques_start_idx = get_ques_start_token_idx(batch_size, model.tokenizer, org_prompt, padding_side=\"left\")\n",
    "                org_prompt_len = get_prompt_token_len(model.tokenizer, org_prompt, padding_side=\"left\")\n",
    "\n",
    "                with model.session() as session:\n",
    "                    alt_layer_out = defaultdict(dict)\n",
    "                    with model.trace(alt_prompt):\n",
    "                        for t_idx, t in enumerate([i for i in range(alt_ques_start_idx+3, alt_ques_start_idx+5)]):\n",
    "                            alt_layer_out[t_idx] = model.model.layers[layer_idx].output[0][0, t].save()\n",
    "\n",
    "                    with model.generate(org_prompt, max_new_tokens=2, do_sample=False, num_return_sequences=1, pad_token_id=model.tokenizer.pad_token_id, eos_token_id=model.tokenizer.eos_token_id):\n",
    "                        sing_vec = sing_vecs[layer_idx].cuda()\n",
    "                        masked_vec = sing_vec * rounded.unsqueeze(-1)\n",
    "                        proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "                        \n",
    "                        for t_idx, t in enumerate([i for i in range(org_ques_start_idx+3, org_ques_start_idx+5)]):\n",
    "                            curr_output = model.model.layers[layer_idx].output[0][0, t].clone()\n",
    "                            alt_proj = torch.matmul(alt_layer_out[t_idx], proj_matrix)\n",
    "                            org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "                            modified_out = curr_output - org_proj + alt_proj\n",
    "                            model.model.layers[layer_idx].output[0][0, t] = modified_out\n",
    "\n",
    "                        out = model.generator.output.save()\n",
    "\n",
    "                    del alt_layer_out\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                pred = model.tokenizer.decode(out[0][org_prompt_len:-1]).strip()\n",
    "                print(f\"Prediction: {pred} | Target: {target}\")\n",
    "                if pred.lower() in target.lower():\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "            print(f\"Validation accuracy: {correct / total:.2f} | Correct: {correct} | Total: {total}\\n\")\n",
    "            valid_accs[lamb][layer_idx] = round(correct / total, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visibility Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sing_vecs = defaultdict(dict)\n",
    "for l in range(model.config.num_hidden_layers):\n",
    "    sing_vecs[l] = torch.load(f\"../svd_results/BigToM/visibility_sent/singular_vecs/{l}.pt\").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 80\n",
    "valid_size = 20\n",
    "batch_size = 1\n",
    "\n",
    "dataset = get_bigtom_visibility_exps(df_false, df_true, train_size+valid_size)\n",
    "train_dataset = dataset[:train_size]\n",
    "valid_dataset = dataset[train_size:]\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\n",
      "\n",
      "Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor sees her coworker swapping the milk.\n",
      "Question: Does Noor believe the milk pitcher contains oat milk or almond milk?\n",
      "Answer: almond milk\n",
      "Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\n",
      "\n",
      "Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor does not see her coworker swapping the milk.\n",
      "Question: Does Noor believe the milk pitcher contains oat milk or almond milk?\n",
      "Answer: oat milk\n",
      "Target: almond milk\n"
     ]
    }
   ],
   "source": [
    "idx=0\n",
    "print(dataset[idx]['alt_prompt'], dataset[idx]['alt_ans'])\n",
    "print(dataset[idx]['org_prompt'], dataset[idx]['org_ans'])\n",
    "print(f\"Target: {dataset[idx]['target']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer: 1, lambda: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Task Loss: -24.2031, L1 Loss: 23.2500, Total Loss: -0.9531\n",
      "#Rank: 4640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4/80 [00:40<12:55, 10.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 4, Task Loss: -44.9375, L1 Loss: 15.6875, Total Loss: -10.3328\n",
      "#Rank: 4640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 8/80 [01:22<12:24, 10.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 8, Task Loss: -23.2500, L1 Loss: 12.0625, Total Loss: -12.8203\n",
      "#Rank: 2005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 12/80 [02:05<12:05, 10.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 12, Task Loss: -74.3125, L1 Loss: 8.1875, Total Loss: -16.7770\n",
      "#Rank: 1513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 16/80 [02:46<11:02, 10.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 16, Task Loss: -18.5469, L1 Loss: 6.1875, Total Loss: -19.1347\n",
      "#Rank: 1206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 20/80 [03:25<10:00, 10.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 20, Task Loss: -10.9766, L1 Loss: 5.0938, Total Loss: -21.5722\n",
      "#Rank: 1033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 24/80 [04:08<09:51, 10.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 24, Task Loss: -35.1250, L1 Loss: 4.4062, Total Loss: -22.2608\n",
      "#Rank: 901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 28/80 [04:50<09:11, 10.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 28, Task Loss: -61.4375, L1 Loss: 3.8594, Total Loss: -25.7022\n",
      "#Rank: 791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 32/80 [05:32<08:19, 10.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 32, Task Loss: -90.0000, L1 Loss: 3.4375, Total Loss: -28.9471\n",
      "#Rank: 686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 36/80 [06:15<07:48, 10.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 36, Task Loss: -32.9375, L1 Loss: 3.0625, Total Loss: -28.7096\n",
      "#Rank: 598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 40/80 [06:57<06:50, 10.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 40, Task Loss: -43.7188, L1 Loss: 2.7344, Total Loss: -31.4974\n",
      "#Rank: 535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 44/80 [07:39<06:09, 10.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 44, Task Loss: -42.6875, L1 Loss: 2.5469, Total Loss: -32.1181\n",
      "#Rank: 505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 48/80 [08:23<05:39, 10.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 48, Task Loss: -54.3750, L1 Loss: 2.3438, Total Loss: -33.1761\n",
      "#Rank: 471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 52/80 [09:05<04:55, 10.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 52, Task Loss: -74.1875, L1 Loss: 2.2031, Total Loss: -34.9528\n",
      "#Rank: 460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 56/80 [09:47<04:09, 10.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 56, Task Loss: -23.2812, L1 Loss: 2.0781, Total Loss: -34.6247\n",
      "#Rank: 424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 60/80 [10:31<03:36, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 60, Task Loss: -68.1250, L1 Loss: 1.9531, Total Loss: -35.5019\n",
      "#Rank: 394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 64/80 [11:13<02:51, 10.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 64, Task Loss: -41.1562, L1 Loss: 1.8516, Total Loss: -35.7427\n",
      "#Rank: 381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 68/80 [11:55<02:07, 10.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 68, Task Loss: -41.0938, L1 Loss: 1.7188, Total Loss: -35.9665\n",
      "#Rank: 350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 72/80 [12:36<01:22, 10.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 72, Task Loss: -55.5312, L1 Loss: 1.5703, Total Loss: -37.1371\n",
      "#Rank: 325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 76/80 [13:16<00:39,  9.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 76, Task Loss: -15.7109, L1 Loss: 1.4688, Total Loss: -35.7820\n",
      "#Rank: 284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [13:54<00:00, 10.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for layer: 1, lambda: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.tokenizer.padding_side = \"left\"\n",
    "\n",
    "valid_accs = defaultdict(dict)\n",
    "for layer_idx in range(1, 2, 1):\n",
    "\n",
    "    n_epochs = 1\n",
    "    lambs = [0.005]\n",
    "    for lamb in lambs:\n",
    "        modules = [i for i in range(sing_vecs[layer_idx].size(0))]\n",
    "        mask = torch.ones(len(modules), requires_grad=True, device=\"cuda\", dtype=torch.bfloat16)\n",
    "        optimizer = torch.optim.Adam([mask], lr=1e-1)\n",
    "\n",
    "        print(f\"Training layer: {layer_idx}, lambda: {lamb}\")\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0\n",
    "\n",
    "            for bi, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "                alt_prompt = batch[\"alt_prompt\"]\n",
    "                org_prompt = batch[\"org_prompt\"]\n",
    "                target = batch[\"target\"]\n",
    "                target_token = model.tokenizer(target, return_tensors=\"pt\", padding=True)\n",
    "                target_input_ids = target_token.input_ids[:, 1:]\n",
    "                batch_size = target_input_ids.size(0)\n",
    "\n",
    "                alt_ques_start_idx = get_ques_start_token_idx(batch_size, model.tokenizer, alt_prompt)\n",
    "                alt_vis_sent_start_idx = get_visitibility_sent_start_idx(model.tokenizer, alt_prompt)\n",
    "                alt_prompt_len = get_prompt_token_len(model.tokenizer, alt_prompt)\n",
    "                org_ques_start_idx = get_ques_start_token_idx(batch_size, model.tokenizer, org_prompt)\n",
    "                org_vis_sent_start_idx = get_visitibility_sent_start_idx(model.tokenizer, org_prompt)\n",
    "                org_prompt_len = get_prompt_token_len(model.tokenizer, org_prompt)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with model.trace() as tracer:\n",
    "                    alt_layer_out = defaultdict(dict)\n",
    "                    with tracer.invoke(alt_prompt):\n",
    "                        for t_idx, t in enumerate([i for i in range(alt_vis_sent_start_idx, alt_ques_start_idx)]):\n",
    "                            alt_layer_out[t_idx] = model.model.layers[layer_idx].output[0][0, t].save()\n",
    "\n",
    "                    with tracer.invoke(org_prompt):\n",
    "                        sing_vec = sing_vecs[layer_idx].cuda()\n",
    "                        masked_vec = sing_vec * mask.unsqueeze(-1)\n",
    "                        proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "\n",
    "                        for t_idx, t in enumerate([i for i in range(org_vis_sent_start_idx, org_vis_sent_start_idx+len(alt_layer_out))]):\n",
    "                            curr_output = model.model.layers[layer_idx].output[0][0, t].clone()\n",
    "                            alt_proj = torch.matmul(alt_layer_out[t_idx], proj_matrix)\n",
    "                            org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "                            modified_out = curr_output - org_proj + alt_proj\n",
    "                            model.model.layers[layer_idx].output[0][0, t] = modified_out\n",
    "\n",
    "                        logits = model.lm_head.output[0, -1].save()\n",
    "\n",
    "                        del sing_vec, proj_matrix, masked_vec\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                target_logit = logits[target_input_ids[0]].sum()\n",
    "\n",
    "                task_loss = -(target_logit/batch_size)\n",
    "                l1_loss = lamb * torch.norm(mask, p=1)\n",
    "                loss = task_loss + l1_loss.to(task_loss.device)\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                if bi % 4 == 0:\n",
    "                    mean_loss = epoch_loss / (bi + 1)\n",
    "                    print(f\"Epoch: {epoch}, Batch: {bi}, Task Loss: {task_loss.item():.4f}, \"\n",
    "                        f\"L1 Loss: {l1_loss.item():.4f}, Total Loss: {mean_loss:.4f}\")\n",
    "                    with torch.no_grad():\n",
    "                        mask.data.clamp_(0, 1)\n",
    "                        rounded = torch.round(mask)\n",
    "                        print(f\"#Rank: {(rounded == 1).sum().item()}\")\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    mask.data.clamp_(0, 1)\n",
    "\n",
    "        print(f\"Training finished for layer: {layer_idx}, lambda: {lamb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation started for layer: 1, lambda: 0.005\n",
      "Rank: 260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:06<02:03,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: shifted | Target: has shifted due to the earthquake\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:12<01:52,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Correct | Target: too hot for baking biscotti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:19<01:50,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: R | Target: damaged by the hailstorm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:26<01:46,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Strong | Target: that the fishing net has a large hole in it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:33<01:40,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Fresh | Target: the ingredients are nibbled on and no longer fresh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:40<01:35,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: W | Target: wilted and less ideal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:42<01:39,  7.09s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m org_vis_sent_start_idx \u001b[38;5;241m=\u001b[39m get_visitibility_sent_start_idx(model\u001b[38;5;241m.\u001b[39mtokenizer, org_prompt)\n\u001b[1;32m     22\u001b[0m org_prompt_len \u001b[38;5;241m=\u001b[39m get_prompt_token_len(model\u001b[38;5;241m.\u001b[39mtokenizer, org_prompt)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39msession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m     25\u001b[0m     alt_layer_out \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrace(alt_prompt):\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/intervention/contexts/session.py:37\u001b[0m, in \u001b[0;36mSession.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/tracing/contexts/tracer.py:25\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglobals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalTracingContext\n\u001b[1;32m     23\u001b[0m GlobalTracingContext\u001b[38;5;241m.\u001b[39mtry_deregister(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/tracing/contexts/base.py:82\u001b[0m, in \u001b[0;36mContext.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     78\u001b[0m graph \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mstack\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m     80\u001b[0m graph\u001b[38;5;241m.\u001b[39malive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/tracing/backends/base.py:25\u001b[0m, in \u001b[0;36mExecutionBackend.__call__\u001b[0;34m(self, graph)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph: Graph) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minjection:\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontexts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Context\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/tracing/graph/node.py:289\u001b[0m, in \u001b[0;36mNode.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget, Protocol):\n\u001b[0;32m--> 289\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m         \u001b[38;5;66;03m# Prepare arguments.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m         args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs))\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/tracing/contexts/base.py:93\u001b[0m, in \u001b[0;36mContext.execute\u001b[0;34m(cls, node)\u001b[0m\n\u001b[1;32m     90\u001b[0m graph: GraphType \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     92\u001b[0m graph\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 93\u001b[0m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m node\u001b[38;5;241m.\u001b[39mset_value(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/tracing/graph/graph.py:76\u001b[0m, in \u001b[0;36mGraph.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m         \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m NNsightError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     78\u001b[0m         err \u001b[38;5;241m=\u001b[39m (node\u001b[38;5;241m.\u001b[39mindex, e)\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/tracing/graph/node.py:289\u001b[0m, in \u001b[0;36mNode.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget, Protocol):\n\u001b[0;32m--> 289\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m         \u001b[38;5;66;03m# Prepare arguments.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m         args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs))\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/intervention/contexts/interleaving.py:159\u001b[0m, in \u001b[0;36mInterleavingTracer.execute\u001b[0;34m(cls, node)\u001b[0m\n\u001b[1;32m    155\u001b[0m graph\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m    157\u001b[0m interleaver \u001b[38;5;241m=\u001b[39m Interleaver(graph, batch_groups\u001b[38;5;241m=\u001b[39mbatch_groups)\n\u001b[0;32m--> 159\u001b[0m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterleaver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minvoker_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minvoker_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m graph\u001b[38;5;241m.\u001b[39mcleanup()\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/modeling/mixins/meta.py:51\u001b[0m, in \u001b[0;36mMetaMixin.interleave\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatched:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch()\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterleave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/intervention/base.py:342\u001b[0m, in \u001b[0;36mNNsight.interleave\u001b[0;34m(self, interleaver, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m interleaver\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mexecute()\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m interleaver:\n\u001b[0;32m--> 342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/modeling/language.py:297\u001b[0m, in \u001b[0;36mLanguageModel._execute\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: BatchEncoding, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    295\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1793\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1790\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1791\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1793\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1798\u001b[0m     ):\n\u001b[1;32m   1799\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:834\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    831\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    833\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 834\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    848\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    849\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1793\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1790\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1791\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1793\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1798\u001b[0m     ):\n\u001b[1;32m   1799\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:592\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    581\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    582\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m         position_embeddings,\n\u001b[1;32m    590\u001b[0m     )\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 592\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1793\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1790\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1791\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1793\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1798\u001b[0m     ):\n\u001b[1;32m   1799\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:335\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 335\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/torch/nn/modules/module.py:1793\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1790\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1791\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1793\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1798\u001b[0m     ):\n\u001b[1;32m   1799\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:274\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    273\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m--> 274\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position}\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:173\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m    171\u001b[0m sin \u001b[38;5;241m=\u001b[39m sin\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[1;32m    172\u001b[0m q_embed \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_half(q) \u001b[38;5;241m*\u001b[39m sin)\n\u001b[0;32m--> 173\u001b[0m k_embed \u001b[38;5;241m=\u001b[39m (k \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (\u001b[43mrotate_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_embed, k_embed\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:147\u001b[0m, in \u001b[0;36mrotate_half\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    145\u001b[0m x1 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, : x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    146\u001b[0m x2 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m :]\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"Validation started for layer: {layer_idx}, lambda: {lamb}\")\n",
    "correct, total = 0, 0\n",
    "\n",
    "with torch.inference_mode():\n",
    "    mask_data = mask.data.clone()\n",
    "    mask_data.clamp_(0, 1)\n",
    "    rounded = torch.round(mask)\n",
    "    print(f\"Rank: {(rounded == 1).sum().item()}\")\n",
    "\n",
    "    for bi, batch in tqdm(enumerate(valid_dataloader), total=len(valid_dataloader)):\n",
    "        alt_prompt = batch[\"alt_prompt\"]\n",
    "        org_prompt = batch[\"org_prompt\"]\n",
    "        alt_ans = batch[\"alt_ans\"]\n",
    "        target = batch[\"target\"][0]\n",
    "        batch_size = len(alt_ans)\n",
    "\n",
    "        alt_ques_start_idx = get_ques_start_token_idx(batch_size, model.tokenizer, alt_prompt)\n",
    "        alt_vis_sent_start_idx = get_visitibility_sent_start_idx(model.tokenizer, alt_prompt)\n",
    "        alt_prompt_len = get_prompt_token_len(model.tokenizer, alt_prompt)\n",
    "        org_ques_start_idx = get_ques_start_token_idx(batch_size, model.tokenizer, org_prompt)\n",
    "        org_vis_sent_start_idx = get_visitibility_sent_start_idx(model.tokenizer, org_prompt)\n",
    "        org_prompt_len = get_prompt_token_len(model.tokenizer, org_prompt)\n",
    "\n",
    "        with model.session() as session:\n",
    "            alt_layer_out = defaultdict(dict)\n",
    "            with model.trace(alt_prompt):\n",
    "                for t_idx, t in enumerate([i for i in range(alt_vis_sent_start_idx, alt_ques_start_idx)]):\n",
    "                    alt_layer_out[t_idx] = model.model.layers[layer_idx].output[0][0, t].save()\n",
    "\n",
    "            with model.generate(org_prompt, max_new_tokens=2, do_sample=False, num_return_sequences=1, pad_token_id=model.tokenizer.pad_token_id, eos_token_id=model.tokenizer.eos_token_id):\n",
    "                sing_vec = sing_vecs[layer_idx].cuda()\n",
    "                masked_vec = sing_vec * rounded.unsqueeze(-1)\n",
    "                proj_matrix = torch.matmul(masked_vec.t(), masked_vec).half()\n",
    "\n",
    "                for t_idx, t in enumerate([i for i in range(org_vis_sent_start_idx, org_vis_sent_start_idx+len(alt_layer_out))]):\n",
    "                    curr_output = model.model.layers[layer_idx].output[0][0, t].clone()\n",
    "                    alt_proj = torch.matmul(alt_layer_out[t_idx], proj_matrix)\n",
    "                    org_proj = torch.matmul(curr_output, proj_matrix)\n",
    "                    modified_out = curr_output - org_proj + alt_proj\n",
    "                    model.model.layers[layer_idx].output[0][0, t] = modified_out\n",
    "\n",
    "                out = model.generator.output.save()\n",
    "\n",
    "            del alt_layer_out\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        pred = model.tokenizer.decode(out[0][org_prompt_len:-1]).strip()\n",
    "        print(f\"Prediction: {pred} | Target: {target}\")\n",
    "        if pred.lower() in target.lower():\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    print(f\"Validation accuracy: {correct / total:.2f} | Correct: {correct} | Total: {total}\\n\")\n",
    "    valid_accs[lamb][layer_idx] = round(correct / total, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subspace Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subspace_similarity_cca(subspace1, subspace2):\n",
    "    \"\"\"\n",
    "    Compute subspace similarity using Canonical Correlation Analysis (CCA).\n",
    "    This works for subspaces of different dimensions.\n",
    "    \n",
    "    Args:\n",
    "        subspace1: First subspace basis vectors\n",
    "        subspace2: Second subspace basis vectors\n",
    "    \n",
    "    Returns:\n",
    "        Average canonical correlation (similarity measure between 0 and 1)\n",
    "    \"\"\"\n",
    "    # Ensure we work with 2D matrices\n",
    "    if len(subspace1.shape) == 1:\n",
    "        subspace1 = subspace1.unsqueeze(1)\n",
    "    if len(subspace2.shape) == 1:\n",
    "        subspace2 = subspace2.unsqueeze(1)\n",
    "    \n",
    "    # Get dimensions\n",
    "    dim = subspace1.shape[1]  # Feature dimension\n",
    "    \n",
    "    # Orthonormalize the bases\n",
    "    if subspace1.shape[0] > 1:  # Only if we have multiple vectors\n",
    "        subspace1, _ = torch.linalg.qr(subspace1.T)\n",
    "        subspace1 = subspace1.T\n",
    "    else:\n",
    "        subspace1 = subspace1 / torch.norm(subspace1, dim=1, keepdim=True)\n",
    "    \n",
    "    if subspace2.shape[0] > 1:  # Only if we have multiple vectors\n",
    "        subspace2, _ = torch.linalg.qr(subspace2.T)\n",
    "        subspace2 = subspace2.T\n",
    "    else:\n",
    "        subspace2 = subspace2 / torch.norm(subspace2, dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute correlation matrix\n",
    "    corr = subspace1 @ subspace2.T\n",
    "    \n",
    "    # Take average of absolute correlations as similarity measure\n",
    "    # This is a simplified CCA for efficiency\n",
    "    similarity = torch.mean(torch.abs(corr))\n",
    "    \n",
    "    return similarity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative simpler approach using projection matrix\n",
    "def compute_subspace_similarity_projection(subspace1, subspace2):\n",
    "    \"\"\"\n",
    "    Compute similarity between subspaces using projection overlap.\n",
    "    This works for subspaces of different dimensions.\n",
    "    \n",
    "    Args:\n",
    "        subspace1: First subspace basis vectors\n",
    "        subspace2: Second subspace basis vectors\n",
    "        \n",
    "    Returns:\n",
    "        Similarity measure between 0 and 1\n",
    "    \"\"\"\n",
    "    # Get feature dimension\n",
    "    feature_dim = subspace1.shape[1]\n",
    "    \n",
    "    # Convert to projection matrices in the common space\n",
    "    P1 = torch.zeros((feature_dim, feature_dim), dtype=subspace1.dtype, device=subspace1.device)\n",
    "    for vec in subspace1:\n",
    "        vec_normalized = vec / torch.norm(vec)\n",
    "        P1 += torch.outer(vec_normalized, vec_normalized)\n",
    "    \n",
    "    P2 = torch.zeros((feature_dim, feature_dim), dtype=subspace2.dtype, device=subspace2.device)\n",
    "    for vec in subspace2:\n",
    "        vec_normalized = vec / torch.norm(vec)\n",
    "        P2 += torch.outer(vec_normalized, vec_normalized)\n",
    "    \n",
    "    # Normalize by subspace dimensions\n",
    "    P1 = P1 / subspace1.shape[0]\n",
    "    P2 = P2 / subspace2.shape[0]\n",
    "    \n",
    "    # Compute Frobenius inner product of normalized projections\n",
    "    similarity = torch.sum(P1 * P2) / torch.sqrt(torch.sum(P1 * P1) * torch.sum(P2 * P2))\n",
    "    \n",
    "    return similarity.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "causaltom_svs, bigtom_svs = defaultdict(dict), defaultdict(dict)\n",
    "for l in range(model.config.num_hidden_layers):\n",
    "    causaltom_svs[l] = torch.load(f\"../svd_results/CausalToM/no_visibility/last_token/singular_vecs/{l}.pt\").cpu()\n",
    "\n",
    "for l in range(model.config.num_hidden_layers):\n",
    "    bigtom_svs[l] = torch.load(f\"../svd_results/BigToM/last_token/singular_vecs/{l}.pt\").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigtom_mask = defaultdict(dict)\n",
    "files = os.listdir(\"../masks/BigToM/answer\")\n",
    "for f in files:\n",
    "    layer_idx = int(f.split(\".\")[0])\n",
    "    bigtom_mask[layer_idx] = torch.round(torch.load(f\"../masks/BigToM/answer/{f}\").cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "causaltom_mask = defaultdict(dict)\n",
    "files = os.listdir(\"../masks/CausalToM/no_visibility/value_fetcher\")\n",
    "for f in files:\n",
    "    layer_idx = int(f.split(\".\")[0])\n",
    "    causaltom_mask[layer_idx] = json.load(open(f\"../masks/CausalToM/no_visibility/value_fetcher/{f}\"))['singular_vector_patching']['metadata']['mask']\n",
    "    causaltom_mask[layer_idx] = torch.tensor(causaltom_mask[layer_idx], dtype=torch.float16).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigtom_subspace, causaltom_subspace = defaultdict(dict), defaultdict(dict)\n",
    "# Filter singular vectors based on the mask\n",
    "for layer_idx in range(model.config.num_hidden_layers):\n",
    "    if layer_idx in bigtom_mask and layer_idx in causaltom_mask:\n",
    "        bigtom_subspace[layer_idx] = bigtom_svs[layer_idx][bigtom_mask[layer_idx] == 1]\n",
    "        causaltom_subspace[layer_idx] = causaltom_svs[layer_idx][causaltom_mask[layer_idx] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 40: bigtom shape=torch.Size([21, 8192]), causaltom shape=torch.Size([25, 8192])\n",
      "Layer 40 overlap: 0.0518\n",
      "Layer 50: bigtom shape=torch.Size([47, 8192]), causaltom shape=torch.Size([27, 8192])\n",
      "Layer 50 overlap: 0.0428\n",
      "Layer 60: bigtom shape=torch.Size([133, 8192]), causaltom shape=torch.Size([27, 8192])\n",
      "Layer 60 overlap: 0.0281\n",
      "Layer 61: bigtom shape=torch.Size([141, 8192]), causaltom shape=torch.Size([24, 8192])\n",
      "Layer 61 overlap: 0.0268\n",
      "Layer 62: bigtom shape=torch.Size([141, 8192]), causaltom shape=torch.Size([25, 8192])\n",
      "Layer 62 overlap: 0.0272\n",
      "Layer 63: bigtom shape=torch.Size([143, 8192]), causaltom shape=torch.Size([26, 8192])\n",
      "Layer 63 overlap: 0.0268\n",
      "Layer 64: bigtom shape=torch.Size([132, 8192]), causaltom shape=torch.Size([25, 8192])\n",
      "Layer 64 overlap: 0.0273\n",
      "Layer 65: bigtom shape=torch.Size([135, 8192]), causaltom shape=torch.Size([25, 8192])\n",
      "Layer 65 overlap: 0.0272\n",
      "Layer 66: bigtom shape=torch.Size([132, 8192]), causaltom shape=torch.Size([25, 8192])\n",
      "Layer 66 overlap: 0.0277\n",
      "Layer 67: bigtom shape=torch.Size([134, 8192]), causaltom shape=torch.Size([25, 8192])\n",
      "Layer 67 overlap: 0.0271\n",
      "Layer 68: bigtom shape=torch.Size([130, 8192]), causaltom shape=torch.Size([25, 8192])\n",
      "Layer 68 overlap: 0.0271\n",
      "Layer 69: bigtom shape=torch.Size([132, 8192]), causaltom shape=torch.Size([25, 8192])\n",
      "Layer 69 overlap: 0.0271\n",
      "Layer 70: bigtom shape=torch.Size([137, 8192]), causaltom shape=torch.Size([25, 8192])\n",
      "Layer 70 overlap: 0.0270\n",
      "Layer 71: bigtom shape=torch.Size([134, 8192]), causaltom shape=torch.Size([25, 8192])\n",
      "Layer 71 overlap: 0.0272\n",
      "Layer 72: bigtom shape=torch.Size([135, 8192]), causaltom shape=torch.Size([27, 8192])\n",
      "Layer 72 overlap: 0.0269\n",
      "Layer 73: bigtom shape=torch.Size([149, 8192]), causaltom shape=torch.Size([26, 8192])\n",
      "Layer 73 overlap: 0.0265\n",
      "Layer 74: bigtom shape=torch.Size([145, 8192]), causaltom shape=torch.Size([25, 8192])\n",
      "Layer 74 overlap: 0.0273\n",
      "Layer 75: bigtom shape=torch.Size([141, 8192]), causaltom shape=torch.Size([26, 8192])\n",
      "Layer 75 overlap: 0.0270\n",
      "Layer 76: bigtom shape=torch.Size([142, 8192]), causaltom shape=torch.Size([26, 8192])\n",
      "Layer 76 overlap: 0.0281\n",
      "Layer 77: bigtom shape=torch.Size([145, 8192]), causaltom shape=torch.Size([26, 8192])\n",
      "Layer 77 overlap: 0.0293\n",
      "Layer 78: bigtom shape=torch.Size([160, 8192]), causaltom shape=torch.Size([27, 8192])\n",
      "Layer 78 overlap: 0.0301\n",
      "Layer 79: bigtom shape=torch.Size([161, 8192]), causaltom shape=torch.Size([29, 8192])\n",
      "Layer 79 overlap: 0.0373\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare subspaces for each layer\n",
    "overlaps = {}\n",
    "for layer_idx in bigtom_subspace.keys():\n",
    "    if layer_idx in causaltom_subspace:\n",
    "        if bigtom_subspace[layer_idx].shape[0] > 0 and causaltom_subspace[layer_idx].shape[0] > 0:\n",
    "            try:\n",
    "                # Print shapes for debugging\n",
    "                # print(f\"Layer {layer_idx}: bigtom shape={bigtom_subspace[layer_idx].shape}, causaltom shape={causaltom_subspace[layer_idx].shape}\")\n",
    "                \n",
    "                overlaps[layer_idx] = compute_subspace_similarity_cca(\n",
    "                    bigtom_subspace[layer_idx], \n",
    "                    causaltom_subspace[layer_idx]\n",
    "                )\n",
    "                print(f\"Layer {layer_idx} overlap: {overlaps[layer_idx]:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in layer {layer_idx}: {e}\")\n",
    "\n",
    "# Visualize the results if we have any\n",
    "if overlaps:\n",
    "    import matplotlib.pyplot as plt\n",
    "    layers = sorted(overlaps.keys())\n",
    "    values = [overlaps[l] for l in layers]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(layers, values)\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('Subspace Similarity (CCA)')\n",
    "    plt.title('Similarity between BigToM and CausalToM Belief Subspaces')\n",
    "    plt.ylim(0, 1)  # Similarity is between 0 and 1\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid overlaps computed. Check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying alternative projection method:\n",
      "Layer 40 projection overlap: 0.1013\n",
      "Layer 50 projection overlap: 0.1088\n",
      "Layer 60 projection overlap: 0.0811\n",
      "Layer 61 projection overlap: 0.0712\n",
      "Layer 62 projection overlap: 0.0737\n",
      "Layer 63 projection overlap: 0.0727\n",
      "Layer 64 projection overlap: 0.0723\n",
      "Layer 65 projection overlap: 0.0741\n",
      "Layer 66 projection overlap: 0.0752\n",
      "Layer 67 projection overlap: 0.0717\n",
      "Layer 68 projection overlap: 0.0695\n",
      "Layer 69 projection overlap: 0.0715\n",
      "Layer 70 projection overlap: 0.0724\n",
      "Layer 71 projection overlap: 0.0723\n",
      "Layer 72 projection overlap: 0.0760\n",
      "Layer 73 projection overlap: 0.0741\n",
      "Layer 74 projection overlap: 0.0768\n",
      "Layer 75 projection overlap: 0.0751\n",
      "Layer 76 projection overlap: 0.0818\n",
      "Layer 77 projection overlap: 0.0890\n",
      "Layer 78 projection overlap: 0.1014\n",
      "Layer 79 projection overlap: 0.1735\n"
     ]
    }
   ],
   "source": [
    "# Try the alternative method\n",
    "print(\"\\nTrying alternative projection method:\")\n",
    "projection_overlaps = {}\n",
    "for layer_idx in bigtom_subspace.keys():\n",
    "    if layer_idx in causaltom_subspace:\n",
    "        if bigtom_subspace[layer_idx].shape[0] > 0 and causaltom_subspace[layer_idx].shape[0] > 0:\n",
    "            try:\n",
    "                projection_overlaps[layer_idx] = compute_subspace_similarity_projection(\n",
    "                    bigtom_subspace[layer_idx], \n",
    "                    causaltom_subspace[layer_idx]\n",
    "                )\n",
    "                print(f\"Layer {layer_idx} projection overlap: {projection_overlaps[layer_idx]:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in layer {layer_idx} (projection method): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "causaltom_svs, bigtom_svs = defaultdict(dict), defaultdict(dict)\n",
    "for l in range(model.config.num_hidden_layers):\n",
    "    causaltom_svs[l] = torch.load(f\"../svd_results/CausalToM/no_visibility/last_token/singular_vecs/{l}.pt\").cpu()\n",
    "\n",
    "for l in range(model.config.num_hidden_layers):\n",
    "    bigtom_svs[l] = torch.load(f\"../svd_results/BigToM/last_token/singular_vecs/{l}.pt\").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigtom_mask = defaultdict(dict)\n",
    "files = os.listdir(\"../masks/BigToM/answer_state\")\n",
    "for f in files:\n",
    "    layer_idx = int(f.split(\".\")[0])\n",
    "    bigtom_mask[layer_idx] = torch.round(torch.load(f\"../masks/BigToM/answer_state/{f}\").cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "causaltom_mask = defaultdict(dict)\n",
    "files = os.listdir(\"../masks/CausalToM/no_visibility/position_transmitter\")\n",
    "for f in files:\n",
    "    layer_idx = int(f.split(\".\")[0])\n",
    "    causaltom_mask[layer_idx] = json.load(open(f\"../masks/CausalToM/no_visibility/position_transmitter/{f}\"))['singular_vector_patching']['metadata']['mask']\n",
    "    causaltom_mask[layer_idx] = torch.tensor(causaltom_mask[layer_idx], dtype=torch.float16).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigtom_subspace, causaltom_subspace = defaultdict(dict), defaultdict(dict)\n",
    "# Filter singular vectors based on the mask\n",
    "for layer_idx in range(model.config.num_hidden_layers):\n",
    "    if layer_idx in bigtom_mask and layer_idx in causaltom_mask:\n",
    "        bigtom_subspace[layer_idx] = bigtom_svs[layer_idx][bigtom_mask[layer_idx] == 1]\n",
    "        causaltom_subspace[layer_idx] = causaltom_svs[layer_idx][causaltom_mask[layer_idx] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 20 overlap: 0.0558\n",
      "Layer 30 overlap: 0.0265\n",
      "Layer 31 overlap: 0.0292\n",
      "Layer 32 overlap: 0.0271\n",
      "Layer 33 overlap: 0.0266\n",
      "Layer 34 overlap: 0.0297\n",
      "Layer 35 overlap: 0.0270\n",
      "Layer 36 overlap: 0.0265\n",
      "Layer 37 overlap: 0.0269\n",
      "Layer 38 overlap: 0.0279\n",
      "Layer 39 overlap: 0.0279\n",
      "Layer 40 overlap: 0.0275\n",
      "Layer 41 overlap: 0.0264\n",
      "Layer 42 overlap: 0.0294\n",
      "Layer 43 overlap: 0.0299\n",
      "Layer 44 overlap: 0.0307\n",
      "Layer 45 overlap: 0.0305\n",
      "Layer 46 overlap: 0.0314\n",
      "Layer 47 overlap: 0.0283\n",
      "Layer 48 overlap: 0.0279\n",
      "Layer 49 overlap: 0.0274\n",
      "Layer 50 overlap: 0.0265\n",
      "Layer 60 overlap: 0.0214\n",
      "Layer 70 overlap: 0.0136\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare subspaces for each layer\n",
    "overlaps = {}\n",
    "for layer_idx in bigtom_subspace.keys():\n",
    "    if layer_idx in causaltom_subspace:\n",
    "        if bigtom_subspace[layer_idx].shape[0] > 0 and causaltom_subspace[layer_idx].shape[0] > 0:\n",
    "            try:\n",
    "                # Print shapes for debugging\n",
    "                # print(f\"Layer {layer_idx}: bigtom shape={bigtom_subspace[layer_idx].shape}, causaltom shape={causaltom_subspace[layer_idx].shape}\")\n",
    "                \n",
    "                overlaps[layer_idx] = compute_subspace_similarity_cca(\n",
    "                    bigtom_subspace[layer_idx], \n",
    "                    causaltom_subspace[layer_idx]\n",
    "                )\n",
    "                print(f\"Layer {layer_idx} overlap: {overlaps[layer_idx]:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in layer {layer_idx}: {e}\")\n",
    "\n",
    "# Visualize the results if we have any\n",
    "if overlaps:\n",
    "    import matplotlib.pyplot as plt\n",
    "    layers = sorted(overlaps.keys())\n",
    "    values = [overlaps[l] for l in layers]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(layers, values)\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('Subspace Similarity (CCA)')\n",
    "    plt.title('Similarity between BigToM and CausalToM Belief Subspaces')\n",
    "    plt.ylim(0, 1)  # Similarity is between 0 and 1\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid overlaps computed. Check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying alternative projection method:\n",
      "Layer 20 projection overlap: 0.0158\n"
     ]
    }
   ],
   "source": [
    "# Try the alternative method\n",
    "print(\"\\nTrying alternative projection method:\")\n",
    "projection_overlaps = {}\n",
    "for layer_idx in bigtom_subspace.keys():\n",
    "    if layer_idx in causaltom_subspace:\n",
    "        if bigtom_subspace[layer_idx].shape[0] > 0 and causaltom_subspace[layer_idx].shape[0] > 0:\n",
    "            try:\n",
    "                projection_overlaps[layer_idx] = compute_subspace_similarity_projection(\n",
    "                    bigtom_subspace[layer_idx], \n",
    "                    causaltom_subspace[layer_idx]\n",
    "                )\n",
    "                print(f\"Layer {layer_idx} projection overlap: {projection_overlaps[layer_idx]:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in layer {layer_idx} (projection method): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tomi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
