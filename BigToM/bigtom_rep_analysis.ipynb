{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import Any, List, Optional\n",
    "import nnsight\n",
    "from nnsight import CONFIG, LanguageModel\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from einops import einsum\n",
    "import time\n",
    "from einops import rearrange, reduce\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from src.dataset import SampleV3, DatasetV3, STORY_TEMPLATES\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "random.seed(10)\n",
    "\n",
    "CONFIG.set_default_api_key(\"d9e00ab7d4f74643b3176de0913f24a7\")\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "CONFIG.APP.REMOTE_LOGGING = False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards:  27%|██▋       | 8/30 [00:09<00:26,  1.19s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# model = LanguageModel(\"meta-llama/Meta-Llama-3.1-405B\")\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLanguageModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Meta-Llama-3-70B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/disk/u/nikhil/.cache/huggingface/hub\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/modeling/language.py:103\u001b[0m, in \u001b[0;36mLanguageModel.__init__\u001b[0;34m(self, config, tokenizer, automodel, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m tokenizer\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepo_id: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator: Envoy[InterventionProxyType, InterventionNodeType] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    106\u001b[0m     LanguageModel\u001b[38;5;241m.\u001b[39mGenerator()\n\u001b[1;32m    107\u001b[0m )\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/modeling/mixins/meta.py:22\u001b[0m, in \u001b[0;36mMetaMixin.__init__\u001b[0;34m(self, dispatch, meta_buffers, rename, *args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule) \u001b[38;5;129;01mor\u001b[39;00m dispatch:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m init_empty_weights(include_buffers\u001b[38;5;241m=\u001b[39mmeta_buffers):\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/modeling/mixins/loadable.py:14\u001b[0m, in \u001b[0;36mLoadableMixin.__init__\u001b[0;34m(self, rename, *args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, rename: Optional[Dict[\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m---> 14\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m         model \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/nnsight/modeling/language.py:175\u001b[0m, in \u001b[0;36mLanguageModel._load\u001b[0;34m(self, repo_id, tokenizer_kwargs, patch_llama_scan, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    168\u001b[0m     patch_llama_scan\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, LlamaConfig)\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrope_scaling, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrope_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrope_scaling\n\u001b[1;32m    172\u001b[0m ):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrope_scaling[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrope_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 175\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/modeling_utils.py:4245\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4235\u001b[0m         load_contexts\u001b[38;5;241m.\u001b[39mappend(tp_device)\n\u001b[1;32m   4237\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(load_contexts):\n\u001b[1;32m   4238\u001b[0m         (\n\u001b[1;32m   4239\u001b[0m             model,\n\u001b[1;32m   4240\u001b[0m             missing_keys,\n\u001b[1;32m   4241\u001b[0m             unexpected_keys,\n\u001b[1;32m   4242\u001b[0m             mismatched_keys,\n\u001b[1;32m   4243\u001b[0m             offload_index,\n\u001b[1;32m   4244\u001b[0m             error_msgs,\n\u001b[0;32m-> 4245\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4246\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4247\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4248\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4249\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4252\u001b[0m \u001b[43m            \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4253\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4254\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4255\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4256\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4257\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4258\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4259\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4260\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4261\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4262\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4263\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4265\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4266\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/modeling_utils.py:4815\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4813\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4814\u001b[0m         fixed_state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_fix_state_dict_keys_on_load(state_dict)\n\u001b[0;32m-> 4815\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4816\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4817\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfixed_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4818\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4819\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4820\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4821\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4822\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4823\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4824\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4825\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4826\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4827\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4828\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4829\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4830\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4831\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4832\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4833\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m/disk/u/nikhil/.conda/envs/tomi/lib/python3.10/site-packages/transformers/modeling_utils.py:751\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    747\u001b[0m                 new_val \u001b[38;5;241m=\u001b[39m new_val\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    748\u001b[0m             \u001b[38;5;28msetattr\u001b[39m(submodule, param_name, new_val)\n\u001b[0;32m--> 751\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_load_state_dict_into_meta_model\u001b[39m(\n\u001b[1;32m    752\u001b[0m     model,\n\u001b[1;32m    753\u001b[0m     state_dict,\n\u001b[1;32m    754\u001b[0m     start_prefix,\n\u001b[1;32m    755\u001b[0m     expected_keys,\n\u001b[1;32m    756\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    757\u001b[0m     offload_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    758\u001b[0m     offload_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    759\u001b[0m     state_dict_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    760\u001b[0m     state_dict_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    761\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    762\u001b[0m     hf_quantizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    763\u001b[0m     is_safetensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    764\u001b[0m     keep_in_fp32_modules\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    765\u001b[0m     unexpected_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# passing `unexpected` for cleanup from quantization items\u001b[39;00m\n\u001b[1;32m    766\u001b[0m     pretrained_model_name_or_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# for flagging the user when the model contains renamed keys\u001b[39;00m\n\u001b[1;32m    767\u001b[0m ):\n\u001b[1;32m    768\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;124;03m    This is somewhat similar to `_load_state_dict_into_model`, but deals with a model that has some or all of its\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;124;03m    params on a `meta` device. It replaces the model params with the data from the `state_dict`, while moving the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    775\u001b[0m \n\u001b[1;32m    776\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;66;03m# XXX: remaining features to implement to be fully compatible with _load_state_dict_into_model\u001b[39;00m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# - deepspeed zero 3 support\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# - need to copy metadata if any - see _load_state_dict_into_model\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# - handling error_msgs - mimicking the error handling in module._load_from_state_dict()\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model = LanguageModel(\"meta-llama/Meta-Llama-3.1-405B\")\n",
    "model = LanguageModel(\"meta-llama/Meta-Llama-3-70B-Instruct\", cache_dir=\"/disk/u/nikhil/.cache/huggingface/hub\", device_map=\"auto\", load_in_4bit=True, torch_dtype=torch.float16, dispatch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Singular Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cached_acts.pt using torch.load\n",
    "cached_acts = torch.load(\"../caches/Llama-70B-Instruct/BigToM/query_charac_acts.pt\")\n",
    "# visibility_sent_lens = torch.load(\"../caches/Llama-70B-Instruct/BigToM/visibility_sent_lens.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cached_acts[:, 0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:   0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:   1%|▏         | 1/80 [00:00<00:44,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:   2%|▎         | 2/80 [00:01<00:50,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:   4%|▍         | 3/80 [00:01<00:50,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:   5%|▌         | 4/80 [00:02<00:48,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:   6%|▋         | 5/80 [00:03<00:47,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:   8%|▊         | 6/80 [00:03<00:51,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:   9%|▉         | 7/80 [00:04<00:49,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  10%|█         | 8/80 [00:05<00:46,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  11%|█▏        | 9/80 [00:05<00:44,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  12%|█▎        | 10/80 [00:06<00:42,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  14%|█▍        | 11/80 [00:06<00:42,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  15%|█▌        | 12/80 [00:07<00:40,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  16%|█▋        | 13/80 [00:08<00:38,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  18%|█▊        | 14/80 [00:08<00:38,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  19%|█▉        | 15/80 [00:09<00:37,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  20%|██        | 16/80 [00:09<00:37,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  21%|██▏       | 17/80 [00:10<00:36,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  22%|██▎       | 18/80 [00:10<00:34,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  24%|██▍       | 19/80 [00:11<00:33,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  25%|██▌       | 20/80 [00:12<00:33,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  26%|██▋       | 21/80 [00:12<00:33,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  28%|██▊       | 22/80 [00:13<00:33,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  29%|██▉       | 23/80 [00:13<00:32,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  30%|███       | 24/80 [00:14<00:32,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  31%|███▏      | 25/80 [00:14<00:31,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  32%|███▎      | 26/80 [00:15<00:30,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  34%|███▍      | 27/80 [00:16<00:29,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  35%|███▌      | 28/80 [00:16<00:29,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  36%|███▋      | 29/80 [00:17<00:29,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  38%|███▊      | 30/80 [00:17<00:28,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  39%|███▉      | 31/80 [00:18<00:28,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  40%|████      | 32/80 [00:18<00:27,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  41%|████▏     | 33/80 [00:19<00:27,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  42%|████▎     | 34/80 [00:20<00:30,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  44%|████▍     | 35/80 [00:20<00:28,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  45%|████▌     | 36/80 [00:21<00:26,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  46%|████▋     | 37/80 [00:22<00:25,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  48%|████▊     | 38/80 [00:22<00:24,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  49%|████▉     | 39/80 [00:23<00:23,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  50%|█████     | 40/80 [00:23<00:22,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  51%|█████▏    | 41/80 [00:24<00:22,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  52%|█████▎    | 42/80 [00:24<00:21,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  54%|█████▍    | 43/80 [00:25<00:20,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  55%|█████▌    | 44/80 [00:26<00:20,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  56%|█████▋    | 45/80 [00:26<00:20,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  57%|█████▊    | 46/80 [00:27<00:19,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  59%|█████▉    | 47/80 [00:27<00:18,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  60%|██████    | 48/80 [00:28<00:17,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  61%|██████▏   | 49/80 [00:28<00:17,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  62%|██████▎   | 50/80 [00:29<00:16,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  64%|██████▍   | 51/80 [00:29<00:16,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  65%|██████▌   | 52/80 [00:30<00:15,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  66%|██████▋   | 53/80 [00:31<00:15,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  68%|██████▊   | 54/80 [00:31<00:14,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  69%|██████▉   | 55/80 [00:32<00:14,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  70%|███████   | 56/80 [00:32<00:13,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  71%|███████▏  | 57/80 [00:33<00:14,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  72%|███████▎  | 58/80 [00:34<00:13,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  74%|███████▍  | 59/80 [00:34<00:12,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  75%|███████▌  | 60/80 [00:35<00:11,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  76%|███████▋  | 61/80 [00:35<00:11,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  78%|███████▊  | 62/80 [00:36<00:10,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  79%|███████▉  | 63/80 [00:37<00:09,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  80%|████████  | 64/80 [00:37<00:09,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  81%|████████▏ | 65/80 [00:38<00:08,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  82%|████████▎ | 66/80 [00:38<00:08,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  84%|████████▍ | 67/80 [00:39<00:07,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  85%|████████▌ | 68/80 [00:39<00:06,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  86%|████████▋ | 69/80 [00:40<00:06,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  88%|████████▊ | 70/80 [00:41<00:05,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  89%|████████▉ | 71/80 [00:41<00:05,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  90%|█████████ | 72/80 [00:42<00:04,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  91%|█████████▏| 73/80 [00:42<00:03,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  92%|█████████▎| 74/80 [00:43<00:03,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  94%|█████████▍| 75/80 [00:43<00:02,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  95%|█████████▌| 76/80 [00:44<00:02,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  96%|█████████▋| 77/80 [00:45<00:01,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  98%|█████████▊| 78/80 [00:45<00:01,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  99%|█████████▉| 79/80 [00:46<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped acts: torch.Size([800, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers: 100%|██████████| 80/80 [00:46<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in tqdm(range(80), desc=\"Processing layers\"):\n",
    "    os.makedirs(f\"../svd_results/BigToM/query_charac_new/singular_values/\", exist_ok=True)\n",
    "    os.makedirs(f\"../svd_results/BigToM/query_charac_new/singular_vecs/\", exist_ok=True)\n",
    "\n",
    "    # Create boolean mask for valid tokens\n",
    "    # batch_size = cached_acts.size(0)\n",
    "    # max_tokens = cached_acts.size(2)\n",
    "    # mask = torch.arange(max_tokens).unsqueeze(0) < visibility_sent_lens.unsqueeze(1)\n",
    "    # mask = mask.to(cached_acts.device)\n",
    "\n",
    "    # Use boolean indexing to get all valid activations at once\n",
    "    # acts = cached_acts[:, layer][mask]  # This will automatically flatten the valid activations\n",
    "\n",
    "    if len(cached_acts[:, layer].shape) > 2:\n",
    "        acts = cached_acts[:, layer].reshape(-1, cached_acts.size(-1))\n",
    "        print(f\"Reshaped acts: {acts.shape}\")\n",
    "    else:\n",
    "        acts = cached_acts[:, layer]\n",
    "\n",
    "    _, singular_values, Vh = torch.linalg.svd(acts, full_matrices=False)\n",
    "    torch.save(singular_values, f\"../svd_results/BigToM/query_charac_new/singular_values/{layer}.pt\")\n",
    "    torch.save(Vh, f\"../svd_results/BigToM/query_charac_new/singular_vecs/{layer}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "batch_size = 1\n",
    "\n",
    "configs = []\n",
    "for _ in range(n_samples):\n",
    "    template_1 = STORY_TEMPLATES['templates'][0]\n",
    "    template_2 = STORY_TEMPLATES['templates'][1]\n",
    "    characters = random.sample(all_characters, 2)\n",
    "    containers = random.sample(all_containers[template_1[\"container_type\"]], 2)\n",
    "    states = random.sample(all_states[template_1[\"state_type\"]], 2)\n",
    "    event_idx = None\n",
    "    event_noticed = False\n",
    "    visibility = random.choice([True, False])\n",
    "\n",
    "    sample = SampleV3(\n",
    "        template=template_2 if visibility else template_1,\n",
    "        characters=characters,\n",
    "        containers=containers,\n",
    "        states=states,\n",
    "        visibility=visibility,\n",
    "        event_idx=event_idx,\n",
    "        event_noticed=event_noticed,\n",
    "    )\n",
    "    configs.append(sample)\n",
    "\n",
    "dataset = DatasetV3(configs)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "sample = dataset[idx]\n",
    "print(sample['prompt'], sample['target'], sample['visibility'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts_charac = torch.empty(n_samples, model.config.num_hidden_layers, model.config.hidden_size)\n",
    "acts_obj = torch.empty(n_samples, model.config.num_hidden_layers, model.config.hidden_size)\n",
    "character_indices, object_indices, visibility = [], [], []\n",
    "\n",
    "for bi, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "    prompt = data['prompt'][0]\n",
    "    character_idx = data['character_idx'][0]\n",
    "    character_indices.append(character_idx)\n",
    "    object_indices.append(data['object_idx'][0])\n",
    "    visibility.append(data['visibility'][0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        with model.trace() as tracer:\n",
    "\n",
    "            with tracer.invoke(prompt):\n",
    "                for l in range(model.config.num_hidden_layers):\n",
    "                    acts_charac[bi, l] = model.model.layers[l].output[0][0, -8].cpu().save()\n",
    "                    acts_obj[bi, l] = model.model.layers[l].output[0][0, -5].cpu().save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection & Visualization onto Singular Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Character Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_acts = {}\n",
    "for l in range(0, 40, 2):\n",
    "    V = torch.load(f\"../svd_results/charac_pos/Vh_{l}.pt\")\n",
    "    acts_l = acts_charac[:, l, :].cuda()\n",
    "    projected_acts[l] = torch.matmul(acts_l, V[1:2, :].t()).cpu().numpy()\n",
    "\n",
    "    del acts_l, V\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Create a plot with 10 subplots, each subplot showing the projection of activations of layer l onto the first singular vector.\n",
    "fig, axs = plt.subplots(5, 4, figsize=(12, 12))\n",
    "for l in range(0, 40, 2):\n",
    "    i = l//2\n",
    "\n",
    "    for j, idx in enumerate(character_indices):\n",
    "        axs[i // 4, i % 4].scatter(\n",
    "            projected_acts[l][j][0], \n",
    "            np.zeros_like(projected_acts[l][j][0]), \n",
    "            color='r' if idx == 0 else 'b', \n",
    "            alpha=0.6\n",
    "        )\n",
    "    \n",
    "    axs[i // 4, i % 4].set_title(f\"Layer {l}\")\n",
    "    axs[i // 4, i % 4].set_yticks([])\n",
    "    axs[i // 4, i % 4].set_xlabel(\"Projection values\")\n",
    "\n",
    "# Create a custom legend\n",
    "custom_legend = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='r', markersize=8, label=\"First Character\"),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='b', markersize=8, label=\"Second Character\"),\n",
    "]\n",
    "fig.legend(handles=custom_legend, loc='upper center', ncol=2, bbox_to_anchor=(0.5, 0.95))\n",
    "\n",
    "plt.suptitle(\"Projection of activations onto the Third singular vector\")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit the legend\n",
    "plt.show()\n",
    "# plt.savefig(\"../plots/rep_viz/third_SV.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Object Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_acts = {}\n",
    "for l in range(0, 40, 2):\n",
    "    V = torch.load(f\"../svd_results/obj_pos/Vh_{l}.pt\")\n",
    "    acts_l = acts_obj[:, l, :].cuda()\n",
    "    projected_acts[l] = torch.matmul(acts_l, V[2:4, :].t()).cpu().numpy()\n",
    "\n",
    "    del acts_l, V\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Create a plot with 10 subplots, each subplot showing the projection of activations of layer l onto the first singular vector.\n",
    "fig, axs = plt.subplots(5, 4, figsize=(12, 12))\n",
    "for l in range(0, 40, 2):\n",
    "    i = l//2\n",
    "\n",
    "    for j, idx in enumerate(object_indices):\n",
    "        axs[i // 4, i % 4].scatter(\n",
    "            projected_acts[l][j][0], \n",
    "            projected_acts[l][j][1], \n",
    "            color='r' if idx == 0 else 'b', \n",
    "            alpha=0.6\n",
    "        )\n",
    "    \n",
    "    axs[i // 4, i % 4].set_title(f\"Layer {l}\")\n",
    "    axs[i // 4, i % 4].set_yticks([])\n",
    "    axs[i // 4, i % 4].set_xlabel(\"Projection values\")\n",
    "\n",
    "# Create a custom legend\n",
    "custom_legend = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='r', markersize=8, label=\"First Object\"),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='b', markersize=8, label=\"Second Object\"),\n",
    "]\n",
    "fig.legend(handles=custom_legend, loc='upper center', ncol=2, bbox_to_anchor=(0.5, 0.95))\n",
    "\n",
    "plt.suptitle(\"Projection of activations onto the Third and Forth singular vectors\")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit the legend\n",
    "plt.show()\n",
    "# plt.savefig(\"../plots/rep_viz/obj_pos_SV_3_4.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visibility Viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Charac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_acts = {}\n",
    "for l in range(0, 40, 2):\n",
    "    V = torch.load(f\"../svd_results/selected_tokens/singular_vecs/{l}.pt\")\n",
    "    acts_l = acts_charac[:, l, :].cuda()\n",
    "    projected_acts[l] = torch.matmul(acts_l, V[3:4, :].t()).cpu().numpy()\n",
    "\n",
    "    del acts_l\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Create a plot with 10 subplots, each subplot showing the projection of activations of layer l onto the first singular vector.\n",
    "fig, axs = plt.subplots(5, 4, figsize=(12, 12))\n",
    "for l in range(0, 40, 2):\n",
    "    i = l//2\n",
    "\n",
    "    for j, idx in enumerate(visibility):\n",
    "        axs[i // 4, i % 4].scatter(\n",
    "            projected_acts[l][j][0], \n",
    "            np.zeros_like(projected_acts[l][j][0]), \n",
    "            color='r' if idx == 0 else 'b', \n",
    "            alpha=0.6\n",
    "        )\n",
    "    \n",
    "    axs[i // 4, i % 4].set_title(f\"Layer {l}\")\n",
    "    axs[i // 4, i % 4].set_yticks([])\n",
    "    axs[i // 4, i % 4].set_xlabel(\"Projection values\")\n",
    "\n",
    "# Create a custom legend\n",
    "custom_legend = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='r', markersize=8, label=\"Non Visible\"),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='b', markersize=8, label=\"Visible\"),\n",
    "]\n",
    "fig.legend(handles=custom_legend, loc='upper center', ncol=2, bbox_to_anchor=(0.5, 0.95))\n",
    "\n",
    "plt.suptitle(\"Projection of activations onto the third singular vector\")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit the legend\n",
    "plt.show()\n",
    "# plt.savefig(\"../plots/rep_viz/third_SV.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_acts = {}\n",
    "for l in range(0, 40, 2):\n",
    "    V = torch.load(f\"../svd_results/last_token/Vh_{l}.pt\")\n",
    "    acts_l = acts_charac[:, l, :].cuda()\n",
    "    projected_acts[l] = torch.matmul(acts_l, V[1:2, :].t()).cpu().numpy()\n",
    "\n",
    "    del acts_l\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Create a plot with 10 subplots, each subplot showing the projection of activations of layer l onto the first singular vector.\n",
    "fig, axs = plt.subplots(5, 4, figsize=(12, 12))\n",
    "for l in range(0, 40, 2):\n",
    "    i = l//2\n",
    "\n",
    "    for j, idx in enumerate(visibility):\n",
    "        axs[i // 4, i % 4].scatter(\n",
    "            projected_acts[l][j][0], \n",
    "            np.zeros_like(projected_acts[l][j][0]), \n",
    "            color='r' if idx == 0 else 'b', \n",
    "            alpha=0.6\n",
    "        )\n",
    "    \n",
    "    axs[i // 4, i % 4].set_title(f\"Layer {l}\")\n",
    "    axs[i // 4, i % 4].set_yticks([])\n",
    "    axs[i // 4, i % 4].set_xlabel(\"Projection values\")\n",
    "\n",
    "# Create a custom legend\n",
    "custom_legend = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='r', markersize=8, label=\"Non Visible\"),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='b', markersize=8, label=\"Visible\"),\n",
    "]\n",
    "fig.legend(handles=custom_legend, loc='upper center', ncol=2, bbox_to_anchor=(0.5, 0.95))\n",
    "\n",
    "plt.suptitle(\"Projection of activations onto the third singular vector\")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit the legend\n",
    "plt.show()\n",
    "# plt.savefig(\"../plots/rep_viz/third_SV.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Intervention for Character Position Info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "batch_size = 1\n",
    "\n",
    "dataset = query_charac_pos(STORY_TEMPLATES,\n",
    "                             all_characters,\n",
    "                             all_containers,\n",
    "                             all_states,\n",
    "                             n_samples,\n",
    "                             question_type=\"belief_question\",\n",
    "                             diff_visibility=False)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "print(dataset[idx]['corrupt_prompt'], dataset[idx]['corrupt_ans'])\n",
    "print(dataset[idx]['clean_prompt'], dataset[idx]['clean_ans'])\n",
    "print(f\"Target: {dataset[idx]['target']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, total = 0, 0\n",
    "errors = []\n",
    "with torch.no_grad():\n",
    "    for bi, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        clean_prompt = batch['clean_prompt'][0]\n",
    "        corrupt_prompt = batch['corrupt_prompt'][0]\n",
    "        clean_target = batch['clean_ans'][0]\n",
    "        corrupt_target = batch['corrupt_ans'][0]\n",
    "\n",
    "        with model.trace(clean_prompt):\n",
    "            clean_pred = model.lm_head.output[0, -1].argmax(dim=-1).item().save()\n",
    "\n",
    "        with model.trace(corrupt_prompt):\n",
    "            corrupt_pred = model.lm_head.output[0, -1].argmax(dim=-1).item().save()\n",
    "\n",
    "        print(f\"Clean: {model.tokenizer.decode([clean_pred]).lower().strip()} | Corrupt: {model.tokenizer.decode([corrupt_pred]).lower().strip()}\")\n",
    "        if model.tokenizer.decode([clean_pred]).lower().strip() == clean_target and model.tokenizer.decode([corrupt_pred]).lower().strip() == corrupt_target:\n",
    "            correct += 1\n",
    "        else:\n",
    "            errors.append(bi)\n",
    "        total += 1\n",
    "       \n",
    "        del clean_pred, corrupt_pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Accuracy: {round(correct / total, 2)}\")\n",
    "print(f\"correct: {correct} | total: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patching with Singular Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singular_vecs = {}\n",
    "for l in range(41):\n",
    "    singular_vecs[l] = torch.load(f\"../svd_results/charac_pos/Vh_{l}.pt\").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accs_query_charac_1_second_sv = {}\n",
    "\n",
    "for layer_idx in range(12, 20, 2):\n",
    "    correct, total = 0, 0\n",
    "    for bi, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        if bi in errors:\n",
    "            continue\n",
    "        corrupt_prompt = batch[\"corrupt_prompt\"][0]\n",
    "        clean_prompt = batch[\"clean_prompt\"][0]\n",
    "        target = batch[\"target\"][0]\n",
    "\n",
    "        corrupt_layer_out, clean_layer_out = defaultdict(dict), defaultdict(dict)\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            with model.trace() as tracer:\n",
    "                with tracer.invoke(corrupt_prompt):\n",
    "                    corrupt_layer_out = model.model.layers[layer_idx].output[0][0, -8].save()\n",
    "\n",
    "                with tracer.invoke(clean_prompt):\n",
    "                    vec = singular_vecs[layer_idx][1:2, :].t().half().cuda()\n",
    "                    # Calculate a projection matrix using the outer product of the singular vector\n",
    "                    proj_matrix = torch.matmul(vec, vec.t())\n",
    "\n",
    "                    corrupt_pos = torch.matmul(corrupt_layer_out, proj_matrix.T)\n",
    "                    clean_pos = torch.matmul(model.model.layers[layer_idx].output[0][0, -8], proj_matrix.T)\n",
    "\n",
    "                    model.model.layers[layer_idx].output[0][0, -8] = (model.model.layers[layer_idx].output[0][0, -8] - clean_pos) + corrupt_pos\n",
    "\n",
    "                    del vec, proj_matrix\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                    pred = model.lm_head.output[0, -1].argmax(dim=-1).save()\n",
    "\n",
    "            # print(f\"Pred: {model.tokenizer.decode([pred]).lower().strip()} | Target: {target}\")\n",
    "            if model.tokenizer.decode([pred]).lower().strip() == target:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "            del corrupt_layer_out, pred\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    acc = round(correct / total, 2)\n",
    "    accs_query_charac_1_second_sv[layer_idx] = acc\n",
    "    print(f\"Layer: {layer_idx} | Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_query_charac_1_second_sv = dict(sorted(accs_query_charac_1_second_sv.items(), key=lambda x:x[0]))\n",
    "accs_query_charac_1_second_sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort accs_query_charac_second_sv by key\n",
    "accs_query_charac_second_sv = dict(sorted(accs_query_charac_second_sv.items(), key=lambda x: x[0]))\n",
    "accs_query_charac_second_sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_stories = [\n",
    "    {\n",
    "        \"story\": dataset[0][\"corrupt_story\"],\n",
    "        \"question\": dataset[0][\"corrupt_question\"],\n",
    "        \"answer\": dataset[0][\"corrupt_ans\"],\n",
    "    },\n",
    "    {\n",
    "        \"story\": dataset[0][\"clean_story\"],\n",
    "        \"question\": dataset[0][\"clean_question\"],\n",
    "        \"answer\": dataset[0][\"clean_ans\"],\n",
    "    }\n",
    "]\n",
    "\n",
    "arrows = [{'start': token_pos_coords['e1_query_charac'], 'end': token_pos_coords['e2_query_charac'], 'color': 'red'}]\n",
    "\n",
    "plot_data = {\n",
    "    \"labels\": accs_query_charac_second_sv.keys(),\n",
    "    \"acc_one_layer\": accs_query_charac_second_sv.values(),\n",
    "    \"title\": \"Aligning Query Character Position info\",\n",
    "    \"x_label\": \"Layers\",\n",
    "    \"y_label\": \"Intervention Accuracy\",\n",
    "}\n",
    "\n",
    "characters = list(set(dataset[0]['clean_characters'] + dataset[0]['corrupt_characters']))\n",
    "objects = list(set(dataset[0]['clean_objects'] + dataset[0]['corrupt_objects']))\n",
    "states = list(set(dataset[0]['clean_states'] + dataset[0]['corrupt_states']))\n",
    "\n",
    "generator = StoryGenerator(characters=characters, objects=objects, states=states, stories=true_stories, target=dataset[0]['target'], arrows=arrows, plot_data=plot_data)\n",
    "generator.save_html(filename=\"../plots/belief_exps/second_obj/query_charac_2_sv_2.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Intervention for Object Position Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "batch_size = 1\n",
    "\n",
    "dataset = query_obj_pos(STORY_TEMPLATES,\n",
    "                             all_characters,\n",
    "                             all_containers,\n",
    "                             all_states,\n",
    "                             n_samples,\n",
    "                             question_type=\"belief_question\",\n",
    "                             diff_visibility=False)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "print(dataset[idx]['corrupt_prompt'], dataset[idx]['corrupt_ans'])\n",
    "print(dataset[idx]['clean_prompt'], dataset[idx]['clean_ans'])\n",
    "print(f\"Target: {dataset[idx]['target']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, total = 0, 0\n",
    "errors = []\n",
    "with torch.no_grad():\n",
    "    for bi, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        clean_prompt = batch['clean_prompt'][0]\n",
    "        corrupt_prompt = batch['corrupt_prompt'][0]\n",
    "        clean_target = batch['clean_ans'][0]\n",
    "        corrupt_target = batch['corrupt_ans'][0]\n",
    "\n",
    "        with model.trace(clean_prompt):\n",
    "            clean_pred = model.lm_head.output[0, -1].argmax(dim=-1).item().save()\n",
    "\n",
    "        with model.trace(corrupt_prompt):\n",
    "            corrupt_pred = model.lm_head.output[0, -1].argmax(dim=-1).item().save()\n",
    "\n",
    "        print(f\"Clean: {model.tokenizer.decode([clean_pred]).lower().strip()} | Corrupt: {model.tokenizer.decode([corrupt_pred]).lower().strip()}\")\n",
    "        if model.tokenizer.decode([clean_pred]).lower().strip() == clean_target and model.tokenizer.decode([corrupt_pred]).lower().strip() == corrupt_target:\n",
    "            correct += 1\n",
    "        else:\n",
    "            errors.append(bi)\n",
    "        total += 1\n",
    "       \n",
    "        del clean_pred, corrupt_pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Accuracy: {round(correct / total, 2)}\")\n",
    "print(f\"correct: {correct} | total: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patching with Singular Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singular_vecs = {}\n",
    "for l in range(41):\n",
    "    singular_vecs[l] = torch.load(f\"../svd_results/obj_pos/Vh_{l}.pt\").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accs_query_obj_2_sv_2 = {}\n",
    "\n",
    "for layer_idx in range(0, 10, 10):\n",
    "    correct, total = 0, 0\n",
    "    for bi, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # if bi in errors:\n",
    "        #     continue\n",
    "        corrupt_prompt = batch[\"corrupt_prompt\"][0]\n",
    "        clean_prompt = batch[\"clean_prompt\"][0]\n",
    "        target = batch[\"target\"][0]\n",
    "\n",
    "        corrupt_layer_out, clean_layer_out = defaultdict(dict), defaultdict(dict)\n",
    "        with torch.no_grad():\n",
    "\n",
    "            with model.trace() as tracer:\n",
    "                with tracer.invoke(corrupt_prompt):\n",
    "                    for l in range(40):\n",
    "                        corrupt_layer_out[l] = model.model.layers[l].output[0][0, -5].save()\n",
    "\n",
    "                with tracer.invoke(clean_prompt):\n",
    "                    for l in range(40):\n",
    "                        vec = singular_vecs[l][2:4, :].t().half().cuda()\n",
    "                        # Calculate a projection matrix using the outer product of the singular vector\n",
    "                        proj_matrix = torch.matmul(vec, vec.t())\n",
    "\n",
    "                        corrupt_pos = torch.matmul(corrupt_layer_out[l], proj_matrix.T)\n",
    "                        clean_pos = torch.matmul(model.model.layers[l].output[0][0, -5], proj_matrix.T)\n",
    "\n",
    "                        # Find cosine similarity between the clean and corrupt position\n",
    "                        cos_sim = torch.nn.functional.cosine_similarity(clean_pos, corrupt_pos, dim=0)\n",
    "                        tracer.log(f\"cosine_similarity_{l}\", cos_sim.item())\n",
    "\n",
    "                        model.model.layers[l].output[0][0, -5] = (model.model.layers[l].output[0][0, -5] - clean_pos) + corrupt_pos\n",
    "\n",
    "                        del vec, proj_matrix\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                    pred = model.lm_head.output[0, -1].argmax(dim=-1).save()\n",
    "\n",
    "            print(f\"Pred: {model.tokenizer.decode([pred]).lower().strip()} | Target: {target}\")\n",
    "            if model.tokenizer.decode([pred]).lower().strip() == target:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "            del corrupt_layer_out, pred\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    acc = round(correct / total, 2)\n",
    "    # accs_query_obj_2_sv_2[layer_idx] = acc\n",
    "    print(f\"Layer: {layer_idx} | Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_query_obj_2_sv_2_3_from = dict(sorted(accs_query_obj_2_sv_2_3_from.items()))\n",
    "accs_query_obj_2_sv_2_3_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_query_obj_2_sv_2_3 = dict(sorted(accs_query_obj_2_sv_2_3.items()))\n",
    "accs_query_obj_2_sv_2_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_query_obj_sv_3_4 = dict(sorted(accs_query_obj_sv_3_4.items(), key=lambda x: x[0]))\n",
    "accs_query_obj_sv_3_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_stories = [\n",
    "    {\n",
    "        \"story\": dataset[0][\"corrupt_story\"],\n",
    "        \"question\": dataset[0][\"corrupt_question\"],\n",
    "        \"answer\": dataset[0][\"corrupt_ans\"],\n",
    "    },\n",
    "    {\n",
    "        \"story\": dataset[0][\"clean_story\"],\n",
    "        \"question\": dataset[0][\"clean_question\"],\n",
    "        \"answer\": dataset[0][\"clean_ans\"],\n",
    "    }\n",
    "]\n",
    "\n",
    "arrows = [{'start': token_pos_coords['e1_query_obj_belief'], 'end': token_pos_coords['e2_query_obj_belief'], 'color': 'red'}]\n",
    "\n",
    "plot_data = {\n",
    "    \"labels\": accs_query_obj_2_sv_2_3.keys(),\n",
    "    \"acc_upto_layer\": accs_query_obj_2_sv_2_3.values(),\n",
    "    \"acc_from_layer\": accs_query_obj_2_sv_2_3_from.values(),\n",
    "    \"title\": \"Aligning Query Object Position info\",\n",
    "    \"x_label\": \"Layers\",\n",
    "    \"y_label\": \"Intervention Accuracy\",\n",
    "}\n",
    "\n",
    "characters = list(set(dataset[0]['clean_characters'] + dataset[0]['corrupt_characters']))\n",
    "objects = list(set(dataset[0]['clean_objects'] + dataset[0]['corrupt_objects']))\n",
    "states = list(set(dataset[0]['clean_states'] + dataset[0]['corrupt_states']))\n",
    "\n",
    "generator = StoryGenerator(characters=characters, objects=objects, states=states, stories=true_stories, target=dataset[0]['target'], arrows=arrows, plot_data=plot_data)\n",
    "generator.save_html(filename=\"../plots/belief_exps/second_obj/query_obj_2_sv_1_2.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Intervention for Visibility Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "batch_size = 1\n",
    "\n",
    "dataset = get_visibility_align_exps(STORY_TEMPLATES,\n",
    "                             all_characters,\n",
    "                             all_containers,\n",
    "                             all_states,\n",
    "                             n_samples,\n",
    "                             question_type=\"belief_question\",\n",
    "                             diff_visibility=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "print(dataset[idx]['corrupt_prompt'], dataset[idx]['corrupt_ans'])\n",
    "print(dataset[idx]['clean_prompt'], dataset[idx]['clean_ans'])\n",
    "print(f\"Target: {dataset[idx]['target']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, total = 0, 0\n",
    "errors = []\n",
    "for bi, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "    clean_prompt = batch['clean_prompt'][0]\n",
    "    corrupt_prompt = batch['corrupt_prompt'][0]\n",
    "    clean_target = batch['clean_ans'][0]\n",
    "    corrupt_target = batch['corrupt_ans'][0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        with model.trace() as tracer:\n",
    "\n",
    "            with tracer.invoke(clean_prompt):\n",
    "                clean_pred = model.lm_head.output[0, -1].argmax(dim=-1).item().save()\n",
    "\n",
    "            with tracer.invoke(corrupt_prompt):\n",
    "                corrupt_pred = model.lm_head.output[0, -1].argmax(dim=-1).item().save()\n",
    "\n",
    "    print(f\"Clean: {model.tokenizer.decode([clean_pred]).lower().strip()} | Corrupt: {model.tokenizer.decode([corrupt_pred]).lower().strip()}\")\n",
    "    if model.tokenizer.decode([clean_pred]).lower().strip() == clean_target and model.tokenizer.decode([corrupt_pred]).lower().strip() == corrupt_target:\n",
    "        correct += 1\n",
    "    else:\n",
    "        errors.append(bi)\n",
    "    total += 1\n",
    "\n",
    "    del clean_pred, corrupt_pred\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Accuracy: {round(correct / total, 2)}\")\n",
    "print(f\"correct: {correct} | total: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patching with Singular Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sing_vecs = defaultdict(dict)\n",
    "for l in range(41):\n",
    "    sing_vecs[l] = torch.load(f\"../svd_results/selected_tokens_diff/singular_vecs/{l}.pt\").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_indices = {\n",
    "    -8: 0,\n",
    "    -7: 1,\n",
    "    -6: 2,\n",
    "    -5: 3,\n",
    "    -4: 4,\n",
    "    -3: 5,\n",
    "    -2: 6,\n",
    "    -1: 7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visibility_diff_reverse_sv = {}\n",
    "\n",
    "for layer_idx in range(32, 34, 2):\n",
    "    correct, total = 0, 0\n",
    "    for bi, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # if bi in errors:\n",
    "        #     continue\n",
    "        corrupt_prompt = batch[\"clean_prompt\"][0]\n",
    "        clean_prompt = batch[\"corrupt_prompt\"][0]\n",
    "        target = batch[\"target\"][0]\n",
    "\n",
    "        corrupt_layer_out, clean_layer_out = defaultdict(dict), defaultdict(dict)\n",
    "        with torch.no_grad():\n",
    "\n",
    "            with model.trace() as tracer:\n",
    "\n",
    "                with tracer.invoke(corrupt_prompt):\n",
    "                    for t in [-8, -7, -5, -3, -1]:\n",
    "                        corrupt_layer_out[t] = model.model.layers[layer_idx].output[0][0, t].save()\n",
    "\n",
    "                with tracer.invoke(clean_prompt):\n",
    "                    for t in [-8, -7, -5, -3, -1]:\n",
    "                        vec = sing_vecs[layer_idx][:6, :].t().half()\n",
    "                        proj_matrix = torch.matmul(vec, vec.t())\n",
    "\n",
    "                        corrupt_pos_charac = torch.matmul(corrupt_layer_out[t], proj_matrix)\n",
    "                        clean_pos_charac = torch.matmul(model.model.layers[layer_idx].output[0][0, t], proj_matrix)\n",
    "                        model.model.layers[layer_idx].output[0][0, t] = (model.model.layers[layer_idx].output[0][0, t] - clean_pos_charac) + corrupt_pos_charac\n",
    "                        # model.model.layers[layer_idx].output[0][0, t] = corrupt_layer_out[t]\n",
    "\n",
    "                    del vec, proj_matrix, corrupt_pos_charac\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                    pred = model.lm_head.output[0, -1].argmax(dim=-1).save()\n",
    "\n",
    "            print(f\"Pred: {model.tokenizer.decode([pred]).lower().strip()} | Target: {target}\")\n",
    "            if model.tokenizer.decode([pred]).lower().strip() == \"unknown\":\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "            del corrupt_layer_out, pred\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    acc = round(correct / total, 2)\n",
    "    # visibility_diff_reverse_sv[layer_idx] = acc\n",
    "    print(f\"Layer: {layer_idx} | Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort accs_visibility_full by key\n",
    "visibility_diff_reverse_sv = dict(sorted(visibility_diff_reverse_sv.items(), key=lambda x: x[0]))\n",
    "visibility_diff_reverse_sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_stories = [\n",
    "    {\n",
    "        \"story\": dataset[0][\"clean_story\"],\n",
    "        \"question\": dataset[0][\"clean_question\"],\n",
    "        \"answer\": dataset[0][\"clean_ans\"],\n",
    "    },\n",
    "    {\n",
    "        \"story\": dataset[0][\"corrupt_story\"],\n",
    "        \"question\": dataset[0][\"corrupt_question\"],\n",
    "        \"answer\": dataset[0][\"corrupt_ans\"],\n",
    "    }\n",
    "]\n",
    "\n",
    "arrows = [{'start': token_pos_coords['e1_query_charac'], 'end': token_pos_coords['e2_query_charac'], 'color': 'red'}]\n",
    "\n",
    "plot_data = {\n",
    "    \"labels\": visibility_diff_reverse_sv.keys(),\n",
    "    \"acc_one_layer\": visibility_diff_reverse_sv.values(),\n",
    "    \"title\": \"Aligning Visibility info\",\n",
    "    \"x_label\": \"Layers\",\n",
    "    \"y_label\": \"Intervention Accuracy\",\n",
    "}\n",
    "\n",
    "characters = list(set(dataset[0]['clean_characters'] + dataset[0]['corrupt_characters']))\n",
    "objects = list(set(dataset[0]['clean_objects'] + dataset[0]['corrupt_objects']))\n",
    "states = list(set(dataset[0]['clean_states'] + dataset[0]['corrupt_states']))\n",
    "\n",
    "generator = StoryGenerator(characters=characters, objects=objects, states=states, stories=true_stories, target=\"unknown\", arrows=[], plot_data=plot_data)\n",
    "generator.save_html(filename=\"../plots/visibility_exps/first_obj/visibility_diff_reverse_sv.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intervening on BigToM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sing_vecs = defaultdict(dict)\n",
    "for l in range(41):\n",
    "    sing_vecs[l] = torch.load(f\"../svd_results/bigtom/singular_vecs/{l}.pt\").cpu()\n",
    "    # sing_vecs[l] = torch.load(f\"../svd_results/selected_tokens_diff/singular_vecs/{l}.pt\").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ques_start_token_idx(tokenizer, prompt):\n",
    "    input_tokens = tokenizer.encode(prompt, return_tensors=\"pt\").squeeze()\n",
    "    corrolary_token = tokenizer.encode(\":\", return_tensors=\"pt\").squeeze()[-1].item()\n",
    "    ques_start_idx = (input_tokens == corrolary_token).nonzero()[2].item()\n",
    "\n",
    "    return ques_start_idx-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_token_len(tokenizer, prompt):\n",
    "    input_tokens = tokenizer.encode(prompt, return_tensors=\"pt\").squeeze()\n",
    "    return len(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pred(pred, target, verbose=False):\n",
    "    prompt = f\"Instruction: Check if the following ground truth and prediction are same or different. If they are the same, then predict 'Yes', else 'No' \\n\\nGround truth: {target}\\nPrediction: {pred}\\nAnswer:\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(prompt)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with model.generate(prompt, max_new_tokens=5, do_sample=False, num_return_sequences=1, pad_token_id=model.tokenizer.pad_token_id):\n",
    "            out = model.generator.output.save()\n",
    "\n",
    "    prompt_len = get_prompt_token_len(model.tokenizer, prompt)\n",
    "\n",
    "    return model.tokenizer.decode(out[0][prompt_len:-1]).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading BigToM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a csv file\n",
    "df_false = pd.read_csv(\"../data/bigtom/0_forward_belief_false_belief/stories.csv\", delimiter=\";\")\n",
    "df_true = pd.read_csv(\"../data/bigtom/0_forward_belief_true_belief/stories.csv\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each row in the dataframe extract story, answer, and distractor\n",
    "true_stories, false_stories = [], []\n",
    "for i in range(len(df_true)):\n",
    "    story = df_true.iloc[i]['story']\n",
    "    question = df_true.iloc[i]['question']\n",
    "    answer = df_true.iloc[i]['answer']\n",
    "    distractor = df_true.iloc[i]['distractor']\n",
    "    true_stories.append({\"story\": story, \"question\": question, \"answer\": answer, \"distractor\": distractor})\n",
    "\n",
    "for i in range(len(df_false)):\n",
    "    story = df_false.iloc[i]['story']\n",
    "    question = df_true.iloc[i]['question']\n",
    "    answer = df_false.iloc[i]['answer']\n",
    "    distractor = df_false.iloc[i]['distractor']\n",
    "    false_stories.append({\"story\": story, \"question\": question, \"answer\": answer, \"distractor\": distractor})\n",
    "\n",
    "dataset = []\n",
    "instruction = \"1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\"\n",
    "\n",
    "for i in range(min(len(true_stories), len(false_stories))):\n",
    "    question = true_stories[i]['question']\n",
    "    visible_prompt = f\"Instructions: {instruction}\\n\\nStory: {true_stories[i]['story']}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "    question = false_stories[i]['question']\n",
    "    invisible_prompt = f\"Instructions: {instruction}\\n\\nStory: {false_stories[i]['story']}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "    visible_ans = true_stories[i]['answer'].split()\n",
    "    invisible_ans = false_stories[i]['answer'].split()\n",
    "\n",
    "    # Find the index of first word which is different in both answers\n",
    "    diff_idx = 0\n",
    "    for idx, (v, j) in enumerate(zip(visible_ans, invisible_ans)):\n",
    "        if v != j:\n",
    "            diff_idx = idx\n",
    "            break\n",
    "    \n",
    "    visible_ans = \" \".join(visible_ans[diff_idx:])[:-1]\n",
    "    invisible_ans = \" \".join(invisible_ans[diff_idx:])[:-1]\n",
    "\n",
    "    dataset.append({\n",
    "        \"visible_story\": true_stories[i]['story'],\n",
    "        \"visible_question\": true_stories[i]['question'],\n",
    "        \"visible_prompt\": visible_prompt,\n",
    "        \"visible_ans\": visible_ans,\n",
    "        \"invisible_story\": false_stories[i]['story'],\n",
    "        \"invisible_question\": false_stories[i]['question'],\n",
    "        \"invisible_prompt\": invisible_prompt,\n",
    "        \"invisible_ans\": invisible_ans,\n",
    "    })\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "print(dataset[idx]['visible_prompt'], dataset[idx]['visible_ans'])\n",
    "print(dataset[idx]['invisible_prompt'], dataset[idx]['invisible_ans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errors = []\n",
    "# for bi, batch in enumerate(dataloader):\n",
    "#     visible_prompt = batch['visible_prompt'][0]\n",
    "#     visible_ans = batch['visible_ans'][0]\n",
    "#     invisible_prompt = batch['invisible_prompt'][0]\n",
    "#     invisible_ans = batch['invisible_ans'][0]\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         with model.generate(visible_prompt, max_new_tokens=5, do_sample=False, num_return_sequences=1, pad_token_id=model.tokenizer.pad_token_id):\n",
    "#             vis_out = model.generator.output.save()\n",
    "\n",
    "#         with model.generate(invisible_prompt, max_new_tokens=5, do_sample=False, num_return_sequences=1, pad_token_id=model.tokenizer.pad_token_id):\n",
    "#             invis_out = model.generator.output.save()\n",
    "\n",
    "#     vis_prompt_len = get_prompt_token_len(model.tokenizer, visible_prompt)\n",
    "#     invis_prompt_len = get_prompt_token_len(model.tokenizer, invisible_prompt)\n",
    "\n",
    "#     vis_check = check_pred(model.tokenizer.decode(vis_out[0][vis_prompt_len:-1]), visible_ans, verbose=True)\n",
    "#     invis_check = check_pred(model.tokenizer.decode(invis_out[0][invis_prompt_len:-1]), invisible_ans, verbose=True)\n",
    "\n",
    "#     print(f\"Bi: {bi} | Visible: {vis_check} | Invisible: {invis_check}\\n\")\n",
    "\n",
    "#     if vis_check == \"No\" or invis_check == \"No\":\n",
    "#         errors.append(bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patching Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accs_sv = {}\n",
    "\n",
    "for layer_idx in range(22, 26, 2):\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for bi, batch in tqdm(enumerate(dataloader), total=20):\n",
    "        if bi > 19:\n",
    "            break\n",
    "        visible_prompt = batch['visible_prompt'][0]\n",
    "        visible_ans = batch['visible_ans'][0]\n",
    "        invisible_prompt = batch['invisible_prompt'][0]\n",
    "        invisible_ans = batch['invisible_ans'][0]\n",
    "\n",
    "        visible_ques_idx = get_ques_start_token_idx(model.tokenizer, visible_prompt)\n",
    "        visible_prompt_len = get_prompt_token_len(model.tokenizer, visible_prompt)\n",
    "        invisible_ques_idx = get_ques_start_token_idx(model.tokenizer, invisible_prompt)\n",
    "        invisible_prompt_len = get_prompt_token_len(model.tokenizer, invisible_prompt)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with model.session() as session:\n",
    "\n",
    "                visible_layer_out = defaultdict(dict)\n",
    "                with model.trace(visible_prompt):\n",
    "                    for t_idx, t in enumerate(range(visible_ques_idx, visible_prompt_len)):\n",
    "                        visible_layer_out[t_idx] = model.model.layers[layer_idx].output[0][0, t].save()\n",
    "\n",
    "                with model.generate(invisible_prompt, max_new_tokens=5, do_sample=False, num_return_sequences=1, pad_token_id=model.tokenizer.pad_token_id, eos_token_id=model.tokenizer.eos_token_id):\n",
    "                    vec = sing_vecs[layer_idx][:100, :].t().half()\n",
    "                    proj_matrix = torch.matmul(vec, vec.t())\n",
    "\n",
    "                    for t_idx, t in enumerate(range(invisible_ques_idx, invisible_prompt_len)):\n",
    "                        corrupt_pos_charac = torch.matmul(visible_layer_out[t_idx], proj_matrix)\n",
    "                        clean_pos_charac = torch.matmul(model.model.layers[layer_idx].output[0][0, t], proj_matrix)\n",
    "\n",
    "                        model.model.layers[layer_idx].output[0][0, t] = (model.model.layers[layer_idx].output[0][0, t] - clean_pos_charac) + corrupt_pos_charac\n",
    "\n",
    "                        # model.model.layers[layer_idx].output[0][0, t] = visible_layer_out[t_idx]\n",
    "\n",
    "                    out = model.generator.output.save()\n",
    "\n",
    "                del visible_layer_out, vec, proj_matrix, corrupt_pos_charac, clean_pos_charac\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            out_check = check_pred(model.tokenizer.decode(out[0][invisible_prompt_len:-1]), visible_ans, verbose=True)\n",
    "            print(f\"Output check: {out_check}\\n\")\n",
    "\n",
    "            if out_check == \"Yes\":\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        \n",
    "    print(f\"Layer: {layer_idx} | Accuracy: {round(correct / total, 2)}\")\n",
    "    accs_sv[layer_idx] = round(correct / total, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort accs by key\n",
    "accs_sv = dict(sorted(accs_sv.items(), key=lambda x: x[0]))\n",
    "accs_sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_stories = [\n",
    "    {\n",
    "        \"story\": dataset[0][\"visible_story\"],\n",
    "        \"question\": dataset[0][\"visible_question\"],\n",
    "        \"answer\": dataset[0][\"visible_ans\"],\n",
    "    },\n",
    "    {\n",
    "        \"story\": dataset[0][\"invisible_story\"],\n",
    "        \"question\": dataset[0][\"invisible_question\"],\n",
    "        \"answer\": dataset[0][\"invisible_ans\"],\n",
    "    }\n",
    "]\n",
    "\n",
    "arrows = [{'start': token_pos_coords['e1_query_charac'], 'end': token_pos_coords['e2_query_charac'], 'color': 'red'}]\n",
    "\n",
    "plot_data = {\n",
    "    \"labels\": accs_sv.keys(),\n",
    "    \"acc_one_layer\": accs_sv.values(),\n",
    "    \"title\": \"Aligning Visibility info\",\n",
    "    \"x_label\": \"Layers\",\n",
    "    \"y_label\": \"Intervention Accuracy\",\n",
    "}\n",
    "\n",
    "# characters = list(set(dataset[0]['clean_characters'] + dataset[0]['corrupt_characters']))\n",
    "# objects = list(set(dataset[0]['clean_objects'] + dataset[0]['corrupt_objects']))\n",
    "# states = list(set(dataset[0]['clean_states'] + dataset[0]['corrupt_states']))\n",
    "\n",
    "generator = StoryGenerator(characters=[\"Noor\"], objects=[\"pitcher\"], states=[\"oat\", \"almond\"], stories=true_stories, target=dataset[0][\"visible_ans\"], arrows=[], plot_data=plot_data)\n",
    "generator.save_html(filename=\"../plots/visibility_exps/bigtom/invisibility_to_visbility_sv.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
