{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dtype\n",
    "import torch\n",
    "import argparse\n",
    "from pytorch_lightning import Trainer\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import os\n",
    "from train_utils import load_model_tokenizer, DataModule, TrainingModule\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    model, tokenizer = load_model_tokenizer(args['model_name'])\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "    # wandb_logger = WandbLogger(\n",
    "    #     log_model=\"all\",\n",
    "    #     name=f\"{args['model_name']}_local\",\n",
    "    #     project=\"tom\",\n",
    "    # )\n",
    "\n",
    "    datamodule = DataModule(\n",
    "        tokenizer,\n",
    "        args['data_path'],\n",
    "        args['batch_size'],\n",
    "        model.config.n_positions,\n",
    "        args['seed'],\n",
    "    )\n",
    "    model = TrainingModule(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        seed=args['seed'],\n",
    "        output_dir=args['output_dir'],\n",
    "        lr=args['lr'],\n",
    "        eval_metric=datamodule.eval_metric(),\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        default_root_dir=args[\"output_dir\"],\n",
    "        devices=args[\"devices\"],\n",
    "        max_epochs=args[\"epochs\"],\n",
    "        precision=\"32-true\",\n",
    "        gradient_clip_val=1.0,\n",
    "        deterministic=True,\n",
    "        accumulate_grad_batches=args[\"accumulate_grad_batches\"],\n",
    "        accelerator=\"gpu\",\n",
    "        enable_checkpointing=False,\n",
    "        val_check_interval=0.1,\n",
    "        strategy=\"ddp_notebook\",\n",
    "        log_every_n_steps=1,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule)\n",
    "    # trainer.test(model, datamodule.test_dataloader())\n",
    "    trainer.print(f\"Memory used: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 580, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 943, in _run\n    self.strategy.setup_environment()\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py\", line 153, in setup_environment\n    super().setup_environment()\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py\", line 129, in setup_environment\n    self.accelerator.setup_device(self.root_device)\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/pytorch_lightning/accelerators/cuda.py\", line 46, in setup_device\n    _check_cuda_matmul_precision(device)\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/lightning_fabric/accelerators/cuda.py\", line 361, in _check_cuda_matmul_precision\n    if not torch.cuda.is_available() or not _is_ampere_or_later(device):\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/lightning_fabric/accelerators/cuda.py\", line 355, in _is_ampere_or_later\n    major, _ = torch.cuda.get_device_capability(device)\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 357, in get_device_capability\n    prop = get_device_properties(device)\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 371, in get_device_properties\n    _lazy_init()  # will define _get_device_properties\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 217, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_data_small.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     11\u001b[0m }\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 41\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m TrainingModule(\n\u001b[1;32m     18\u001b[0m     model,\n\u001b[1;32m     19\u001b[0m     tokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     eval_metric\u001b[38;5;241m=\u001b[39mdatamodule\u001b[38;5;241m.\u001b[39meval_metric(),\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     27\u001b[0m     default_root_dir\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     28\u001b[0m     devices\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevices\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     log_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     39\u001b[0m )\n\u001b[0;32m---> 41\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# trainer.test(model, datamodule.test_dataloader())\u001b[39;00m\n\u001b[1;32m     43\u001b[0m trainer\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory used: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmax_memory_allocated()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1e9\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.02f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/anima/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/anima/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/.conda/envs/anima/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:144\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m process_context \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mstart_processes(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapping_function,\n\u001b[1;32m    138\u001b[0m     args\u001b[38;5;241m=\u001b[39mprocess_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     join\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# we will join ourselves to get the process references\u001b[39;00m\n\u001b[1;32m    142\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocs \u001b[38;5;241m=\u001b[39m process_context\u001b[38;5;241m.\u001b[39mprocesses\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mprocess_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    147\u001b[0m worker_output \u001b[38;5;241m=\u001b[39m return_queue\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/.conda/envs/anima/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:160\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    158\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-- Process \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m error_index\n\u001b[1;32m    159\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m original_trace\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[38;5;241m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 580, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 943, in _run\n    self.strategy.setup_environment()\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py\", line 153, in setup_environment\n    super().setup_environment()\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py\", line 129, in setup_environment\n    self.accelerator.setup_device(self.root_device)\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/pytorch_lightning/accelerators/cuda.py\", line 46, in setup_device\n    _check_cuda_matmul_precision(device)\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/lightning_fabric/accelerators/cuda.py\", line 361, in _check_cuda_matmul_precision\n    if not torch.cuda.is_available() or not _is_ampere_or_later(device):\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/lightning_fabric/accelerators/cuda.py\", line 355, in _is_ampere_or_later\n    major, _ = torch.cuda.get_device_capability(device)\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 357, in get_device_capability\n    prop = get_device_properties(device)\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 371, in get_device_properties\n    _lazy_init()  # will define _get_device_properties\n  File \"/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 217, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"model_name\": \"gpt2\",\n",
    "    \"data_path\": \"training_data_small.jsonl\",\n",
    "    \"seed\": 0,\n",
    "    \"output_dir\": \"results\",\n",
    "    \"lr\": 1e-4,\n",
    "    \"devices\": [0],\n",
    "    \"accumulate_grad_batches\": 1,\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 1,\n",
    "}\n",
    "\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"llama\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--data_path\",\n",
    "        type=str,\n",
    "        default=\"/home/local_nikhil/Projects/PositionLens/datasets/gpt2/position_detector.json\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"./weights\")\n",
    "    parser.add_argument(\"--devices\", type=list, default=[0])\n",
    "    parser.add_argument(\"--accumulate_grad_batches\", type=int, default=1)\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1)\n",
    "    parser.add_argument(\"--seed\", type=int, default=10)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.output_dir = os.path.join(args.output_dir, args.model_name)\n",
    "\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6fbd64eb2846d4b91646c14c2398e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/local_nikhil/.cache/huggingface/datasets/json/default-7eb05ebb7d4c0727/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea36d2de2494a4782895b4053c0de01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb125aa14bca4c7b932b48c55d68d1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebfc6632fab84a65bb047f960c11f5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/local_nikhil/.cache/huggingface/datasets/json/default-7eb05ebb7d4c0727/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/local_nikhil/.cache/huggingface/datasets/json/default-c2c79431c022fd19/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e57d6f4f9b41b19790058a0aeee961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072dc2e7f16d4f0b902c58ce0555ed12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd9a27273f9436999ee13601220bf85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/local_nikhil/.cache/huggingface/datasets/json/default-c2c79431c022fd19/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d00336461b847f1b7472bbf4c9329c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\"json\", data_files=\"training_data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nComplete the dialog using provided information.\\n### Input:{input}\\n### Response:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt, add_eos_token=True):\n",
    "    # there's probably a way to do this with the tokenizer settings\n",
    "    # but again, gotta move fast\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=model.config.max_position_embeddings,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < model.config.max_position_embeddings\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point, train_on_inputs=True, add_eos_token=False):\n",
    "    full_prompt = TEMPLATE.format(input=data_point[\"input\"])\n",
    "\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    if not train_on_inputs:\n",
    "        user_prompt = TEMPLATE.format(data_point[\"input\"])\n",
    "        tokenized_user_prompt = tokenize(\n",
    "            user_prompt, add_eos_token=add_eos_token\n",
    "        )\n",
    "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "        if add_eos_token:\n",
    "            user_prompt_len -= 1\n",
    "\n",
    "        tokenized_full_prompt[\"labels\"] = [\n",
    "            -100\n",
    "        ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "            user_prompt_len:\n",
    "        ]  # could be sped up, probably\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DatasetDict' object has no attribute 'train_test_split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m val_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m----> 2\u001b[0m train_val \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_test_split\u001b[49m(test_size\u001b[38;5;241m=\u001b[39mval_size, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m train_data \u001b[38;5;241m=\u001b[39m train_val[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshuffle()\u001b[38;5;241m.\u001b[39mmap(generate_and_tokenize_prompt)\n\u001b[1;32m      4\u001b[0m val_data \u001b[38;5;241m=\u001b[39m train_val[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshuffle()\u001b[38;5;241m.\u001b[39mmap(generate_and_tokenize_prompt)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DatasetDict' object has no attribute 'train_test_split'"
     ]
    }
   ],
   "source": [
    "val_size = 0.1\n",
    "train_val = data.train_test_split(test_size=val_size, seed=0)\n",
    "train_data = train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "val_data = train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader that will return batches of data\n",
    "batch_size = 8\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anima",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
